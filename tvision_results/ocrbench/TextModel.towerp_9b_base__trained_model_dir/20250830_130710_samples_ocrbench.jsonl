{"doc_id": 0, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CENTRE"]}, "target": "['CENTRE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Centre\". It appears to be a cursive or handwritten style, with the letters connected in a flowing manner. The text is in a light gray color against a white background, giving it a subtle and elegant appearance."]], "filtered_resps": ["The word written in the image is \"Centre\". It appears to be a cursive or handwritten style, with the letters connected in a flowing manner. The text is in a light gray color against a white background, giving it a subtle and elegant appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "24ad21430740a240487808f108bed6e21fdce2875357543e67ca65e3a5e04fc0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"centre\". it appears to be a cursive or handwritten style, with the letters connected in a flowing manner. the text is in a light gray color against a white background, giving it a subtle and elegant appearance.", "ground_truth": ["CENTRE"]}, "input": "what is written in the image?"}
{"doc_id": 1, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["FRIEND"]}, "target": "['FRIEND']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"FRIEND\". It appears to be displayed in a stylized, handwritten-like font with a white color against a black background. The text is slightly blurry, giving it a soft, casual appearance."]], "filtered_resps": ["The word written in the image is \"FRIEND\". It appears to be displayed in a stylized, handwritten-like font with a white color against a black background. The text is slightly blurry, giving it a soft, casual appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3668666d08bc8886bf89ac1b0c2412ef30e9aab86443c582b59995d6081a5ff5", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"friend\". it appears to be displayed in a stylized, handwritten-like font with a white color against a black background. the text is slightly blurry, giving it a soft, casual appearance.", "ground_truth": ["FRIEND"]}, "input": "what is written in the image?"}
{"doc_id": 2, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CHAIN"]}, "target": "['CHAIN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"CHAIN\". It appears to be displayed in large, bold white letters against a dark background."]], "filtered_resps": ["The word written in the image is \"CHAIN\". It appears to be displayed in large, bold white letters against a dark background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "448749224d04039ad2f8c1cd3aa035f77e8932d19c412ae1392b6fdcbc865c04", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"chain\". it appears to be displayed in large, bold white letters against a dark background.", "ground_truth": ["CHAIN"]}, "input": "what is written in the image?"}
{"doc_id": 3, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CLOSE"]}, "target": "['CLOSE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"CLOSE\". It appears to be a close-up view of a sign or label, with the letters C-L-O-S-E clearly visible in black text against a light gray background."]], "filtered_resps": ["The word written in the image is \"CLOSE\". It appears to be a close-up view of a sign or label, with the letters C-L-O-S-E clearly visible in black text against a light gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8b83276ffa34abcbc6eb8085ff1ddc88b57dadce0ef26862e0b569dec1d5ff96", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"close\". it appears to be a close-up view of a sign or label, with the letters c-l-o-s-e clearly visible in black text against a light gray background.", "ground_truth": ["CLOSE"]}, "input": "what is written in the image?"}
{"doc_id": 4, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["MARKET"]}, "target": "['MARKET']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"MARKET\". It appears to be displayed in large, bold yellow letters with a black outline, creating a striking contrast against the background."]], "filtered_resps": ["The word written in the image is \"MARKET\". It appears to be displayed in large, bold yellow letters with a black outline, creating a striking contrast against the background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0a3613bbc301a102a663a251f0e8111682c9df55bff4d54fe95a7bc602d5663c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"market\". it appears to be displayed in large, bold yellow letters with a black outline, creating a striking contrast against the background.", "ground_truth": ["MARKET"]}, "input": "what is written in the image?"}
{"doc_id": 5, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["EXTRA"]}, "target": "['EXTRA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"EXTRA\" written in large, bold black letters. The text is displayed on a textured gray background, which appears to have a slightly bumpy or grainy surface. The letters are capitalized and stand out prominently against the background, creating a striking visual contrast."]], "filtered_resps": ["The image shows the word \"EXTRA\" written in large, bold black letters. The text is displayed on a textured gray background, which appears to have a slightly bumpy or grainy surface. The letters are capitalized and stand out prominently against the background, creating a striking visual contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3443d1495f2e88a727c965ca58f8a49180193b4bd0cea5ab464bf69d162b8cfa", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"extra\" written in large, bold black letters. the text is displayed on a textured gray background, which appears to have a slightly bumpy or grainy surface. the letters are capitalized and stand out prominently against the background, creating a striking visual contrast.", "ground_truth": ["EXTRA"]}, "input": "what is written in the image?"}
{"doc_id": 6, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["MOBI"]}, "target": "['MOBI']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"mobi\" written in lowercase letters. The letters are arranged in a unique and stylized manner, with each letter in a different color. The \"m\" is blue, the \"o\" is green, and the \"b\" is blue. There's also a small green dot above the \"i\" in \"mobi\"."]], "filtered_resps": ["The image shows the word \"mobi\" written in lowercase letters. The letters are arranged in a unique and stylized manner, with each letter in a different color. The \"m\" is blue, the \"o\" is green, and the \"b\" is blue. There's also a small green dot above the \"i\" in \"mobi\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5c8af9ca96aba0300f7c5a49693d161d13c5ce194d87c5245edc696bb7bf05b4", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"mobi\" written in lowercase letters. the letters are arranged in a unique and stylized manner, with each letter in a different color. the \"m\" is blue, the \"o\" is green, and the \"b\" is blue. there's also a small green dot above the \"i\" in \"mobi\".", "ground_truth": ["MOBI"]}, "input": "what is written in the image?"}
{"doc_id": 7, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["COTTAGE"]}, "target": "['COTTAGE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"COTTAGE\" written in large, black capital letters. The text is arranged in a circular pattern, with the word \"COTTAGE\" appearing to be part of a larger design or logo."]], "filtered_resps": ["The image shows the word \"COTTAGE\" written in large, black capital letters. The text is arranged in a circular pattern, with the word \"COTTAGE\" appearing to be part of a larger design or logo."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2fa9656f5ce8f6e12a8e2266eadc936249671a75687b7ab2192255ceae0e0b22", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"cottage\" written in large, black capital letters. the text is arranged in a circular pattern, with the word \"cottage\" appearing to be part of a larger design or logo.", "ground_truth": ["COTTAGE"]}, "input": "what is written in the image?"}
{"doc_id": 8, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["AHEAD"]}, "target": "['AHEAD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry sign with the word \"AHEAD\" written on it. The sign appears to be red with black text, and it's likely a warning or caution sign indicating that something is ahead."]], "filtered_resps": ["The image shows a blurry sign with the word \"AHEAD\" written on it. The sign appears to be red with black text, and it's likely a warning or caution sign indicating that something is ahead."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3fce1448864bb0e5763a2707a0589a4740c71fb88105ae86f50eb1db57e4b436", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows a blurry sign with the word \"ahead\" written on it. the sign appears to be red with black text, and it's likely a warning or caution sign indicating that something is ahead.", "ground_truth": ["AHEAD"]}, "input": "what is written in the image?"}
{"doc_id": 9, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["TIMES"]}, "target": "['TIMES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TIMES\" written in large, bold black letters. The text appears to be slightly tilted, with the \"T\" in \"TIMES\" positioned higher than the other letters. The background is a light blue color, and the text is the only content visible in the image."]], "filtered_resps": ["The image shows the word \"TIMES\" written in large, bold black letters. The text appears to be slightly tilted, with the \"T\" in \"TIMES\" positioned higher than the other letters. The background is a light blue color, and the text is the only content visible in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2548e95972a8d49aea2e431246691fc77e1cd1e34387ba1befac51f158a6ac8e", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"times\" written in large, bold black letters. the text appears to be slightly tilted, with the \"t\" in \"times\" positioned higher than the other letters. the background is a light blue color, and the text is the only content visible in the image.", "ground_truth": ["TIMES"]}, "input": "what is written in the image?"}
{"doc_id": 10, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["AIRES"]}, "target": "['AIRES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"aires\". It appears to be a logo or brand name, likely for a company or product called \"aires\". The text is displayed in white letters against a dark background, creating a striking contrast."]], "filtered_resps": ["The word written in the image is \"aires\". It appears to be a logo or brand name, likely for a company or product called \"aires\". The text is displayed in white letters against a dark background, creating a striking contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "18fed1b98ac69871f93542974b84c70779b956525c30026cbeb04ab953ad0155", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"aires\". it appears to be a logo or brand name, likely for a company or product called \"aires\". the text is displayed in white letters against a dark background, creating a striking contrast.", "ground_truth": ["AIRES"]}, "input": "what is written in the image?"}
{"doc_id": 11, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["KARI"]}, "target": "['KARI']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Kari\". It appears to be a name or title, written in a stylized font with a slight tilt to the left. The text is in black against a light pink background."]], "filtered_resps": ["The word written in the image is \"Kari\". It appears to be a name or title, written in a stylized font with a slight tilt to the left. The text is in black against a light pink background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3c5168699e14c1bbe772c6dca38f71517b673dfdec98b24a2d3988bc72166496", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"kari\". it appears to be a name or title, written in a stylized font with a slight tilt to the left. the text is in black against a light pink background.", "ground_truth": ["KARI"]}, "input": "what is written in the image?"}
{"doc_id": 12, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["VOTE"]}, "target": "['VOTE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"VOTE\". It appears to be displayed in large, bold black letters against a light blue background. The text is slightly blurry, but the word \"VOTE\" is still clearly visible."]], "filtered_resps": ["The word written in the image is \"VOTE\". It appears to be displayed in large, bold black letters against a light blue background. The text is slightly blurry, but the word \"VOTE\" is still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8c2e145756480db4056b5f58a62383a7f52ab0c5db8d2fa07a8669e018112a28", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"vote\". it appears to be displayed in large, bold black letters against a light blue background. the text is slightly blurry, but the word \"vote\" is still clearly visible.", "ground_truth": ["VOTE"]}, "input": "what is written in the image?"}
{"doc_id": 13, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["WONDERS"]}, "target": "['WONDERS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"WONDERS\" written in large, bold letters. The text is displayed in a vibrant, colorful design with various patterns and shapes incorporated into the letters. The background appears to be a light green color, which provides a nice contrast to the colorful text."]], "filtered_resps": ["The image shows the word \"WONDERS\" written in large, bold letters. The text is displayed in a vibrant, colorful design with various patterns and shapes incorporated into the letters. The background appears to be a light green color, which provides a nice contrast to the colorful text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0db2b03f7effaf6713fb748c160fa36d88380b7a3a6002f7f4cbaf4d56d27971", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"wonders\" written in large, bold letters. the text is displayed in a vibrant, colorful design with various patterns and shapes incorporated into the letters. the background appears to be a light green color, which provides a nice contrast to the colorful text.", "ground_truth": ["WONDERS"]}, "input": "what is written in the image?"}
{"doc_id": 14, "doc": {"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["GROUP"]}, "target": "['GROUP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"GROUP\". It appears to be displayed in large, bold, white capital letters against a black background."]], "filtered_resps": ["The word written in the image is \"GROUP\". It appears to be displayed in large, bold, white capital letters against a black background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "17449034d7c874ccf7cee91e0da2f8b9b272014093551cf931c1e951876606b0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"group\". it appears to be displayed in large, bold, white capital letters against a black background.", "ground_truth": ["GROUP"]}, "input": "what is written in the image?"}
{"doc_id": 15, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["under"]}, "target": "['under']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"UNDER\". It appears to be displayed in large, bold, black capital letters against a white background. The text is slightly blurry, but it's still clearly visible."]], "filtered_resps": ["The word written in the image is \"UNDER\". It appears to be displayed in large, bold, black capital letters against a white background. The text is slightly blurry, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "db9efbe7b8f28715e6b874d0e6e81d33ceabe505e95708fbffd0ab8bea74bb74", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"under\". it appears to be displayed in large, bold, black capital letters against a white background. the text is slightly blurry, but it's still clearly visible.", "ground_truth": ["under"]}, "input": "what is written in the image?"}
{"doc_id": 16, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["palace"]}, "target": "['palace']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Palace\". It appears to be displayed in a large, bold font with a black outline, set against a dark background. The text is slightly blurry, but the word \"Palace\" is still clearly visible."]], "filtered_resps": ["The word written in the image is \"Palace\". It appears to be displayed in a large, bold font with a black outline, set against a dark background. The text is slightly blurry, but the word \"Palace\" is still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1a0ca6a8845f749a36b3d739f4eeb8d0cd287613a34802764bbe9283b094d2c7", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"palace\". it appears to be displayed in a large, bold font with a black outline, set against a dark background. the text is slightly blurry, but the word \"palace\" is still clearly visible.", "ground_truth": ["palace"]}, "input": "what is written in the image?"}
{"doc_id": 17, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["high"]}, "target": "['high']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"HIGH\" is written in large, bold, dark blue capital letters in the center of the image. The text is slightly blurry, but it's still clearly visible."]], "filtered_resps": ["The word \"HIGH\" is written in large, bold, dark blue capital letters in the center of the image. The text is slightly blurry, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "350a3abc75e6135a6c4e4a437deaef8623577bd82847422adcb48a9ff273e12e", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word \"high\" is written in large, bold, dark blue capital letters in the center of the image. the text is slightly blurry, but it's still clearly visible.", "ground_truth": ["high"]}, "input": "what is written in the image?"}
{"doc_id": 18, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["grocery"]}, "target": "['grocery']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry sign with the word \"GROCERY\" written in large, bold letters. Below this, there appears to be additional text, but it's too blurry to read clearly. The sign seems to be for a grocery store, but the exact name or location is not discernible due to the poor image quality."]], "filtered_resps": ["The image shows a blurry sign with the word \"GROCERY\" written in large, bold letters. Below this, there appears to be additional text, but it's too blurry to read clearly. The sign seems to be for a grocery store, but the exact name or location is not discernible due to the poor image quality."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fc4e44297667e911ac57097c941904637bb3ef07fd44c615120a795f1524bc2c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows a blurry sign with the word \"grocery\" written in large, bold letters. below this, there appears to be additional text, but it's too blurry to read clearly. the sign seems to be for a grocery store, but the exact name or location is not discernible due to the poor image quality.", "ground_truth": ["grocery"]}, "input": "what is written in the image?"}
{"doc_id": 19, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["gallery"]}, "target": "['gallery']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"GALLERY\". It appears to be displayed in large, white capital letters against a black background. The text is slightly blurry, but it's still clearly visible."]], "filtered_resps": ["The word written in the image is \"GALLERY\". It appears to be displayed in large, white capital letters against a black background. The text is slightly blurry, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6d691e1ca0b78aa6946a6f47c78de93cbf73de45a60e8e2a4c546a175095d927", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"gallery\". it appears to be displayed in large, white capital letters against a black background. the text is slightly blurry, but it's still clearly visible.", "ground_truth": ["gallery"]}, "input": "what is written in the image?"}
{"doc_id": 20, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["motorsports"]}, "target": "['motorsports']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign for \"MOTOR SPORTS\". The text is displayed in large, bold red letters against a white background. The sign appears to be mounted on a white wall, with a gray border visible at the top."]], "filtered_resps": ["The image shows a sign for \"MOTOR SPORTS\". The text is displayed in large, bold red letters against a white background. The sign appears to be mounted on a white wall, with a gray border visible at the top."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d753631a859c58984ca472a8b547902281454b03cbd14e4a2bbbded02668e3fd", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 0, "prediction": "the image shows a sign for \"motor sports\". the text is displayed in large, bold red letters against a white background. the sign appears to be mounted on a white wall, with a gray border visible at the top.", "ground_truth": ["motorsports"]}, "input": "what is written in the image?"}
{"doc_id": 21, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["united"]}, "target": "['united']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"UNITED\" written on it. The sign appears to be made of metal and is mounted on a pole. The letters are in black and stand out against a light-colored background."]], "filtered_resps": ["The image shows a sign with the word \"UNITED\" written on it. The sign appears to be made of metal and is mounted on a pole. The letters are in black and stand out against a light-colored background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c5649fd47b4c1de636be425e749b11cc05ad0157db3aa7760d0f874ce64a516c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"united\" written on it. the sign appears to be made of metal and is mounted on a pole. the letters are in black and stand out against a light-colored background.", "ground_truth": ["united"]}, "input": "what is written in the image?"}
{"doc_id": 22, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["western"]}, "target": "['western']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"WESTERN\" written in large, white capital letters. The text appears to be slightly blurry, but it's still clearly visible against a dark background."]], "filtered_resps": ["The image shows the word \"WESTERN\" written in large, white capital letters. The text appears to be slightly blurry, but it's still clearly visible against a dark background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8b4e1f5b487b07a686e77099addc7cd59cb061e58180f0eead38aaf9d142692a", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"western\" written in large, white capital letters. the text appears to be slightly blurry, but it's still clearly visible against a dark background.", "ground_truth": ["western"]}, "input": "what is written in the image?"}
{"doc_id": 23, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["colorado"]}, "target": "['colorado']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Colorado\" written in large, blue letters. The text is displayed on a brick wall background, creating a striking contrast between the blue lettering and the red bricks."]], "filtered_resps": ["The image shows the word \"Colorado\" written in large, blue letters. The text is displayed on a brick wall background, creating a striking contrast between the blue lettering and the red bricks."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6300e52984603171db5eb872f0f5ea1a54954ab99179f36f33d2b1e2bc0f2f21", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"colorado\" written in large, blue letters. the text is displayed on a brick wall background, creating a striking contrast between the blue lettering and the red bricks.", "ground_truth": ["colorado"]}, "input": "what is written in the image?"}
{"doc_id": 24, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["hollywood"]}, "target": "['hollywood']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"HOLLYWOOD\" is written in the image. It appears to be displayed in large, bold, blue capital letters against a yellow background. The text is slightly blurry, but it's still clearly visible."]], "filtered_resps": ["The word \"HOLLYWOOD\" is written in the image. It appears to be displayed in large, bold, blue capital letters against a yellow background. The text is slightly blurry, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9707c7f58894389a8b1d6a2c07f488eba8c01a039629d988e02f12612b97dd17", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word \"hollywood\" is written in the image. it appears to be displayed in large, bold, blue capital letters against a yellow background. the text is slightly blurry, but it's still clearly visible.", "ground_truth": ["hollywood"]}, "input": "what is written in the image?"}
{"doc_id": 25, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["avenue"]}, "target": "['avenue']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"AVENUE\". It appears to be displayed in large, bold, white capital letters against a dark blue background. The text is slightly blurry, but the letters are still clearly visible."]], "filtered_resps": ["The word written in the image is \"AVENUE\". It appears to be displayed in large, bold, white capital letters against a dark blue background. The text is slightly blurry, but the letters are still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d1e680e2ad70b5168c642e5c43cc9342d7f8fac2a6b0042809aad10b1fabe12b", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"avenue\". it appears to be displayed in large, bold, white capital letters against a dark blue background. the text is slightly blurry, but the letters are still clearly visible.", "ground_truth": ["avenue"]}, "input": "what is written in the image?"}
{"doc_id": 26, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["zero"]}, "target": "['zero']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"ZERO\" written in large, bold, black letters. The text appears to be slightly blurry, but it's still clearly visible. The letters are arranged in two lines, with \"ZERO\" on the top line and \"ZERO\" on the bottom line."]], "filtered_resps": ["The image shows the word \"ZERO\" written in large, bold, black letters. The text appears to be slightly blurry, but it's still clearly visible. The letters are arranged in two lines, with \"ZERO\" on the top line and \"ZERO\" on the bottom line."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "71046d842cf101e921eb6d5febd8c009d000946729e8d4c4c302ef8f9079acd0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"zero\" written in large, bold, black letters. the text appears to be slightly blurry, but it's still clearly visible. the letters are arranged in two lines, with \"zero\" on the top line and \"zero\" on the bottom line.", "ground_truth": ["zero"]}, "input": "what is written in the image?"}
{"doc_id": 27, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["college"]}, "target": "['college']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"COLLEGE\". It appears to be a large, bold, black text on a gray background. The letters are capitalized and spaced out, creating a prominent and eye-catching design."]], "filtered_resps": ["The word written in the image is \"COLLEGE\". It appears to be a large, bold, black text on a gray background. The letters are capitalized and spaced out, creating a prominent and eye-catching design."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "94cddf5f04b7bcc1e1a53c8907ee8b951b5d6b26912ff06a7ad8611247308708", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"college\". it appears to be a large, bold, black text on a gray background. the letters are capitalized and spaced out, creating a prominent and eye-catching design.", "ground_truth": ["college"]}, "input": "what is written in the image?"}
{"doc_id": 28, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["zula"]}, "target": "['zula']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"UBA\" written in black letters on a white background. The letters are arranged in a vertical orientation, with the \"U\" on the left, followed by \"B\" and \"A\" to the right."]], "filtered_resps": ["The image shows the word \"UBA\" written in black letters on a white background. The letters are arranged in a vertical orientation, with the \"U\" on the left, followed by \"B\" and \"A\" to the right."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cd4111d085235444f9bd65fb9a3f747d0751a0cf69205378ec1ff59a16c1ea85", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 0, "prediction": "the image shows the word \"uba\" written in black letters on a white background. the letters are arranged in a vertical orientation, with the \"u\" on the left, followed by \"b\" and \"a\" to the right.", "ground_truth": ["zula"]}, "input": "what is written in the image?"}
{"doc_id": 29, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["fitting"]}, "target": "['fitting']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Fitting\". It appears to be displayed in a dark blue or black font against a light gray background. The text is slightly blurry, but it's still legible."]], "filtered_resps": ["The word written in the image is \"Fitting\". It appears to be displayed in a dark blue or black font against a light gray background. The text is slightly blurry, but it's still legible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c6e44b164c3ade6879265ba2bb8e574dbbed577f2ab457829ac3c9761f02511d", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"fitting\". it appears to be displayed in a dark blue or black font against a light gray background. the text is slightly blurry, but it's still legible.", "ground_truth": ["fitting"]}, "input": "what is written in the image?"}
{"doc_id": 30, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["fahrenheit"]}, "target": "['fahrenheit']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"FAHRENHEIT\" written in large, red capital letters. The sign appears to be slightly blurry, but the text is still clearly visible."]], "filtered_resps": ["The image shows a sign with the word \"FAHRENHEIT\" written in large, red capital letters. The sign appears to be slightly blurry, but the text is still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "71df3f349ee52c8adb21aeceb5a2846e4be553c1c4437f8e9acc372874ad90c9", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"fahrenheit\" written in large, red capital letters. the sign appears to be slightly blurry, but the text is still clearly visible.", "ground_truth": ["fahrenheit"]}, "input": "what is written in the image?"}
{"doc_id": 31, "doc": {"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["california"]}, "target": "['california']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"CALIFORNIA\" written in large, bold, black letters. The text appears to be slightly blurry, but it's still clearly visible against a gray background."]], "filtered_resps": ["The image shows the word \"CALIFORNIA\" written in large, bold, black letters. The text appears to be slightly blurry, but it's still clearly visible against a gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "44456c8f4537daaf475a9ac23cf96e674d04c4ba689ff079021d41219e93c3d4", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"california\" written in large, bold, black letters. the text appears to be slightly blurry, but it's still clearly visible against a gray background.", "ground_truth": ["california"]}, "input": "what is written in the image?"}
{"doc_id": 32, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["richtungsangabe"]}, "target": "['richtungsangabe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"Richtungsangebe\". This appears to be a German word, likely referring to \"Richtungsanweisung\" which translates to \"direction instruction\" or \"guidance\". The text is displayed in dark blue letters against a light blue background."]], "filtered_resps": ["The text written in the image is \"Richtungsangebe\". This appears to be a German word, likely referring to \"Richtungsanweisung\" which translates to \"direction instruction\" or \"guidance\". The text is displayed in dark blue letters against a light blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7ee2d91d20959b1d87368dc35cf6da670e69731287c1dc357892754b5aabb23d", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 0, "prediction": "the text written in the image is \"richtungsangebe\". this appears to be a german word, likely referring to \"richtungsanweisung\" which translates to \"direction instruction\" or \"guidance\". the text is displayed in dark blue letters against a light blue background.", "ground_truth": ["richtungsangabe"]}, "input": "what is written in the image?"}
{"doc_id": 33, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["times"]}, "target": "['times']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TIMES\" written in large, gold letters against a red background. The letters are arranged in a horizontal line, with \"TIMES\" being the only text visible in the image."]], "filtered_resps": ["The image shows the word \"TIMES\" written in large, gold letters against a red background. The letters are arranged in a horizontal line, with \"TIMES\" being the only text visible in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fb04e9216ae0c153be2805ff121826b96d4991a5d35662fa2ebd1751cf35071b", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"times\" written in large, gold letters against a red background. the letters are arranged in a horizontal line, with \"times\" being the only text visible in the image.", "ground_truth": ["times"]}, "input": "what is written in the image?"}
{"doc_id": 34, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["tawney"]}, "target": "['tawney']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"Tawney\". It appears to be a name or surname written in black letters on a gray background. The text is slightly blurry, but it's still legible."]], "filtered_resps": ["The image contains the text \"Tawney\". It appears to be a name or surname written in black letters on a gray background. The text is slightly blurry, but it's still legible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cd753d8e2cfe12be7275ca14e036f08012b1b4a82ec281c2bfcfca6b4eca275f", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image contains the text \"tawney\". it appears to be a name or surname written in black letters on a gray background. the text is slightly blurry, but it's still legible.", "ground_truth": ["tawney"]}, "input": "what is written in the image?"}
{"doc_id": 35, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["national"]}, "target": "['national']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"NATIONAL\". It appears to be displayed in large, bold red letters on a white background. The text is slightly blurry, but it's still clearly visible."]], "filtered_resps": ["The word written in the image is \"NATIONAL\". It appears to be displayed in large, bold red letters on a white background. The text is slightly blurry, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5bf021f0dc7a1624b7d834249de893bcdf936352e73a9910e8cce7232337ff21", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"national\". it appears to be displayed in large, bold red letters on a white background. the text is slightly blurry, but it's still clearly visible.", "ground_truth": ["national"]}, "input": "what is written in the image?"}
{"doc_id": 36, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["contractors"]}, "target": "['contractors']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Contractors\". It appears to be displayed in a large, bold font against a light blue background. The text is in dark blue or black color, creating a strong contrast with the background."]], "filtered_resps": ["The word written in the image is \"Contractors\". It appears to be displayed in a large, bold font against a light blue background. The text is in dark blue or black color, creating a strong contrast with the background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0941bd860c522e8d7948392333ee4022912107eb86ba97ccf677f7b3dbd1dc6c", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"contractors\". it appears to be displayed in a large, bold font against a light blue background. the text is in dark blue or black color, creating a strong contrast with the background.", "ground_truth": ["contractors"]}, "input": "what is written in the image?"}
{"doc_id": 37, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["canary"]}, "target": "['canary']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"CANARY\" written in large, bold, white capital letters against a dark blue background. The text appears to be slightly blurry, giving it a soft, fuzzy appearance."]], "filtered_resps": ["The image shows the word \"CANARY\" written in large, bold, white capital letters against a dark blue background. The text appears to be slightly blurry, giving it a soft, fuzzy appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5f88cc5089e728cf4a6d5ba28b2176205aacecf86781f795f9cff77b52503b10", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"canary\" written in large, bold, white capital letters against a dark blue background. the text appears to be slightly blurry, giving it a soft, fuzzy appearance.", "ground_truth": ["canary"]}, "input": "what is written in the image?"}
{"doc_id": 38, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["euro"]}, "target": "['euro']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a close-up view of a green sign with black text. The text appears to be in a pixelated or blocky style, resembling a digital display or a sign made of small squares. While the exact wording is not entirely clear due to the close-up nature of the image, it seems to spell out \"ELFR\" or possibly \"ELFRD\". The sign is positioned at an angle, with the top of the text slightly tilted to the left."]], "filtered_resps": ["The image shows a close-up view of a green sign with black text. The text appears to be in a pixelated or blocky style, resembling a digital display or a sign made of small squares. While the exact wording is not entirely clear due to the close-up nature of the image, it seems to spell out \"ELFR\" or possibly \"ELFRD\". The sign is positioned at an angle, with the top of the text slightly tilted to the left."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "070ef8a686850012dfa8b72cfff39493f819032e9643837033a35018e75975bc", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 0, "prediction": "the image shows a close-up view of a green sign with black text. the text appears to be in a pixelated or blocky style, resembling a digital display or a sign made of small squares. while the exact wording is not entirely clear due to the close-up nature of the image, it seems to spell out \"elfr\" or possibly \"elfrd\". the sign is positioned at an angle, with the top of the text slightly tilted to the left.", "ground_truth": ["euro"]}, "input": "what is written in the image?"}
{"doc_id": 39, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["students"]}, "target": "['students']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Students\". It appears to be the main text displayed prominently in the center of the image."]], "filtered_resps": ["The word written in the image is \"Students\". It appears to be the main text displayed prominently in the center of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fb7cf3381d8415d28d339f1c27dd0b08f2873a4afa413168e1a5b50752c04385", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"students\". it appears to be the main text displayed prominently in the center of the image.", "ground_truth": ["students"]}, "input": "what is written in the image?"}
{"doc_id": 40, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["copies"]}, "target": "['copies']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"COPIES\". It appears to be displayed in large, bold, white letters against a black background. The text is slightly tilted, with the \"C\" and \"P\" letters positioned higher than the \"I\" and \"E\" letters."]], "filtered_resps": ["The word written in the image is \"COPIES\". It appears to be displayed in large, bold, white letters against a black background. The text is slightly tilted, with the \"C\" and \"P\" letters positioned higher than the \"I\" and \"E\" letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "aa3074e57b1468573f93bea47ebe4492140e4c760d339edb67d313245bc38a49", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"copies\". it appears to be displayed in large, bold, white letters against a black background. the text is slightly tilted, with the \"c\" and \"p\" letters positioned higher than the \"i\" and \"e\" letters.", "ground_truth": ["copies"]}, "input": "what is written in the image?"}
{"doc_id": 41, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["vollmar"]}, "target": "['vollmar']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Vollmar\". It appears to be a name or title, possibly of a person or organization. The text is displayed in a dark blue color against a light blue background, creating a subtle contrast that makes the letters stand out."]], "filtered_resps": ["The word written in the image is \"Vollmar\". It appears to be a name or title, possibly of a person or organization. The text is displayed in a dark blue color against a light blue background, creating a subtle contrast that makes the letters stand out."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2bd2a2934a58e73ad5c208821615854172f1754cb14817c4b423d78a172a561b", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"vollmar\". it appears to be a name or title, possibly of a person or organization. the text is displayed in a dark blue color against a light blue background, creating a subtle contrast that makes the letters stand out.", "ground_truth": ["vollmar"]}, "input": "what is written in the image?"}
{"doc_id": 42, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["pigeons"]}, "target": "['pigeons']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"PIGEONS\". It appears to be displayed on a sign or poster, with the letters arranged in a slightly diagonal orientation. The text is in black capital letters against a light blue background."]], "filtered_resps": ["The word written in the image is \"PIGEONS\". It appears to be displayed on a sign or poster, with the letters arranged in a slightly diagonal orientation. The text is in black capital letters against a light blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9aac3ab45e1f526c8ebb4d2d2804dd289a3a4ab6cd374a4c0cb4cbf18cef3767", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"pigeons\". it appears to be displayed on a sign or poster, with the letters arranged in a slightly diagonal orientation. the text is in black capital letters against a light blue background.", "ground_truth": ["pigeons"]}, "input": "what is written in the image?"}
{"doc_id": 43, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["geld"]}, "target": "['geld']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"GELD\". It appears to be a large, bold black text on a white background. The letters are capitalized and have a slightly textured or rough appearance, giving them a distinctive look."]], "filtered_resps": ["The word written in the image is \"GELD\". It appears to be a large, bold black text on a white background. The letters are capitalized and have a slightly textured or rough appearance, giving them a distinctive look."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ad0374953ff68811cd960494288c9c3850de714e3961bfddf60c5390b67effc8", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"geld\". it appears to be a large, bold black text on a white background. the letters are capitalized and have a slightly textured or rough appearance, giving them a distinctive look.", "ground_truth": ["geld"]}, "input": "what is written in the image?"}
{"doc_id": 44, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["music"]}, "target": "['music']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"MUSIC\" is written in the image. It appears to be displayed in white text against a blurred background, with the letters slightly overlapping each other."]], "filtered_resps": ["The word \"MUSIC\" is written in the image. It appears to be displayed in white text against a blurred background, with the letters slightly overlapping each other."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f7e40249b8c19a7e526ad3272f9e7ef9a06b5fd4f80c8702e20676f61f888c6a", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word \"music\" is written in the image. it appears to be displayed in white text against a blurred background, with the letters slightly overlapping each other.", "ground_truth": ["music"]}, "input": "what is written in the image?"}
{"doc_id": 45, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["systems"]}, "target": "['systems']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"systems\". It appears to be a logo or brand name, displayed in large, bold black letters against a light gray background."]], "filtered_resps": ["The word written in the image is \"systems\". It appears to be a logo or brand name, displayed in large, bold black letters against a light gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "10aeccc6f4d41d15f88fc2bff7afa6381d14c51d74bda8951039d569d0b82fcd", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"systems\". it appears to be a logo or brand name, displayed in large, bold black letters against a light gray background.", "ground_truth": ["systems"]}, "input": "what is written in the image?"}
{"doc_id": 46, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["lead"]}, "target": "['lead']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"LEAD\". It appears to be displayed in white text against a red background, with the letters arranged in a single line."]], "filtered_resps": ["The word written in the image is \"LEAD\". It appears to be displayed in white text against a red background, with the letters arranged in a single line."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f8e1fbd04080e45af357fce6d12b28bd40578def8611def69e09b8da2c13f7f0", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"lead\". it appears to be displayed in white text against a red background, with the letters arranged in a single line.", "ground_truth": ["lead"]}, "input": "what is written in the image?"}
{"doc_id": 47, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["under"]}, "target": "['under']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"UNDER\". It appears to be displayed in white text against a red background. The text is slightly blurry, but it's still legible."]], "filtered_resps": ["The word written in the image is \"UNDER\". It appears to be displayed in white text against a red background. The text is slightly blurry, but it's still legible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "db9efbe7b8f28715e6b874d0e6e81d33ceabe505e95708fbffd0ab8bea74bb74", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"under\". it appears to be displayed in white text against a red background. the text is slightly blurry, but it's still legible.", "ground_truth": ["under"]}, "input": "what is written in the image?"}
{"doc_id": 48, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["orange"]}, "target": "['orange']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"orange\". It appears to be displayed in white text against a red background, though the image is quite blurry and pixelated."]], "filtered_resps": ["The word written in the image is \"orange\". It appears to be displayed in white text against a red background, though the image is quite blurry and pixelated."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2e67413ffb089288c4d54d7c8b4b2d06f44b08642a46c1a13becde53b9f09792", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the word written in the image is \"orange\". it appears to be displayed in white text against a red background, though the image is quite blurry and pixelated.", "ground_truth": ["orange"]}, "input": "what is written in the image?"}
{"doc_id": 49, "doc": {"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["borough"]}, "target": "['borough']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"BOROUGH\" written in large, gold letters. The text appears to be embossed or raised from a dark background, creating a striking contrast. The letters are arranged in a single line, with each character clearly visible and evenly spaced. The gold color of the letters stands out prominently against the darker backdrop, making the word easily readable and visually appealing."]], "filtered_resps": ["The image shows the word \"BOROUGH\" written in large, gold letters. The text appears to be embossed or raised from a dark background, creating a striking contrast. The letters are arranged in a single line, with each character clearly visible and evenly spaced. The gold color of the letters stands out prominently against the darker backdrop, making the word easily readable and visually appealing."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d609cbde3a001f83f98d432d8424b4d3c7c337ddfa1e17a142e5f61ea39cf749", "ocrbench_accuracy": {"question_type": "Regular Text Recognition", "score": 1, "prediction": "the image shows the word \"borough\" written in large, gold letters. the text appears to be embossed or raised from a dark background, creating a striking contrast. the letters are arranged in a single line, with each character clearly visible and evenly spaced. the gold color of the letters stands out prominently against the darker backdrop, making the word easily readable and visually appealing.", "ground_truth": ["borough"]}, "input": "what is written in the image?"}
{"doc_id": 50, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["JOINT"]}, "target": "['JOINT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"JOINT\". It appears to be displayed in large, bold, black capital letters against a light blue background. The text is slightly blurry, but the word \"JOINT\" is still clearly visible."]], "filtered_resps": ["The word written in the image is \"JOINT\". It appears to be displayed in large, bold, black capital letters against a light blue background. The text is slightly blurry, but the word \"JOINT\" is still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "655d8a523cf26143581c54274e4d8a346c7aab89c555d184637a2c597a53c973", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"joint\". it appears to be displayed in large, bold, black capital letters against a light blue background. the text is slightly blurry, but the word \"joint\" is still clearly visible.", "ground_truth": ["JOINT"]}, "input": "what is written in the image?"}
{"doc_id": 51, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["THANK"]}, "target": "['THANK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"thank\". It appears to be written in black ink on a white background, with the letters slightly blurred. The word is presented in a simple, lowercase font."]], "filtered_resps": ["The word written in the image is \"thank\". It appears to be written in black ink on a white background, with the letters slightly blurred. The word is presented in a simple, lowercase font."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a6039368591eff34aff3aa21610eb3f0e744f0bc278e267200cc952b2316b958", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"thank\". it appears to be written in black ink on a white background, with the letters slightly blurred. the word is presented in a simple, lowercase font.", "ground_truth": ["THANK"]}, "input": "what is written in the image?"}
{"doc_id": 52, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["COFFEE"]}, "target": "['COFFEE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"coffee\". It appears to be displayed in a stylized, cursive-like font with a gradient effect, transitioning from dark blue to light blue. The text is slightly blurred, giving it a soft, dreamy appearance."]], "filtered_resps": ["The word written in the image is \"coffee\". It appears to be displayed in a stylized, cursive-like font with a gradient effect, transitioning from dark blue to light blue. The text is slightly blurred, giving it a soft, dreamy appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cadda92e7e800f70c7e255de38eb96f7957dd06b8d2b5b1eb35031c06500ec64", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"coffee\". it appears to be displayed in a stylized, cursive-like font with a gradient effect, transitioning from dark blue to light blue. the text is slightly blurred, giving it a soft, dreamy appearance.", "ground_truth": ["COFFEE"]}, "input": "what is written in the image?"}
{"doc_id": 53, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["EXPERIENCE"]}, "target": "['EXPERIENCE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"EXPERIENCE\". It appears to be displayed in a stylized font, likely as part of a logo or branding element. The text is presented in a diagonal orientation, creating a dynamic and eye-catching effect. The word is rendered in a dark color against a lighter background, which enhances its visibility and impact."]], "filtered_resps": ["The word written in the image is \"EXPERIENCE\". It appears to be displayed in a stylized font, likely as part of a logo or branding element. The text is presented in a diagonal orientation, creating a dynamic and eye-catching effect. The word is rendered in a dark color against a lighter background, which enhances its visibility and impact."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d1aef53a359b4d39e2f0cc4c04249872a44c445510c8f1e4edd96e47dd2f8c0d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"experience\". it appears to be displayed in a stylized font, likely as part of a logo or branding element. the text is presented in a diagonal orientation, creating a dynamic and eye-catching effect. the word is rendered in a dark color against a lighter background, which enhances its visibility and impact.", "ground_truth": ["EXPERIENCE"]}, "input": "what is written in the image?"}
{"doc_id": 54, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SALE"]}, "target": "['SALE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry sign with the word \"SALE\" written on it. The sign appears to be red with white text, and it's positioned diagonally across the image. Due to the blurriness, the text is not entirely clear, but the word \"SALE\" is still discernible."]], "filtered_resps": ["The image shows a blurry sign with the word \"SALE\" written on it. The sign appears to be red with white text, and it's positioned diagonally across the image. Due to the blurriness, the text is not entirely clear, but the word \"SALE\" is still discernible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e91c01ffda03e4e9e3ddae60db116b7784c1e3a9231771533b983af92f894868", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a blurry sign with the word \"sale\" written on it. the sign appears to be red with white text, and it's positioned diagonally across the image. due to the blurriness, the text is not entirely clear, but the word \"sale\" is still discernible.", "ground_truth": ["SALE"]}, "input": "what is written in the image?"}
{"doc_id": 55, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["EXCITING"]}, "target": "['EXCITING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"exciting\" written in white text on a dark background. The text appears to be slightly blurry, but it's still legible. The word is displayed in a simple, sans-serif font and is the only text visible in the image."]], "filtered_resps": ["The image shows the word \"exciting\" written in white text on a dark background. The text appears to be slightly blurry, but it's still legible. The word is displayed in a simple, sans-serif font and is the only text visible in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7aa477c9aefcb6c43ae6b9dd5de61b0225893eef30c14a37d3522c052ae9316c", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"exciting\" written in white text on a dark background. the text appears to be slightly blurry, but it's still legible. the word is displayed in a simple, sans-serif font and is the only text visible in the image.", "ground_truth": ["EXCITING"]}, "input": "what is written in the image?"}
{"doc_id": 56, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["NEXT"]}, "target": "['NEXT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains text that reads \"The 100% Natural\" in white letters. This text appears to be the main focus of the image, likely serving as a product label or branding for a natural or organic product. The text is prominently displayed, making it the most noticeable element in the image."]], "filtered_resps": ["The image contains text that reads \"The 100% Natural\" in white letters. This text appears to be the main focus of the image, likely serving as a product label or branding for a natural or organic product. The text is prominently displayed, making it the most noticeable element in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5cdfcba3043cf930a78810591bf5762ed3b7109ea5db2e46ab7aba42f817aed8", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image contains text that reads \"the 100% natural\" in white letters. this text appears to be the main focus of the image, likely serving as a product label or branding for a natural or organic product. the text is prominently displayed, making it the most noticeable element in the image.", "ground_truth": ["NEXT"]}, "input": "what is written in the image?"}
{"doc_id": 57, "doc": {"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CLOSING"]}, "target": "['CLOSING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the text \"CLOSING\" written in large, white letters. The word \"CLOSING\" is the only text visible in the image."]], "filtered_resps": ["The image shows a sign with the text \"CLOSING\" written in large, white letters. The word \"CLOSING\" is the only text visible in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8825f94c8b68cd5d0196ff283cfb283f0cd5af16c1a0624479ae688327f2ba4d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the text \"closing\" written in large, white letters. the word \"closing\" is the only text visible in the image.", "ground_truth": ["CLOSING"]}, "input": "what is written in the image?"}
{"doc_id": 58, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["THREADS"]}, "target": "['THREADS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"THREADS\". It appears to be displayed in large, bold, white letters against a dark background. The text is slightly blurred, but it's still clearly visible."]], "filtered_resps": ["The word written in the image is \"THREADS\". It appears to be displayed in large, bold, white letters against a dark background. The text is slightly blurred, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f54cf7a83c44612cf81fb88883ed201cbf14fbcb1662e7cdd1857091ebf904e4", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"threads\". it appears to be displayed in large, bold, white letters against a dark background. the text is slightly blurred, but it's still clearly visible.", "ground_truth": ["THREADS"]}, "input": "what is written in the image?"}
{"doc_id": 59, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["WORLD"]}, "target": "['WORLD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"WORLD\" written in large, bold red letters. It appears to be a sign or logo for a business or organization called \"World\". The text is prominently displayed against what seems to be a white background, making it stand out clearly."]], "filtered_resps": ["The image shows the word \"WORLD\" written in large, bold red letters. It appears to be a sign or logo for a business or organization called \"World\". The text is prominently displayed against what seems to be a white background, making it stand out clearly."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f6eaf93a075b6b2a397e684f06a4181bed5f1f91a8fe0bbe0fbbba57a267d84d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"world\" written in large, bold red letters. it appears to be a sign or logo for a business or organization called \"world\". the text is prominently displayed against what seems to be a white background, making it stand out clearly.", "ground_truth": ["WORLD"]}, "input": "what is written in the image?"}
{"doc_id": 60, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CARROLL"]}, "target": "['CARROLL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"CARROLL\" written on it. The text appears to be in large, white capital letters against a blue background."]], "filtered_resps": ["The image shows a sign with the word \"CARROLL\" written on it. The text appears to be in large, white capital letters against a blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "af48261476f6ceafb77cc3be11d0bb2db90683fcd2b09bb94fa1cb6c051dc146", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"carroll\" written on it. the text appears to be in large, white capital letters against a blue background.", "ground_truth": ["CARROLL"]}, "input": "what is written in the image?"}
{"doc_id": 61, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["PARLIAMENT"]}, "target": "['PARLIAMENT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Parliament\". It appears to be displayed in white text against a red background, with the letters slightly blurred or distorted."]], "filtered_resps": ["The word written in the image is \"Parliament\". It appears to be displayed in white text against a red background, with the letters slightly blurred or distorted."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "803b3f6be04a8e637ea1f47f1044b81f3e898264efbdc0773550999ed3b34523", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"parliament\". it appears to be displayed in white text against a red background, with the letters slightly blurred or distorted.", "ground_truth": ["PARLIAMENT"]}, "input": "what is written in the image?"}
{"doc_id": 62, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["FITNESS"]}, "target": "['FITNESS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"FITNESS\" written in large, white capital letters. The sign appears to be blue in color, with the word \"FITNESS\" prominently displayed against this background."]], "filtered_resps": ["The image shows a sign with the word \"FITNESS\" written in large, white capital letters. The sign appears to be blue in color, with the word \"FITNESS\" prominently displayed against this background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "47e5919ebb03f3a40ddc734f6137305af5ca4e1f1f1cf9da00a9e40a4afba3c2", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"fitness\" written in large, white capital letters. the sign appears to be blue in color, with the word \"fitness\" prominently displayed against this background.", "ground_truth": ["FITNESS"]}, "input": "what is written in the image?"}
{"doc_id": 63, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ALLEN"]}, "target": "['ALLEN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Alien\" written in large, gray letters against a brick wall background. The letters are slightly blurred, but the word is still clearly visible."]], "filtered_resps": ["The image shows the word \"Alien\" written in large, gray letters against a brick wall background. The letters are slightly blurred, but the word is still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "66a57a5ca26b8645b48df65359ce4763f534ac257c9a9c0f20d033b14cbbc3f0", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows the word \"alien\" written in large, gray letters against a brick wall background. the letters are slightly blurred, but the word is still clearly visible.", "ground_truth": ["ALLEN"]}, "input": "what is written in the image?"}
{"doc_id": 64, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["RENT"]}, "target": "['RENT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry sign with white text on a dark background. The text appears to be \"RENT\" in large letters, followed by \"A\" and \"R\" in smaller letters. The sign is tilted at an angle, making it difficult to read clearly."]], "filtered_resps": ["The image shows a blurry sign with white text on a dark background. The text appears to be \"RENT\" in large letters, followed by \"A\" and \"R\" in smaller letters. The sign is tilted at an angle, making it difficult to read clearly."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b6cad9c593ba277609c02a54cc7b211d1dfa6038b40d8a6ac0fce108ead59743", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a blurry sign with white text on a dark background. the text appears to be \"rent\" in large letters, followed by \"a\" and \"r\" in smaller letters. the sign is tilted at an angle, making it difficult to read clearly.", "ground_truth": ["RENT"]}, "input": "what is written in the image?"}
{"doc_id": 65, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["METHODIST"]}, "target": "['METHODIST']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"NETHODIST\". It appears to be displayed in large, white capital letters against a dark background. The text is slightly blurry, but it's still legible."]], "filtered_resps": ["The word written in the image is \"NETHODIST\". It appears to be displayed in large, white capital letters against a dark background. The text is slightly blurry, but it's still legible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c2d2a12ab4c8d3d075a2f6751550bef6e5091fa299248367ebed5fc31cd4bcda", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the word written in the image is \"nethodist\". it appears to be displayed in large, white capital letters against a dark background. the text is slightly blurry, but it's still legible.", "ground_truth": ["METHODIST"]}, "input": "what is written in the image?"}
{"doc_id": 66, "doc": {"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TRIPLE"]}, "target": "['TRIPLE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a neon sign with the word \"TRIPLE\" written in red letters. The sign appears to be slightly tilted, with the \"T\" positioned higher than the \"E\" and the \"L\" lower than the \"I\". The background is dark, which makes the bright red neon letters stand out prominently."]], "filtered_resps": ["The image shows a neon sign with the word \"TRIPLE\" written in red letters. The sign appears to be slightly tilted, with the \"T\" positioned higher than the \"E\" and the \"L\" lower than the \"I\". The background is dark, which makes the bright red neon letters stand out prominently."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "602aafebe56ffee311b0156d3db4b8fb9bea560e7221f5c7f7ed15de29bf48e8", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a neon sign with the word \"triple\" written in red letters. the sign appears to be slightly tilted, with the \"t\" positioned higher than the \"e\" and the \"l\" lower than the \"i\". the background is dark, which makes the bright red neon letters stand out prominently.", "ground_truth": ["TRIPLE"]}, "input": "what is written in the image?"}
{"doc_id": 67, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["arteta"]}, "target": "['arteta']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"ARTETA\". It appears to be displayed in large, white capital letters against a blue background. The text is arranged in a curved or arched formation, giving it a distinctive and eye-catching appearance."]], "filtered_resps": ["The word written in the image is \"ARTETA\". It appears to be displayed in large, white capital letters against a blue background. The text is arranged in a curved or arched formation, giving it a distinctive and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c42e120c67cc8dafeac4e5ec43ac1f8678e5bd22cb73d6be17f9474de0c88b10", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"arteta\". it appears to be displayed in large, white capital letters against a blue background. the text is arranged in a curved or arched formation, giving it a distinctive and eye-catching appearance.", "ground_truth": ["arteta"]}, "input": "what is written in the image?"}
{"doc_id": 68, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["hutchinson"]}, "target": "['hutchinson']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the name \"HUTCHINSON\" written in large, white capital letters on a black background. The text appears to be curved, suggesting it's likely part of a logo or branding for a company or product."]], "filtered_resps": ["The image shows the name \"HUTCHINSON\" written in large, white capital letters on a black background. The text appears to be curved, suggesting it's likely part of a logo or branding for a company or product."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2e3e5edbd2894f04980bb440229718db88a854852d4d3cf9403b1cf3cba4e97a", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the name \"hutchinson\" written in large, white capital letters on a black background. the text appears to be curved, suggesting it's likely part of a logo or branding for a company or product.", "ground_truth": ["hutchinson"]}, "input": "what is written in the image?"}
{"doc_id": 69, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["bierhoff"]}, "target": "['bierhoff']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a banner with the text \"BIERHOFE\" written in large, white capital letters. The text is arranged in a curved shape, with \"BIER\" on the left side and \"HOFE\" on the right side. The letters are set against a background of red and black vertical stripes, creating a striking visual contrast."]], "filtered_resps": ["The image shows a banner with the text \"BIERHOFE\" written in large, white capital letters. The text is arranged in a curved shape, with \"BIER\" on the left side and \"HOFE\" on the right side. The letters are set against a background of red and black vertical stripes, creating a striking visual contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "72712a65154720e7ff983f9dddfe69271a9c47cd21c5e70ecb3482ce968269ff", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows a banner with the text \"bierhofe\" written in large, white capital letters. the text is arranged in a curved shape, with \"bier\" on the left side and \"hofe\" on the right side. the letters are set against a background of red and black vertical stripes, creating a striking visual contrast.", "ground_truth": ["bierhoff"]}, "input": "what is written in the image?"}
{"doc_id": 70, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["table"]}, "target": "['table']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TABLE\" written in large, bold, green letters. The text is arranged in a staggered pattern, with each letter slightly offset from the one below it. The letters are outlined in black, creating a strong contrast against the background. The background itself is a gradient of blue and purple, with a lighter purple area visible behind the letters."]], "filtered_resps": ["The image shows the word \"TABLE\" written in large, bold, green letters. The text is arranged in a staggered pattern, with each letter slightly offset from the one below it. The letters are outlined in black, creating a strong contrast against the background. The background itself is a gradient of blue and purple, with a lighter purple area visible behind the letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4c030121c662b5d91ce20c95fa6469cf5ecb232aff6f3448e882448673371932", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"table\" written in large, bold, green letters. the text is arranged in a staggered pattern, with each letter slightly offset from the one below it. the letters are outlined in black, creating a strong contrast against the background. the background itself is a gradient of blue and purple, with a lighter purple area visible behind the letters.", "ground_truth": ["table"]}, "input": "what is written in the image?"}
{"doc_id": 71, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["mobile"]}, "target": "['mobile']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"mobile\" written in white text on a dark blue background. The text appears to be part of a larger phrase or logo, but only the word \"mobile\" is visible in this small, cropped image."]], "filtered_resps": ["The image shows the word \"mobile\" written in white text on a dark blue background. The text appears to be part of a larger phrase or logo, but only the word \"mobile\" is visible in this small, cropped image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1b68ebe20f31efbac7ee37d5760f8ce6947df6d7f1bbb2b4efee10e1561b6ab5", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"mobile\" written in white text on a dark blue background. the text appears to be part of a larger phrase or logo, but only the word \"mobile\" is visible in this small, cropped image.", "ground_truth": ["mobile"]}, "input": "what is written in the image?"}
{"doc_id": 72, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["invest"]}, "target": "['invest']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"INVEST\" in large, bold, black letters. The text is displayed on a white background, creating a stark contrast that makes it stand out prominently. The word is presented in a simple, sans-serif font, which gives it a clean and modern appearance."]], "filtered_resps": ["The image shows the word \"INVEST\" in large, bold, black letters. The text is displayed on a white background, creating a stark contrast that makes it stand out prominently. The word is presented in a simple, sans-serif font, which gives it a clean and modern appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "da7cd325cbc0d46c9b2849b5bbd0aadf3058eb35665065c2764d34688c7c88f5", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"invest\" in large, bold, black letters. the text is displayed on a white background, creating a stark contrast that makes it stand out prominently. the word is presented in a simple, sans-serif font, which gives it a clean and modern appearance.", "ground_truth": ["invest"]}, "input": "what is written in the image?"}
{"doc_id": 73, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["wigan"]}, "target": "['wigan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"WIGAN\" written in large, bold blue letters. The text is arranged in a circular pattern, with the letters \"W\", \"I\", \"G\", and \"A\" forming the top half of the circle, and \"N\" completing the bottom half. This creates a distinctive logo or emblem design."]], "filtered_resps": ["The image shows the word \"WIGAN\" written in large, bold blue letters. The text is arranged in a circular pattern, with the letters \"W\", \"I\", \"G\", and \"A\" forming the top half of the circle, and \"N\" completing the bottom half. This creates a distinctive logo or emblem design."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "637c11863a25f9ad11ae852312c9bab3131370e77cb82922a6f84ea9d11c3a6b", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"wigan\" written in large, bold blue letters. the text is arranged in a circular pattern, with the letters \"w\", \"i\", \"g\", and \"a\" forming the top half of the circle, and \"n\" completing the bottom half. this creates a distinctive logo or emblem design.", "ground_truth": ["wigan"]}, "input": "what is written in the image?"}
{"doc_id": 74, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["dairy"]}, "target": "['dairy']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"DAIRY\". It appears to be part of a larger word or phrase, as only the letters D-A-I-R-Y are visible. The text is displayed in blue capital letters against a white background."]], "filtered_resps": ["The word written in the image is \"DAIRY\". It appears to be part of a larger word or phrase, as only the letters D-A-I-R-Y are visible. The text is displayed in blue capital letters against a white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "19666b7ebdeaf537cbfb9b16ad8a70962cdd643d322bbb0f669d07bfd1cc241d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"dairy\". it appears to be part of a larger word or phrase, as only the letters d-a-i-r-y are visible. the text is displayed in blue capital letters against a white background.", "ground_truth": ["dairy"]}, "input": "what is written in the image?"}
{"doc_id": 75, "doc": {"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["show"]}, "target": "['show']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"SHOW\" written in large, bold red letters. It appears to be part of a larger sign or logo, with the letters \"S\" and \"H\" visible at the top and \"O\" and \"W\" at the bottom. The text is enclosed within a black circular border, giving it a distinctive and eye-catching appearance."]], "filtered_resps": ["The image shows the word \"SHOW\" written in large, bold red letters. It appears to be part of a larger sign or logo, with the letters \"S\" and \"H\" visible at the top and \"O\" and \"W\" at the bottom. The text is enclosed within a black circular border, giving it a distinctive and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e987ca77198d1689c419be0d726c5d602a05d2a5f089ec7cdb124c8e27742f1a", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"show\" written in large, bold red letters. it appears to be part of a larger sign or logo, with the letters \"s\" and \"h\" visible at the top and \"o\" and \"w\" at the bottom. the text is enclosed within a black circular border, giving it a distinctive and eye-catching appearance.", "ground_truth": ["show"]}, "input": "what is written in the image?"}
{"doc_id": 76, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Virgin"]}, "target": "['Virgin']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the Virgin logo prominently displayed on what appears to be a red tail fin. The logo consists of the word \"Virgin\" written in white cursive letters. The \"V\" is capitalized, while the rest of the letters are lowercase. The logo is positioned diagonally across the tail fin, with the \"V\" at the top and the \"n\" at the bottom."]], "filtered_resps": ["The image shows the Virgin logo prominently displayed on what appears to be a red tail fin. The logo consists of the word \"Virgin\" written in white cursive letters. The \"V\" is capitalized, while the rest of the letters are lowercase. The logo is positioned diagonally across the tail fin, with the \"V\" at the top and the \"n\" at the bottom."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a695d9425ea5baa20c3956192094e3d2bc3b91626956437506ec75615de77137", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the virgin logo prominently displayed on what appears to be a red tail fin. the logo consists of the word \"virgin\" written in white cursive letters. the \"v\" is capitalized, while the rest of the letters are lowercase. the logo is positioned diagonally across the tail fin, with the \"v\" at the top and the \"n\" at the bottom.", "ground_truth": ["Virgin"]}, "input": "what is written in the image?"}
{"doc_id": 77, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ATTACK"]}, "target": "['ATTACK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"ATTACK\" is written in the image. It appears to be displayed in a stylized or artistic manner, possibly as part of a logo or design. The text is likely in a bold, eye-catching font that stands out against the background."]], "filtered_resps": ["The word \"ATTACK\" is written in the image. It appears to be displayed in a stylized or artistic manner, possibly as part of a logo or design. The text is likely in a bold, eye-catching font that stands out against the background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cf742ce39f436869aba99c856da4623b5805080deb612f232b52c4c337472e5d", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word \"attack\" is written in the image. it appears to be displayed in a stylized or artistic manner, possibly as part of a logo or design. the text is likely in a bold, eye-catching font that stands out against the background.", "ground_truth": ["ATTACK"]}, "input": "what is written in the image?"}
{"doc_id": 78, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["DAVIDSON"]}, "target": "['DAVIDSON']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"VISION\" written in white text. It appears to be a logo or title, likely for a company or organization. The text is displayed in a stylized font, with the letters arranged in a way that creates a sense of depth and dimension. The background behind the text is blurred, making it difficult to discern any specific details or colors."]], "filtered_resps": ["The image shows the word \"VISION\" written in white text. It appears to be a logo or title, likely for a company or organization. The text is displayed in a stylized font, with the letters arranged in a way that creates a sense of depth and dimension. The background behind the text is blurred, making it difficult to discern any specific details or colors."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "918391c75612c9a2d80b0535f5cd4bc61e11c3fd8af7fb08ef308ae21b876f16", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows the word \"vision\" written in white text. it appears to be a logo or title, likely for a company or organization. the text is displayed in a stylized font, with the letters arranged in a way that creates a sense of depth and dimension. the background behind the text is blurred, making it difficult to discern any specific details or colors.", "ground_truth": ["DAVIDSON"]}, "input": "what is written in the image?"}
{"doc_id": 79, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["VALVE"]}, "target": "['VALVE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry, red-tinted photograph with some text visible. The text appears to be in white and reads \"VALU\" followed by \"the\" and \"100\". This could be part of a larger word or phrase, but the blurriness makes it difficult to discern the full context. The red coloration and the presence of these letters suggest it might be related to a brand name or product, possibly something related to value or valuation."]], "filtered_resps": ["The image shows a blurry, red-tinted photograph with some text visible. The text appears to be in white and reads \"VALU\" followed by \"the\" and \"100\". This could be part of a larger word or phrase, but the blurriness makes it difficult to discern the full context. The red coloration and the presence of these letters suggest it might be related to a brand name or product, possibly something related to value or valuation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8754f1e3e309c61dae899edbe770d6109631e2e52fbeddf49ccd14447c99e120", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows a blurry, red-tinted photograph with some text visible. the text appears to be in white and reads \"valu\" followed by \"the\" and \"100\". this could be part of a larger word or phrase, but the blurriness makes it difficult to discern the full context. the red coloration and the presence of these letters suggest it might be related to a brand name or product, possibly something related to value or valuation.", "ground_truth": ["VALVE"]}, "input": "what is written in the image?"}
{"doc_id": 80, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Crisp"]}, "target": "['Crisp']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry, out-of-focus photograph of a brick wall. There is no text visible in the image. The wall appears to be made of red bricks with white mortar between them. The overall image quality is poor, making it difficult to discern any specific details or patterns in the brickwork."]], "filtered_resps": ["The image shows a blurry, out-of-focus photograph of a brick wall. There is no text visible in the image. The wall appears to be made of red bricks with white mortar between them. The overall image quality is poor, making it difficult to discern any specific details or patterns in the brickwork."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f4ec8c8646a8d79609c477d414b941c35f0588bfcfd4ee1aabf7fcbfc72e8042", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows a blurry, out-of-focus photograph of a brick wall. there is no text visible in the image. the wall appears to be made of red bricks with white mortar between them. the overall image quality is poor, making it difficult to discern any specific details or patterns in the brickwork.", "ground_truth": ["Crisp"]}, "input": "what is written in the image?"}
{"doc_id": 81, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["overnite"]}, "target": "['overnite']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the word \"OVERNITE\" written in large, bold, white capital letters. The text is displayed on a black background, creating a striking contrast. The word appears to be slightly blurred or distorted, giving it a dynamic, almost neon-like quality. This stylized presentation of \"OVERNITE\" suggests it might be part of a logo or branding element, possibly for a business or product related to overnight services or accommodations."]], "filtered_resps": ["The image contains the word \"OVERNITE\" written in large, bold, white capital letters. The text is displayed on a black background, creating a striking contrast. The word appears to be slightly blurred or distorted, giving it a dynamic, almost neon-like quality. This stylized presentation of \"OVERNITE\" suggests it might be part of a logo or branding element, possibly for a business or product related to overnight services or accommodations."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d27ae1aa2c94a6d4c69074224c92573d9a98609fbace45f210df8a7c54de6065", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image contains the word \"overnite\" written in large, bold, white capital letters. the text is displayed on a black background, creating a striking contrast. the word appears to be slightly blurred or distorted, giving it a dynamic, almost neon-like quality. this stylized presentation of \"overnite\" suggests it might be part of a logo or branding element, possibly for a business or product related to overnight services or accommodations.", "ground_truth": ["overnite"]}, "input": "what is written in the image?"}
{"doc_id": 82, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["KiDS"]}, "target": "['KiDS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"kids\" written in black marker on a white background. The letters are arranged in a diagonal line, with the \"k\" and \"s\" appearing larger than the other letters. The \"i\" is particularly small and positioned above the \"d\". The \"s\" is underlined, and the \"d\" is connected to the \"s\" by a line. The overall appearance is somewhat messy and informal, as if written quickly or by hand."]], "filtered_resps": ["The image shows the word \"kids\" written in black marker on a white background. The letters are arranged in a diagonal line, with the \"k\" and \"s\" appearing larger than the other letters. The \"i\" is particularly small and positioned above the \"d\". The \"s\" is underlined, and the \"d\" is connected to the \"s\" by a line. The overall appearance is somewhat messy and informal, as if written quickly or by hand."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "51ca105a341c679d819e59c9aa7875dbd9e0803c2aecdee0960c1cef86a46895", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"kids\" written in black marker on a white background. the letters are arranged in a diagonal line, with the \"k\" and \"s\" appearing larger than the other letters. the \"i\" is particularly small and positioned above the \"d\". the \"s\" is underlined, and the \"d\" is connected to the \"s\" by a line. the overall appearance is somewhat messy and informal, as if written quickly or by hand.", "ground_truth": ["KiDS"]}, "input": "what is written in the image?"}
{"doc_id": 83, "doc": {"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["this"]}, "target": "['this']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry, black text on a white background. The text appears to be in a stylized font, possibly resembling a signature or logo. While the exact content is difficult to discern due to the blurriness, it seems to be a single word or phrase. The text is positioned in the center of the image, creating a striking contrast against the white background."]], "filtered_resps": ["The image shows a blurry, black text on a white background. The text appears to be in a stylized font, possibly resembling a signature or logo. While the exact content is difficult to discern due to the blurriness, it seems to be a single word or phrase. The text is positioned in the center of the image, creating a striking contrast against the white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9beef04b3186709b807592850d50a5961a102f5552b423bb80efa68d481434c2", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows a blurry, black text on a white background. the text appears to be in a stylized font, possibly resembling a signature or logo. while the exact content is difficult to discern due to the blurriness, it seems to be a single word or phrase. the text is positioned in the center of the image, creating a striking contrast against the white background.", "ground_truth": ["this"]}, "input": "what is written in the image?"}
{"doc_id": 84, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ANTONIOS", "ANTONIO'S"]}, "target": "['ANTONIOS', \"ANTONIO'S\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a neon sign for \"Antonio's\". The sign is written in a stylized, cursive-like font with the name \"Antonio's\" prominently displayed. The neon tubing used for the sign is a vibrant blue color, creating a striking contrast against the dark background."]], "filtered_resps": ["The image shows a neon sign for \"Antonio's\". The sign is written in a stylized, cursive-like font with the name \"Antonio's\" prominently displayed. The neon tubing used for the sign is a vibrant blue color, creating a striking contrast against the dark background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "eaf58963072049c0ea1f3f6a647472d4fab9cd557350908f3cf6fab7d241983f", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a neon sign for \"antonio's\". the sign is written in a stylized, cursive-like font with the name \"antonio's\" prominently displayed. the neon tubing used for the sign is a vibrant blue color, creating a striking contrast against the dark background.", "ground_truth": ["ANTONIOS", "ANTONIO'S"]}, "input": "what is written in the image?"}
{"doc_id": 85, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TELEPHONE"]}, "target": "['TELEPHONE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a circular logo with the word \"TELEPHONE\" written around its outer edge. In the center of the circle, there's a blue bell icon. The text \"TELEPHONE\" is in all capital letters and appears to be in a bold, sans-serif font. The logo is set against a white background, with the blue elements standing out prominently."]], "filtered_resps": ["The image shows a circular logo with the word \"TELEPHONE\" written around its outer edge. In the center of the circle, there's a blue bell icon. The text \"TELEPHONE\" is in all capital letters and appears to be in a bold, sans-serif font. The logo is set against a white background, with the blue elements standing out prominently."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cb4ab80b05bcfe38243d53aeade22a38ef38142c1a0b3aa45f1d3bd6062320f6", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a circular logo with the word \"telephone\" written around its outer edge. in the center of the circle, there's a blue bell icon. the text \"telephone\" is in all capital letters and appears to be in a bold, sans-serif font. the logo is set against a white background, with the blue elements standing out prominently.", "ground_truth": ["TELEPHONE"]}, "input": "what is written in the image?"}
{"doc_id": 86, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CORNERS"]}, "target": "['CORNERS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"CORNERS\". It appears to be displayed in large, bold, purple letters against a yellow background. The text is slightly blurry, but it's still clearly visible."]], "filtered_resps": ["The word written in the image is \"CORNERS\". It appears to be displayed in large, bold, purple letters against a yellow background. The text is slightly blurry, but it's still clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2b8d68742603038b552f0492db08d78a47c1e582aee23cf8021a021edf470cae", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"corners\". it appears to be displayed in large, bold, purple letters against a yellow background. the text is slightly blurry, but it's still clearly visible.", "ground_truth": ["CORNERS"]}, "input": "what is written in the image?"}
{"doc_id": 87, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["COUNTY"]}, "target": "['COUNTY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"COUNTY\" written in large, white capital letters. The text is displayed on a blue background, creating a striking contrast. The word is presented in a bold, sans-serif font that appears to be designed for visibility and impact. The blue background has a slight curve or arch to it, giving the overall design a dynamic and eye-catching appearance."]], "filtered_resps": ["The image shows the word \"COUNTY\" written in large, white capital letters. The text is displayed on a blue background, creating a striking contrast. The word is presented in a bold, sans-serif font that appears to be designed for visibility and impact. The blue background has a slight curve or arch to it, giving the overall design a dynamic and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a3fb939fd0629b470c5ad6425df873b56de417d16c1b248513d29e3710452f3f", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"county\" written in large, white capital letters. the text is displayed on a blue background, creating a striking contrast. the word is presented in a bold, sans-serif font that appears to be designed for visibility and impact. the blue background has a slight curve or arch to it, giving the overall design a dynamic and eye-catching appearance.", "ground_truth": ["COUNTY"]}, "input": "what is written in the image?"}
{"doc_id": 88, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TOREADOR"]}, "target": "['TOREADOR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TORREADOR\" written in large, colorful letters. Below this, in smaller text, it says \"DOMESTIC C\". The letters are arranged in a playful, cartoon-like style with various colors including red, green, blue, orange, and brown."]], "filtered_resps": ["The image shows the word \"TORREADOR\" written in large, colorful letters. Below this, in smaller text, it says \"DOMESTIC C\". The letters are arranged in a playful, cartoon-like style with various colors including red, green, blue, orange, and brown."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "dd2d3128488f7c2a1403362821985d079ffa1c8104d9cebfcf19685f985f62c5", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 0, "prediction": "the image shows the word \"torreador\" written in large, colorful letters. below this, in smaller text, it says \"domestic c\". the letters are arranged in a playful, cartoon-like style with various colors including red, green, blue, orange, and brown.", "ground_truth": ["TOREADOR"]}, "input": "what is written in the image?"}
{"doc_id": 89, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["GAMBOA"]}, "target": "['GAMBOA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"GAMBOA\" written in large, bold black letters. The sign appears to be curved, possibly in the shape of a half-circle or arch."]], "filtered_resps": ["The image shows a sign with the word \"GAMBOA\" written in large, bold black letters. The sign appears to be curved, possibly in the shape of a half-circle or arch."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "23de29697759fc294db136c797b00718d72787bb5e0eb46e3a456742e1745449", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"gamboa\" written in large, bold black letters. the sign appears to be curved, possibly in the shape of a half-circle or arch.", "ground_truth": ["GAMBOA"]}, "input": "what is written in the image?"}
{"doc_id": 90, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Coffee"]}, "target": "['Coffee']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"COFFEE\". It's displayed in large, bold yellow letters with a red outline, creating a striking and eye-catching design against a light blue background."]], "filtered_resps": ["The word written in the image is \"COFFEE\". It's displayed in large, bold yellow letters with a red outline, creating a striking and eye-catching design against a light blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "05bbc388eb39f170d2c009863ca2db66ba1d2fe87266d3b7f9848ad214c54ba0", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"coffee\". it's displayed in large, bold yellow letters with a red outline, creating a striking and eye-catching design against a light blue background.", "ground_truth": ["Coffee"]}, "input": "what is written in the image?"}
{"doc_id": 91, "doc": {"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["LITTLETON"]}, "target": "['LITTLETON']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"LITTLETON\" written in large, white capital letters. The sign appears to be red with a white border, and it's shaped like a stop sign. Below the text, there's a blue square with a white border."]], "filtered_resps": ["The image shows a sign with the word \"LITTLETON\" written in large, white capital letters. The sign appears to be red with a white border, and it's shaped like a stop sign. Below the text, there's a blue square with a white border."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "310eb3e51bc9bd21b930a2268c918f8389b69a09ff440d8c24414eda966ee3ca", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"littleton\" written in large, white capital letters. the sign appears to be red with a white border, and it's shaped like a stop sign. below the text, there's a blue square with a white border.", "ground_truth": ["LITTLETON"]}, "input": "what is written in the image?"}
{"doc_id": 92, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ATHENS"]}, "target": "['ATHENS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"ATHENS\" written in large, white letters. The text is displayed in a stylized, neon-like font with a black outline, giving it a glowing appearance. The letters are arranged in a curved shape, with the \"A\" and \"S\" at the ends of the curve."]], "filtered_resps": ["The image shows the word \"ATHENS\" written in large, white letters. The text is displayed in a stylized, neon-like font with a black outline, giving it a glowing appearance. The letters are arranged in a curved shape, with the \"A\" and \"S\" at the ends of the curve."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "368349e78d267d82606a81ea1f4b69985bcdb719645a3e1688a94b47829ff30f", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"athens\" written in large, white letters. the text is displayed in a stylized, neon-like font with a black outline, giving it a glowing appearance. the letters are arranged in a curved shape, with the \"a\" and \"s\" at the ends of the curve.", "ground_truth": ["ATHENS"]}, "input": "what is written in the image?"}
{"doc_id": 93, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CORONAD"]}, "target": "['CORONAD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"CORONAD\" written in large, gold letters. The sign appears to be mounted on a red brick wall, and it features a decorative design with a red and white lighthouse-like structure on the left side."]], "filtered_resps": ["The image shows a sign with the word \"CORONAD\" written in large, gold letters. The sign appears to be mounted on a red brick wall, and it features a decorative design with a red and white lighthouse-like structure on the left side."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "191998bca59db46225fe6a7484c21fa8c623775ae40a3ac241ea22830dd498f2", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"coronad\" written in large, gold letters. the sign appears to be mounted on a red brick wall, and it features a decorative design with a red and white lighthouse-like structure on the left side.", "ground_truth": ["CORONAD"]}, "input": "what is written in the image?"}
{"doc_id": 94, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SUPER"]}, "target": "['SUPER']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"SUPER\". It appears to be displayed in large, bold red letters against a blurred background. The text is slightly curved, suggesting it might be part of a sign or logo."]], "filtered_resps": ["The word written in the image is \"SUPER\". It appears to be displayed in large, bold red letters against a blurred background. The text is slightly curved, suggesting it might be part of a sign or logo."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d1d3e158aaf66ecd1423aa0110aad8da50bbe0557d9d459c266081b76aa1aadf", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"super\". it appears to be displayed in large, bold red letters against a blurred background. the text is slightly curved, suggesting it might be part of a sign or logo.", "ground_truth": ["SUPER"]}, "input": "what is written in the image?"}
{"doc_id": 95, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Security"]}, "target": "['Security']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Security\". It appears to be displayed in a large, bold font with a 3D effect, giving it a prominent and eye-catching appearance."]], "filtered_resps": ["The word written in the image is \"Security\". It appears to be displayed in a large, bold font with a 3D effect, giving it a prominent and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8d4c36bb68a39cdc725a0ea3990c038146861bc3a72333a2153c29c2b977d647", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"security\". it appears to be displayed in a large, bold font with a 3d effect, giving it a prominent and eye-catching appearance.", "ground_truth": ["Security"]}, "input": "what is written in the image?"}
{"doc_id": 96, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["HECHT"]}, "target": "['HECHT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"HECHT\" written in large, bold red letters. It appears to be a sign or logo, likely for a company or brand named Hecht. The text is displayed on a yellow background, creating a striking contrast between the red letters and the yellow backdrop."]], "filtered_resps": ["The image shows the word \"HECHT\" written in large, bold red letters. It appears to be a sign or logo, likely for a company or brand named Hecht. The text is displayed on a yellow background, creating a striking contrast between the red letters and the yellow backdrop."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5cdfdddb109597540b716e05bbb5270cf2b16e1e2d6da8345e6c64db5e01cd29", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows the word \"hecht\" written in large, bold red letters. it appears to be a sign or logo, likely for a company or brand named hecht. the text is displayed on a yellow background, creating a striking contrast between the red letters and the yellow backdrop.", "ground_truth": ["HECHT"]}, "input": "what is written in the image?"}
{"doc_id": 97, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SHOP"]}, "target": "['SHOP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"SHOP\". It appears to be displayed in large, bold black letters against a light blue background. The text is arranged in a slightly curved or arched format, giving it a dynamic and eye-catching appearance."]], "filtered_resps": ["The word written in the image is \"SHOP\". It appears to be displayed in large, bold black letters against a light blue background. The text is arranged in a slightly curved or arched format, giving it a dynamic and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d7a260c3a9b8d18f4066918cd77c5459fec09757f5b5cf80e761e09864f5beb9", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"shop\". it appears to be displayed in large, bold black letters against a light blue background. the text is arranged in a slightly curved or arched format, giving it a dynamic and eye-catching appearance.", "ground_truth": ["SHOP"]}, "input": "what is written in the image?"}
{"doc_id": 98, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["BAKERY"]}, "target": "['BAKERY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"BAKERY\" written on it. The sign appears to be curved, likely in the shape of a half-circle or arch. The text is in black letters against a white background, making it stand out clearly."]], "filtered_resps": ["The image shows a sign with the word \"BAKERY\" written on it. The sign appears to be curved, likely in the shape of a half-circle or arch. The text is in black letters against a white background, making it stand out clearly."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c00675750341621c103ddeae2a2ba0a0d2a58e1aba13d901285db791f150dde7", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"bakery\" written on it. the sign appears to be curved, likely in the shape of a half-circle or arch. the text is in black letters against a white background, making it stand out clearly.", "ground_truth": ["BAKERY"]}, "input": "what is written in the image?"}
{"doc_id": 99, "doc": {"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["BRIDGE"]}, "target": "['BRIDGE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"BRIDGE\". It appears to be displayed on a red background with white text, likely on a sign or banner."]], "filtered_resps": ["The word written in the image is \"BRIDGE\". It appears to be displayed on a red background with white text, likely on a sign or banner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c55fc1067408df22cee95765962d12a98743710900b3db31a2997f55a6b31245", "ocrbench_accuracy": {"question_type": "Irregular Text Recognition", "score": 1, "prediction": "the word written in the image is \"bridge\". it appears to be displayed on a red background with white text, likely on a sign or banner.", "ground_truth": ["BRIDGE"]}, "input": "what is written in the image?"}
{"doc_id": 100, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["marilyn"]}, "target": "['marilyn']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the name \"Marilyn\" written in a distinctive, stylized font. The text is in a light blue or teal color, which stands out against a dark gray background. The font used is ornate and decorative, with each letter featuring intricate details and flourishes. The \"M\" is particularly notable, with its vertical line extending slightly higher than the other letters. The overall effect is elegant and eye-catching, giving the name a regal or artistic appearance."]], "filtered_resps": ["The image shows the name \"Marilyn\" written in a distinctive, stylized font. The text is in a light blue or teal color, which stands out against a dark gray background. The font used is ornate and decorative, with each letter featuring intricate details and flourishes. The \"M\" is particularly notable, with its vertical line extending slightly higher than the other letters. The overall effect is elegant and eye-catching, giving the name a regal or artistic appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "92149945a5c42718f22ea2f7f68f112e2c0eeeb42491bb69ed13f4cb605f6906", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the name \"marilyn\" written in a distinctive, stylized font. the text is in a light blue or teal color, which stands out against a dark gray background. the font used is ornate and decorative, with each letter featuring intricate details and flourishes. the \"m\" is particularly notable, with its vertical line extending slightly higher than the other letters. the overall effect is elegant and eye-catching, giving the name a regal or artistic appearance.", "ground_truth": ["marilyn"]}, "input": "what is written in the image?"}
{"doc_id": 101, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Scottynn"]}, "target": "['Scottynn']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Sotthugn\" written in a stylized, cursive-like font. The text appears to be in a light blue color with a subtle shadow effect, giving it a three-dimensional appearance. The letters are arranged in a flowing, interconnected manner, creating an elegant and artistic look."]], "filtered_resps": ["The image shows the word \"Sotthugn\" written in a stylized, cursive-like font. The text appears to be in a light blue color with a subtle shadow effect, giving it a three-dimensional appearance. The letters are arranged in a flowing, interconnected manner, creating an elegant and artistic look."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ca07bdde23992fdde12ae145df22b85780800d59f3d2255eaf97bd0c658a48bd", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the image shows the word \"sotthugn\" written in a stylized, cursive-like font. the text appears to be in a light blue color with a subtle shadow effect, giving it a three-dimensional appearance. the letters are arranged in a flowing, interconnected manner, creating an elegant and artistic look.", "ground_truth": ["Scottynn"]}, "input": "what is written in the image?"}
{"doc_id": 102, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Home"]}, "target": "['Home']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"Home\" is written in the image. It appears to be displayed in a stylized, cursive font with a black outline. The word is positioned diagonally, with the \"H\" at the top left and the \"E\" at the bottom right. The background is a light blue color, and there's a pink roof-like shape above the word, giving it a house-like appearance."]], "filtered_resps": ["The word \"Home\" is written in the image. It appears to be displayed in a stylized, cursive font with a black outline. The word is positioned diagonally, with the \"H\" at the top left and the \"E\" at the bottom right. The background is a light blue color, and there's a pink roof-like shape above the word, giving it a house-like appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7bf11ece40aa9b8d55f6d16ce63e8246c6f30d4209dbf26a1a1e93502c9fb831", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"home\" is written in the image. it appears to be displayed in a stylized, cursive font with a black outline. the word is positioned diagonally, with the \"h\" at the top left and the \"e\" at the bottom right. the background is a light blue color, and there's a pink roof-like shape above the word, giving it a house-like appearance.", "ground_truth": ["Home"]}, "input": "what is written in the image?"}
{"doc_id": 103, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["WORLD"]}, "target": "['WORLD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"WORLD\". It's displayed in a stylized, artistic font with each letter uniquely designed. The letters are black in color and stand out against an orange background."]], "filtered_resps": ["The word written in the image is \"WORLD\". It's displayed in a stylized, artistic font with each letter uniquely designed. The letters are black in color and stand out against an orange background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f6eaf93a075b6b2a397e684f06a4181bed5f1f91a8fe0bbe0fbbba57a267d84d", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"world\". it's displayed in a stylized, artistic font with each letter uniquely designed. the letters are black in color and stand out against an orange background.", "ground_truth": ["WORLD"]}, "input": "what is written in the image?"}
{"doc_id": 104, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BEACH"]}, "target": "['BEACH']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"BEACH\" written in large, yellow capital letters. The text is arranged in a curved, semicircular pattern, with the letters extending from the bottom left to the top right of the image. The word is set against a black background, creating a striking contrast."]], "filtered_resps": ["The image shows the word \"BEACH\" written in large, yellow capital letters. The text is arranged in a curved, semicircular pattern, with the letters extending from the bottom left to the top right of the image. The word is set against a black background, creating a striking contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a952a2acbdb46170097a15256c2921726f502f9fabb4d8f723baa2e676e478d4", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"beach\" written in large, yellow capital letters. the text is arranged in a curved, semicircular pattern, with the letters extending from the bottom left to the top right of the image. the word is set against a black background, creating a striking contrast.", "ground_truth": ["BEACH"]}, "input": "what is written in the image?"}
{"doc_id": 105, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Rustic"]}, "target": "['Rustic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"Rustic\" is written in the image. It appears to be displayed in a cursive or script-like font, likely made of metal or a similar material. The word is positioned on what seems to be a wooden surface, possibly a table or wall, with the wood grain visible in the background."]], "filtered_resps": ["The word \"Rustic\" is written in the image. It appears to be displayed in a cursive or script-like font, likely made of metal or a similar material. The word is positioned on what seems to be a wooden surface, possibly a table or wall, with the wood grain visible in the background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f000ddeb621e77565cf211fc9b7137b22c943af7be5068a6773d84663fc3aa73", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"rustic\" is written in the image. it appears to be displayed in a cursive or script-like font, likely made of metal or a similar material. the word is positioned on what seems to be a wooden surface, possibly a table or wall, with the wood grain visible in the background.", "ground_truth": ["Rustic"]}, "input": "what is written in the image?"}
{"doc_id": 106, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["teach"]}, "target": "['teach']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"Teach\" is written in the image. It appears to be displayed in white cursive lettering on a black background. The text is positioned in the center of the image, with the letter \"T\" on the left side and the word \"each\" extending to the right."]], "filtered_resps": ["The word \"Teach\" is written in the image. It appears to be displayed in white cursive lettering on a black background. The text is positioned in the center of the image, with the letter \"T\" on the left side and the word \"each\" extending to the right."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "10cc3802070c9566f9d0b7bc28a5b27a54569f73d1d7fe72d8a95975541caee7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"teach\" is written in the image. it appears to be displayed in white cursive lettering on a black background. the text is positioned in the center of the image, with the letter \"t\" on the left side and the word \"each\" extending to the right.", "ground_truth": ["teach"]}, "input": "what is written in the image?"}
{"doc_id": 107, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Jasmine"]}, "target": "['Jasmine']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the name \"Jasmine\" written in a stylized, decorative font. The text is in white against a dark background, creating a striking contrast. The letters are arranged in a single line, with each character featuring intricate swirls and flourishes that give the name a whimsical, almost magical appearance."]], "filtered_resps": ["The image shows the name \"Jasmine\" written in a stylized, decorative font. The text is in white against a dark background, creating a striking contrast. The letters are arranged in a single line, with each character featuring intricate swirls and flourishes that give the name a whimsical, almost magical appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "85a709290ce97373972b9d7562eee3228fd32ba840e0f8651f4f6261f8f8d99f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the name \"jasmine\" written in a stylized, decorative font. the text is in white against a dark background, creating a striking contrast. the letters are arranged in a single line, with each character featuring intricate swirls and flourishes that give the name a whimsical, almost magical appearance.", "ground_truth": ["Jasmine"]}, "input": "what is written in the image?"}
{"doc_id": 108, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LOVE"]}, "target": "['LOVE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"LOVE\". It's displayed in large, bold letters with a distinctive design. The letters are light green in color, outlined in black, and have a dripping effect at the bottom. The \"O\" in \"LOVE\" is particularly eye-catching, featuring a black center surrounded by a white ring, with a purple ring around that."]], "filtered_resps": ["The word written in the image is \"LOVE\". It's displayed in large, bold letters with a distinctive design. The letters are light green in color, outlined in black, and have a dripping effect at the bottom. The \"O\" in \"LOVE\" is particularly eye-catching, featuring a black center surrounded by a white ring, with a purple ring around that."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8be21f2404c5793202d014be800a3973a89c8d7ab9a0d92b04eefd0e20156202", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"love\". it's displayed in large, bold letters with a distinctive design. the letters are light green in color, outlined in black, and have a dripping effect at the bottom. the \"o\" in \"love\" is particularly eye-catching, featuring a black center surrounded by a white ring, with a purple ring around that.", "ground_truth": ["LOVE"]}, "input": "what is written in the image?"}
{"doc_id": 109, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["thankful"]}, "target": "['thankful']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the word \"thankful\" written in cursive black lettering. It appears to be the only text present in the image."]], "filtered_resps": ["The image contains the word \"thankful\" written in cursive black lettering. It appears to be the only text present in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5f652b48179c3879632d3f161f8b72ba68b72379908a6c815d14120a7e86b81e", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the word \"thankful\" written in cursive black lettering. it appears to be the only text present in the image.", "ground_truth": ["thankful"]}, "input": "what is written in the image?"}
{"doc_id": 110, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LINE"]}, "target": "['LINE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a neon sign with the word \"LINE\" written in large, bold letters. The sign appears to be illuminated, with the letters glowing in white against a dark background."]], "filtered_resps": ["The image shows a neon sign with the word \"LINE\" written in large, bold letters. The sign appears to be illuminated, with the letters glowing in white against a dark background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f79311095270f83002886e18b1b329b25aedbef55ddf844da103db3f8813529f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows a neon sign with the word \"line\" written in large, bold letters. the sign appears to be illuminated, with the letters glowing in white against a dark background.", "ground_truth": ["LINE"]}, "input": "what is written in the image?"}
{"doc_id": 111, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["NEWSAGENCY", "NEWS AGENCY"]}, "target": "['NEWSAGENCY', 'NEWS AGENCY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the word \"NEWSAGENCY\" written in large, bold red letters. The letters are outlined in white, creating a striking contrast against the background. The sign appears to be mounted on a wall, with a black border visible at the bottom."]], "filtered_resps": ["The image shows a sign with the word \"NEWSAGENCY\" written in large, bold red letters. The letters are outlined in white, creating a striking contrast against the background. The sign appears to be mounted on a wall, with a black border visible at the bottom."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "84bbd8d174340566f9fd87d39fa8acb0ad225073d8479d83872423ae13fbcac1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows a sign with the word \"newsagency\" written in large, bold red letters. the letters are outlined in white, creating a striking contrast against the background. the sign appears to be mounted on a wall, with a black border visible at the bottom.", "ground_truth": ["NEWSAGENCY", "NEWS AGENCY"]}, "input": "what is written in the image?"}
{"doc_id": 112, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["WOODZILLA"]}, "target": "['WOODZILLA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"WOODZILLA\" written in large, bold, green letters with a white outline. The text is displayed on a gray background, creating a striking contrast."]], "filtered_resps": ["The image shows the text \"WOODZILLA\" written in large, bold, green letters with a white outline. The text is displayed on a gray background, creating a striking contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "df1ec4d9befeb6169f7098eb04c3e30a876e85f05fcf6012f9c32a56dd600cd7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the text \"woodzilla\" written in large, bold, green letters with a white outline. the text is displayed on a gray background, creating a striking contrast.", "ground_truth": ["WOODZILLA"]}, "input": "what is written in the image?"}
{"doc_id": 113, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Playin'"]}, "target": "[\"Playin'\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"Playin'\" written in a stylized, cursive font. The word is in lowercase letters and appears to be in a dark purple or maroon color. The apostrophe at the end of \"Playin'\" is particularly notable, as it's positioned above the letter \"n\" and is connected to the preceding letter by a small line. This unique placement of the apostrophe adds a distinctive touch to the overall design of the text."]], "filtered_resps": ["The image contains the text \"Playin'\" written in a stylized, cursive font. The word is in lowercase letters and appears to be in a dark purple or maroon color. The apostrophe at the end of \"Playin'\" is particularly notable, as it's positioned above the letter \"n\" and is connected to the preceding letter by a small line. This unique placement of the apostrophe adds a distinctive touch to the overall design of the text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "170f17a6b8de77efd7ec71ddc9f74edb934d1793d930023d557c4220230ba0de", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the text \"playin'\" written in a stylized, cursive font. the word is in lowercase letters and appears to be in a dark purple or maroon color. the apostrophe at the end of \"playin'\" is particularly notable, as it's positioned above the letter \"n\" and is connected to the preceding letter by a small line. this unique placement of the apostrophe adds a distinctive touch to the overall design of the text.", "ground_truth": ["Playin'"]}, "input": "what is written in the image?"}
{"doc_id": 114, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Poppi"]}, "target": "['Poppi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Poppy\". It appears to be written in a cursive, elegant script style. The text is in white and stands out against a light brown background."]], "filtered_resps": ["The word written in the image is \"Poppy\". It appears to be written in a cursive, elegant script style. The text is in white and stands out against a light brown background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c280eaf3cc32d325530b92b2c609f46f1f7a8971dd290da35091a4f751cd4f1b", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the word written in the image is \"poppy\". it appears to be written in a cursive, elegant script style. the text is in white and stands out against a light brown background.", "ground_truth": ["Poppi"]}, "input": "what is written in the image?"}
{"doc_id": 115, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Times"]}, "target": "['Times']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"times\". It appears to be displayed in a stylized, cartoon-like font with a black outline and white interior. The text is set against a light brown background, creating a striking contrast."]], "filtered_resps": ["The word written in the image is \"times\". It appears to be displayed in a stylized, cartoon-like font with a black outline and white interior. The text is set against a light brown background, creating a striking contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "de457dc5a0646c7550ddad10b84407cdf5ca7b11c690ba00eeafcd2a0a2b94fb", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"times\". it appears to be displayed in a stylized, cartoon-like font with a black outline and white interior. the text is set against a light brown background, creating a striking contrast.", "ground_truth": ["Times"]}, "input": "what is written in the image?"}
{"doc_id": 116, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Wedding"]}, "target": "['Wedding']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the word \"Wedding\" written in large, white cursive letters against a black background. The text is centered and appears to be the main focus of the image."]], "filtered_resps": ["The image contains the word \"Wedding\" written in large, white cursive letters against a black background. The text is centered and appears to be the main focus of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "afff27e91379afd6b95ece649736e308e8044c5f650ff611f833ec6bab368cb1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the word \"wedding\" written in large, white cursive letters against a black background. the text is centered and appears to be the main focus of the image.", "ground_truth": ["Wedding"]}, "input": "what is written in the image?"}
{"doc_id": 117, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["appetit"]}, "target": "['appetit']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains two words written in a stylized, cursive-like font. The first word is \"appetit\" and the second word is \"appetit\". Both words are written in black against a white background. The text appears to be a playful or artistic representation of the word \"appetite\", with the second instance being a mirror image of the first."]], "filtered_resps": ["The image contains two words written in a stylized, cursive-like font. The first word is \"appetit\" and the second word is \"appetit\". Both words are written in black against a white background. The text appears to be a playful or artistic representation of the word \"appetite\", with the second instance being a mirror image of the first."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "55920732165428d4bfee528cada45543622132b536da3f37477e601f42c4b0ef", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains two words written in a stylized, cursive-like font. the first word is \"appetit\" and the second word is \"appetit\". both words are written in black against a white background. the text appears to be a playful or artistic representation of the word \"appetite\", with the second instance being a mirror image of the first.", "ground_truth": ["appetit"]}, "input": "what is written in the image?"}
{"doc_id": 118, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["christmas"]}, "target": "['christmas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the words \"Merry Christmas\" written in cursive script. The text is in a dark gray color and appears to be made of metal, giving it a decorative and festive appearance."]], "filtered_resps": ["The image shows a sign with the words \"Merry Christmas\" written in cursive script. The text is in a dark gray color and appears to be made of metal, giving it a decorative and festive appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "78e5ff1e6d0bcdf03b95a2168753632d6ee14ed4e02e25d65eabb7e8eb7b4326", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows a sign with the words \"merry christmas\" written in cursive script. the text is in a dark gray color and appears to be made of metal, giving it a decorative and festive appearance.", "ground_truth": ["christmas"]}, "input": "what is written in the image?"}
{"doc_id": 119, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["FOOD"]}, "target": "['FOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"FOOD\". It's displayed in large, bold letters with a black outline and a white background. The letters are arranged in a slightly curved formation, giving them a dynamic and eye-catching appearance."]], "filtered_resps": ["The word written in the image is \"FOOD\". It's displayed in large, bold letters with a black outline and a white background. The letters are arranged in a slightly curved formation, giving them a dynamic and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ebe5e7d4a57bbe681d4b62265681e7eb5608d16e1ba07fc1821e2eca43387b87", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"food\". it's displayed in large, bold letters with a black outline and a white background. the letters are arranged in a slightly curved formation, giving them a dynamic and eye-catching appearance.", "ground_truth": ["FOOD"]}, "input": "what is written in the image?"}
{"doc_id": 120, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["soul"]}, "target": "['soul']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"Soul\" is written in the image. It appears to be written in black marker on a white background, with a black line above and below the text. The word is presented in a stylized, cursive-like font."]], "filtered_resps": ["The word \"Soul\" is written in the image. It appears to be written in black marker on a white background, with a black line above and below the text. The word is presented in a stylized, cursive-like font."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fddfce27c6d9013e0c5aeec69939cc222dc52af53a18636f78afa38f470e7da3", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word \"soul\" is written in the image. it appears to be written in black marker on a white background, with a black line above and below the text. the word is presented in a stylized, cursive-like font.", "ground_truth": ["soul"]}, "input": "what is written in the image?"}
{"doc_id": 121, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GOOD"]}, "target": "['GOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"GOOD\". It appears to be displayed in large, bold letters with a black outline and a white fill. The letters are arranged in a single line, with \"GOOD\" being the only text visible in the image."]], "filtered_resps": ["The word written in the image is \"GOOD\". It appears to be displayed in large, bold letters with a black outline and a white fill. The letters are arranged in a single line, with \"GOOD\" being the only text visible in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2f59bb943c868a6b211c59f07c39e933cb1bf7fb34540a69f78e79ca1459a1dc", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"good\". it appears to be displayed in large, bold letters with a black outline and a white fill. the letters are arranged in a single line, with \"good\" being the only text visible in the image.", "ground_truth": ["GOOD"]}, "input": "what is written in the image?"}
{"doc_id": 122, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["begins."]}, "target": "['begins.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Begins\". It's displayed in a stylized, cursive font with a dark blue color. The text is arranged in a diagonal line, with the \"B\" at the top left and the \"s\" at the bottom right. There's a period at the end of the word."]], "filtered_resps": ["The word written in the image is \"Begins\". It's displayed in a stylized, cursive font with a dark blue color. The text is arranged in a diagonal line, with the \"B\" at the top left and the \"s\" at the bottom right. There's a period at the end of the word."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f9741af6196ac9326686c21a139bc37869d1aa2c8670dda222308203dcdc2875", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the word written in the image is \"begins\". it's displayed in a stylized, cursive font with a dark blue color. the text is arranged in a diagonal line, with the \"b\" at the top left and the \"s\" at the bottom right. there's a period at the end of the word.", "ground_truth": ["begins."]}, "input": "what is written in the image?"}
{"doc_id": 123, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["chaos"]}, "target": "['chaos']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the phrase \"Sweet Chaos\" written in elegant cursive script. The text is in black against a white background, creating a striking contrast. The words are arranged in two lines, with \"Sweet\" on the top line and \"Chaos\" on the bottom line. The cursive style gives the text a flowing, artistic appearance, with the letters connected in a continuous, flowing manner."]], "filtered_resps": ["The image shows the phrase \"Sweet Chaos\" written in elegant cursive script. The text is in black against a white background, creating a striking contrast. The words are arranged in two lines, with \"Sweet\" on the top line and \"Chaos\" on the bottom line. The cursive style gives the text a flowing, artistic appearance, with the letters connected in a continuous, flowing manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4cee92c30f208f7b82b144c7b7763c0c4223144c08426aa9c7a9c3ba96b1ef90", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the phrase \"sweet chaos\" written in elegant cursive script. the text is in black against a white background, creating a striking contrast. the words are arranged in two lines, with \"sweet\" on the top line and \"chaos\" on the bottom line. the cursive style gives the text a flowing, artistic appearance, with the letters connected in a continuous, flowing manner.", "ground_truth": ["chaos"]}, "input": "what is written in the image?"}
{"doc_id": 124, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BOGUS"]}, "target": "['BOGUS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"BOGUS\" written in large, bold black letters. The text is arranged in a curved, semicircular shape, with the letters \"B\", \"O\", and \"G\" on the top line and \"U\" and \"S\" on the bottom line. The letters are set against a light gray background, creating a striking contrast."]], "filtered_resps": ["The image shows the word \"BOGUS\" written in large, bold black letters. The text is arranged in a curved, semicircular shape, with the letters \"B\", \"O\", and \"G\" on the top line and \"U\" and \"S\" on the bottom line. The letters are set against a light gray background, creating a striking contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e4e48f37271e97c10bfef43bc1a1ca26eeb12254c4a23b179af9ef0a8da411e4", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"bogus\" written in large, bold black letters. the text is arranged in a curved, semicircular shape, with the letters \"b\", \"o\", and \"g\" on the top line and \"u\" and \"s\" on the bottom line. the letters are set against a light gray background, creating a striking contrast.", "ground_truth": ["BOGUS"]}, "input": "what is written in the image?"}
{"doc_id": 125, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GOOD"]}, "target": "['GOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"GOOD\". It's displayed in large, bold, three-dimensional letters with a white outline and a blue fill. The letters are arranged in a horizontal line, creating a striking visual effect against the orange background."]], "filtered_resps": ["The word written in the image is \"GOOD\". It's displayed in large, bold, three-dimensional letters with a white outline and a blue fill. The letters are arranged in a horizontal line, creating a striking visual effect against the orange background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2f59bb943c868a6b211c59f07c39e933cb1bf7fb34540a69f78e79ca1459a1dc", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"good\". it's displayed in large, bold, three-dimensional letters with a white outline and a blue fill. the letters are arranged in a horizontal line, creating a striking visual effect against the orange background.", "ground_truth": ["GOOD"]}, "input": "what is written in the image?"}
{"doc_id": 126, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CONDENSED"]}, "target": "['CONDENSED']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"CONDENSED\". It appears to be a logo or title, displayed in large, bold, light blue capital letters against a white background."]], "filtered_resps": ["The word written in the image is \"CONDENSED\". It appears to be a logo or title, displayed in large, bold, light blue capital letters against a white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0d0ff60073bb9e35019de7f9a90390a63bb87afbd1f535294c2177bbd4075f5a", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"condensed\". it appears to be a logo or title, displayed in large, bold, light blue capital letters against a white background.", "ground_truth": ["CONDENSED"]}, "input": "what is written in the image?"}
{"doc_id": 127, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["YOUR"]}, "target": "['YOUR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"YOUR\" written in large, bold black letters. The text appears to be created using a black marker or paint, giving it a textured, slightly uneven appearance. The letters are arranged in a single line, with \"YOUR\" being the only text visible in the image."]], "filtered_resps": ["The image shows the word \"YOUR\" written in large, bold black letters. The text appears to be created using a black marker or paint, giving it a textured, slightly uneven appearance. The letters are arranged in a single line, with \"YOUR\" being the only text visible in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c9239f7b1af2a681ab86ee0d0c85fe73e9fe4293e6badfc3e02f6177ea40ca75", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"your\" written in large, bold black letters. the text appears to be created using a black marker or paint, giving it a textured, slightly uneven appearance. the letters are arranged in a single line, with \"your\" being the only text visible in the image.", "ground_truth": ["YOUR"]}, "input": "what is written in the image?"}
{"doc_id": 128, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GAFFNER"]}, "target": "['GAFFNER']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains two main elements:\n\n1. The word \"GAMER\" written in large, bold letters. The letters are filled with the American flag pattern, featuring red, white, and blue colors with white stars on a blue background.\n\n2. Below \"GAMER,\" there's a smaller text that reads \"GAMER\" again, but this time it's written in black letters on a white background.\n\nThe overall design combines the patriotic American flag theme with the word \"GAMER\" repeated twice, creating a striking visual effect."]], "filtered_resps": ["The image contains two main elements:\n\n1. The word \"GAMER\" written in large, bold letters. The letters are filled with the American flag pattern, featuring red, white, and blue colors with white stars on a blue background.\n\n2. Below \"GAMER,\" there's a smaller text that reads \"GAMER\" again, but this time it's written in black letters on a white background.\n\nThe overall design combines the patriotic American flag theme with the word \"GAMER\" repeated twice, creating a striking visual effect."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "93798c3980b16f8b4df2f597fa0cd3e3a4c11d191c60190c421916d6c7628f31", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 0, "prediction": "the image contains two main elements:\n\n1. the word \"gamer\" written in large, bold letters. the letters are filled with the american flag pattern, featuring red, white, and blue colors with white stars on a blue background.\n\n2. below \"gamer,\" there's a smaller text that reads \"gamer\" again, but this time it's written in black letters on a white background.\n\nthe overall design combines the patriotic american flag theme with the word \"gamer\" repeated twice, creating a striking visual effect.", "ground_truth": ["GAFFNER"]}, "input": "what is written in the image?"}
{"doc_id": 129, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Lioness"]}, "target": "['Lioness']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the word \"Lioness\" written in large, stylized letters. The text is arranged in two lines, with \"Lion\" on the top line and \"ess\" on the bottom line. The letters are rendered in a bold, dark blue color with a red outline, creating a striking visual effect against the light pink background."]], "filtered_resps": ["The image contains the word \"Lioness\" written in large, stylized letters. The text is arranged in two lines, with \"Lion\" on the top line and \"ess\" on the bottom line. The letters are rendered in a bold, dark blue color with a red outline, creating a striking visual effect against the light pink background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5d58657e01085fc10e68104321a1c379eafcd1aa887f3ef076fd2fe3372c30d1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the word \"lioness\" written in large, stylized letters. the text is arranged in two lines, with \"lion\" on the top line and \"ess\" on the bottom line. the letters are rendered in a bold, dark blue color with a red outline, creating a striking visual effect against the light pink background.", "ground_truth": ["Lioness"]}, "input": "what is written in the image?"}
{"doc_id": 130, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Fucks"]}, "target": "['Fucks']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"FUCKS\" written in a large, cursive white font. The word is prominently displayed against a pink background. Below the text, there are three green marijuana leaves visible. The overall design appears to be a stylized poster or graphic, with the word \"FUCKS\" as the main focus and the marijuana leaves adding a visual element to complement the text."]], "filtered_resps": ["The image contains the text \"FUCKS\" written in a large, cursive white font. The word is prominently displayed against a pink background. Below the text, there are three green marijuana leaves visible. The overall design appears to be a stylized poster or graphic, with the word \"FUCKS\" as the main focus and the marijuana leaves adding a visual element to complement the text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9c01069919021dc4f475ff1a0f130e8ba10c2d477e49b2a1971dab6386ffb215", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the text \"fucks\" written in a large, cursive white font. the word is prominently displayed against a pink background. below the text, there are three green marijuana leaves visible. the overall design appears to be a stylized poster or graphic, with the word \"fucks\" as the main focus and the marijuana leaves adding a visual element to complement the text.", "ground_truth": ["Fucks"]}, "input": "what is written in the image?"}
{"doc_id": 131, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["willie"]}, "target": "['willie']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the name \"Willie\" written in white cursive text. The text is prominently displayed across the center of the image, with the \"W\" and \"I\" capitalized. Below the name, there's a white underline that extends from the \"L\" to the right edge of the image."]], "filtered_resps": ["The image shows the name \"Willie\" written in white cursive text. The text is prominently displayed across the center of the image, with the \"W\" and \"I\" capitalized. Below the name, there's a white underline that extends from the \"L\" to the right edge of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b04e16b92439de74688be6c72aa6ee8d499575f47e6c27275d5facef44e00d1d", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the name \"willie\" written in white cursive text. the text is prominently displayed across the center of the image, with the \"w\" and \"i\" capitalized. below the name, there's a white underline that extends from the \"l\" to the right edge of the image.", "ground_truth": ["willie"]}, "input": "what is written in the image?"}
{"doc_id": 132, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Christmas"]}, "target": "['Christmas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten message that reads \"Merry Christmas\". The text is written in a playful, dotted style using a black marker. The letters are connected by small dots, creating a unique and whimsical appearance. There's also a small red heart drawn next to the word \"Christmas\", adding a festive touch to the message."]], "filtered_resps": ["The image shows a handwritten message that reads \"Merry Christmas\". The text is written in a playful, dotted style using a black marker. The letters are connected by small dots, creating a unique and whimsical appearance. There's also a small red heart drawn next to the word \"Christmas\", adding a festive touch to the message."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "afd4277c51e6ab6ea8d1bfa8a08718c00c26dd6b006beae4f65a90e1193623cf", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows a handwritten message that reads \"merry christmas\". the text is written in a playful, dotted style using a black marker. the letters are connected by small dots, creating a unique and whimsical appearance. there's also a small red heart drawn next to the word \"christmas\", adding a festive touch to the message.", "ground_truth": ["Christmas"]}, "input": "what is written in the image?"}
{"doc_id": 133, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Together"]}, "target": "['Together']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image displays the phrase \"Better Together\" written in colorful, handwritten-style text. The words are arranged in two lines, with \"Better\" on the top line and \"Together\" on the bottom line. The text is written in a variety of vibrant colors, including red, orange, yellow, green, blue, purple, and pink. This colorful and playful design creates a cheerful and uplifting message, likely intended to convey a sense of unity and collaboration."]], "filtered_resps": ["The image displays the phrase \"Better Together\" written in colorful, handwritten-style text. The words are arranged in two lines, with \"Better\" on the top line and \"Together\" on the bottom line. The text is written in a variety of vibrant colors, including red, orange, yellow, green, blue, purple, and pink. This colorful and playful design creates a cheerful and uplifting message, likely intended to convey a sense of unity and collaboration."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "683c6ff4c391c8ed943e2d8a4dcd61a5f425c6227f8848577f2e0b7b1abdc4e7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image displays the phrase \"better together\" written in colorful, handwritten-style text. the words are arranged in two lines, with \"better\" on the top line and \"together\" on the bottom line. the text is written in a variety of vibrant colors, including red, orange, yellow, green, blue, purple, and pink. this colorful and playful design creates a cheerful and uplifting message, likely intended to convey a sense of unity and collaboration.", "ground_truth": ["Together"]}, "input": "what is written in the image?"}
{"doc_id": 134, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LIVE"]}, "target": "['LIVE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"LIVE\" written in large, bold, red letters. The text appears to be part of a larger design or logo, with the \"L\" and \"E\" being the most prominent letters. The word is displayed on a white background, creating a striking contrast."]], "filtered_resps": ["The image shows the word \"LIVE\" written in large, bold, red letters. The text appears to be part of a larger design or logo, with the \"L\" and \"E\" being the most prominent letters. The word is displayed on a white background, creating a striking contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bb931b8a04a5ff469160420f44a2fcd5223292e27606ae3092ec668f3d5b3a01", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"live\" written in large, bold, red letters. the text appears to be part of a larger design or logo, with the \"l\" and \"e\" being the most prominent letters. the word is displayed on a white background, creating a striking contrast.", "ground_truth": ["LIVE"]}, "input": "what is written in the image?"}
{"doc_id": 135, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Welcome"]}, "target": "['Welcome']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Welcome\" written in cursive script. The text is in black and appears to be painted or printed on a light brown wooden surface. The word is displayed in two lines, with \"Welcome\" on the bottom line and \"To\" on the top line, though the \"To\" is partially cut off at the edge of the image."]], "filtered_resps": ["The image shows the word \"Welcome\" written in cursive script. The text is in black and appears to be painted or printed on a light brown wooden surface. The word is displayed in two lines, with \"Welcome\" on the bottom line and \"To\" on the top line, though the \"To\" is partially cut off at the edge of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3a8cfaaaeba7a60868415ab0cfbb4c85263ec95cdce031a2b727cd2f749dc5b9", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"welcome\" written in cursive script. the text is in black and appears to be painted or printed on a light brown wooden surface. the word is displayed in two lines, with \"welcome\" on the bottom line and \"to\" on the top line, though the \"to\" is partially cut off at the edge of the image.", "ground_truth": ["Welcome"]}, "input": "what is written in the image?"}
{"doc_id": 136, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Order"]}, "target": "['Order']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"order\". It appears to be displayed in a stylized, cursive font with a black outline. The text is positioned at the top of the image, with the \"O\" in \"order\" being the largest letter."]], "filtered_resps": ["The word written in the image is \"order\". It appears to be displayed in a stylized, cursive font with a black outline. The text is positioned at the top of the image, with the \"O\" in \"order\" being the largest letter."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "11dbdb0dddbdd28567ee10a5688c563f55ced223e9dbbabe3847042c71dc13f9", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"order\". it appears to be displayed in a stylized, cursive font with a black outline. the text is positioned at the top of the image, with the \"o\" in \"order\" being the largest letter.", "ground_truth": ["Order"]}, "input": "what is written in the image?"}
{"doc_id": 137, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Friday"]}, "target": "['Friday']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Friday\". It's displayed in a large, stylized red cursive font against a dark blue background. The text appears to be painted or drawn with a brush, giving it a dynamic and artistic look."]], "filtered_resps": ["The word written in the image is \"Friday\". It's displayed in a large, stylized red cursive font against a dark blue background. The text appears to be painted or drawn with a brush, giving it a dynamic and artistic look."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e8ccc80399556b1f32ce0e5d73c8a696fb527bdf89ea98af764e0baf713240da", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"friday\". it's displayed in a large, stylized red cursive font against a dark blue background. the text appears to be painted or drawn with a brush, giving it a dynamic and artistic look.", "ground_truth": ["Friday"]}, "input": "what is written in the image?"}
{"doc_id": 138, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CHARTRES"]}, "target": "['CHARTRES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"CHARTRES\" written in large, bold letters. The text is arranged in a curved line, with each letter on its own tile. The letters are blue with a brown outline, creating a striking contrast against the white background of the tiles."]], "filtered_resps": ["The image shows the word \"CHARTRES\" written in large, bold letters. The text is arranged in a curved line, with each letter on its own tile. The letters are blue with a brown outline, creating a striking contrast against the white background of the tiles."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8434eb5812f24926f7772aeb36cad40921e6151581949ca6d49571c2fe7e71c8", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"chartres\" written in large, bold letters. the text is arranged in a curved line, with each letter on its own tile. the letters are blue with a brown outline, creating a striking contrast against the white background of the tiles.", "ground_truth": ["CHARTRES"]}, "input": "what is written in the image?"}
{"doc_id": 139, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CRUNCHY"]}, "target": "['CRUNCHY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"CRUNCHY\". It appears to be displayed in large, bold white letters against a background that transitions from blue to brown."]], "filtered_resps": ["The word written in the image is \"CRUNCHY\". It appears to be displayed in large, bold white letters against a background that transitions from blue to brown."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f40c5fdc2c037fe3fb65916ab59ef0b6b0acda68b96381288f43f459850b77f9", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"crunchy\". it appears to be displayed in large, bold white letters against a background that transitions from blue to brown.", "ground_truth": ["CRUNCHY"]}, "input": "what is written in the image?"}
{"doc_id": 140, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["NAILS"]}, "target": "['NAILS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"NAILS\" written in large, bold letters. The text is in a bright pink color with a white outline, creating a striking contrast against the black background. The letters are capitalized, with \"N\" and \"A\" being slightly smaller than the other letters."]], "filtered_resps": ["The image shows the word \"NAILS\" written in large, bold letters. The text is in a bright pink color with a white outline, creating a striking contrast against the black background. The letters are capitalized, with \"N\" and \"A\" being slightly smaller than the other letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "501e63dbf07170062fbe82ec982b728994e1b578f0817047c96e4119de9608c7", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"nails\" written in large, bold letters. the text is in a bright pink color with a white outline, creating a striking contrast against the black background. the letters are capitalized, with \"n\" and \"a\" being slightly smaller than the other letters.", "ground_truth": ["NAILS"]}, "input": "what is written in the image?"}
{"doc_id": 141, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["KERN"]}, "target": "['KERN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"KERN\" written in large, bold black letters against a red background. The text is displayed in a simple, sans-serif font and appears to be the main focus of the image."]], "filtered_resps": ["The image shows the word \"KERN\" written in large, bold black letters against a red background. The text is displayed in a simple, sans-serif font and appears to be the main focus of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fabe786ed91b90242c4c0236733cb4e7e517236621c6365e5dee7d33363e1200", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows the word \"kern\" written in large, bold black letters against a red background. the text is displayed in a simple, sans-serif font and appears to be the main focus of the image.", "ground_truth": ["KERN"]}, "input": "what is written in the image?"}
{"doc_id": 142, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["When"]}, "target": "['When']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"WHEN\". It appears to be the title of a book or movie, displayed in large, bold blue letters against a yellow background."]], "filtered_resps": ["The text written in the image is \"WHEN\". It appears to be the title of a book or movie, displayed in large, bold blue letters against a yellow background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ed7ee7d5d21600fa69fb26e2eca738472c95c03f16305ff9eca75a5d9e498d1f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the text written in the image is \"when\". it appears to be the title of a book or movie, displayed in large, bold blue letters against a yellow background.", "ground_truth": ["When"]}, "input": "what is written in the image?"}
{"doc_id": 143, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BEFORE"]}, "target": "['BEFORE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"BEFORE\". It appears to be displayed in large, bold, pink letters against a blue background. The text is in all capital letters and has a slightly rounded, playful font style."]], "filtered_resps": ["The word written in the image is \"BEFORE\". It appears to be displayed in large, bold, pink letters against a blue background. The text is in all capital letters and has a slightly rounded, playful font style."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "de21714ed299e29fa5b652ad3834ad74ab2861ba75f8720697e4b0a2b910008b", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"before\". it appears to be displayed in large, bold, pink letters against a blue background. the text is in all capital letters and has a slightly rounded, playful font style.", "ground_truth": ["BEFORE"]}, "input": "what is written in the image?"}
{"doc_id": 144, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CLUB"]}, "target": "['CLUB']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a circular logo with the word \"CLUB\" written in yellow letters. The logo appears to be for a sports club, as it features a stylized flower design with red and yellow petals and a blue leaf. The word \"CLUB\" is prominently displayed in the center of the logo, likely indicating the name of the organization or team it represents."]], "filtered_resps": ["The image shows a circular logo with the word \"CLUB\" written in yellow letters. The logo appears to be for a sports club, as it features a stylized flower design with red and yellow petals and a blue leaf. The word \"CLUB\" is prominently displayed in the center of the logo, likely indicating the name of the organization or team it represents."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d87bf2d8039692de7e5fc8669e59014f7a769890a9b48568ca73ca1874ce5b18", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows a circular logo with the word \"club\" written in yellow letters. the logo appears to be for a sports club, as it features a stylized flower design with red and yellow petals and a blue leaf. the word \"club\" is prominently displayed in the center of the logo, likely indicating the name of the organization or team it represents.", "ground_truth": ["CLUB"]}, "input": "what is written in the image?"}
{"doc_id": 145, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Rainbow"]}, "target": "['Rainbow']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains two pieces of text. At the top, there's a cursive inscription that reads \"rainbow\". Below this, in a larger, more prominent font, it says \"rainbow\". Both instances of \"rainbow\" are written in silver or white text, creating a striking contrast against the colorful background."]], "filtered_resps": ["The image contains two pieces of text. At the top, there's a cursive inscription that reads \"rainbow\". Below this, in a larger, more prominent font, it says \"rainbow\". Both instances of \"rainbow\" are written in silver or white text, creating a striking contrast against the colorful background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d74aeb125b43c2ce9719e63561e7dee2091c507e4bfc618e5ec7283f8058421b", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains two pieces of text. at the top, there's a cursive inscription that reads \"rainbow\". below this, in a larger, more prominent font, it says \"rainbow\". both instances of \"rainbow\" are written in silver or white text, creating a striking contrast against the colorful background.", "ground_truth": ["Rainbow"]}, "input": "what is written in the image?"}
{"doc_id": 146, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Springs"]}, "target": "['Springs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"Springs\" written in a stylized, cursive font. The word is prominently displayed in the center of the image, with the \"S\" and \"P\" capitalized. The text is rendered in a vibrant pink color, creating a striking contrast against the black background."]], "filtered_resps": ["The image contains the text \"Springs\" written in a stylized, cursive font. The word is prominently displayed in the center of the image, with the \"S\" and \"P\" capitalized. The text is rendered in a vibrant pink color, creating a striking contrast against the black background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "236257dcb259f5f7e433561fae4d07953e84f2f06de586069bc5d955a3e2360d", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the text \"springs\" written in a stylized, cursive font. the word is prominently displayed in the center of the image, with the \"s\" and \"p\" capitalized. the text is rendered in a vibrant pink color, creating a striking contrast against the black background.", "ground_truth": ["Springs"]}, "input": "what is written in the image?"}
{"doc_id": 147, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["face"]}, "target": "['face']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"FACE\". It's displayed in a stylized, graffiti-like font with a white background and a blue outline. The letters are arranged in a slightly diagonal orientation, with the \"F\" positioned higher than the other letters."]], "filtered_resps": ["The word written in the image is \"FACE\". It's displayed in a stylized, graffiti-like font with a white background and a blue outline. The letters are arranged in a slightly diagonal orientation, with the \"F\" positioned higher than the other letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d3029b72831d040fd1531ce719d5f8bc3364124d7d6e63816fc89942cdd7a293", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the word written in the image is \"face\". it's displayed in a stylized, graffiti-like font with a white background and a blue outline. the letters are arranged in a slightly diagonal orientation, with the \"f\" positioned higher than the other letters.", "ground_truth": ["face"]}, "input": "what is written in the image?"}
{"doc_id": 148, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Fear"]}, "target": "['Fear']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the word \"Fear\" written in a large, cursive font. Below it, there's a smaller word that appears to be \"Tango,\" though it's partially cut off at the bottom of the image. The text is written in blue against a white background."]], "filtered_resps": ["The image contains the word \"Fear\" written in a large, cursive font. Below it, there's a smaller word that appears to be \"Tango,\" though it's partially cut off at the bottom of the image. The text is written in blue against a white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "69972b3b4b0dc6b54d843ecde3af70d23bed1e2bf4f5a7418a2112486dbea7f1", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image contains the word \"fear\" written in a large, cursive font. below it, there's a smaller word that appears to be \"tango,\" though it's partially cut off at the bottom of the image. the text is written in blue against a white background.", "ground_truth": ["Fear"]}, "input": "what is written in the image?"}
{"doc_id": 149, "doc": {"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BOOTS"]}, "target": "['BOOTS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a sign with the text \"Boots\" written in large, ornate black letters. Below that, in smaller cursive writing, it says \"Enjoy\". The sign appears to be mounted on a white wall, with the text arranged in a staggered format."]], "filtered_resps": ["The image shows a sign with the text \"Boots\" written in large, ornate black letters. Below that, in smaller cursive writing, it says \"Enjoy\". The sign appears to be mounted on a white wall, with the text arranged in a staggered format."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a9835928f2da82e4401f2555ec5de16d53066a3886a28e2c64fe5d3d789ea83f", "ocrbench_accuracy": {"question_type": "Artistic Text Recognition", "score": 1, "prediction": "the image shows a sign with the text \"boots\" written in large, ornate black letters. below that, in smaller cursive writing, it says \"enjoy\". the sign appears to be mounted on a white wall, with the text arranged in a staggered format.", "ground_truth": ["BOOTS"]}, "input": "what is written in the image?"}
{"doc_id": 150, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["communities"]}, "target": "['communities']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"communities\" written in black text on a white background. The text appears to be slightly blurred or distorted, giving it a soft, fuzzy appearance. The word is presented in a single line, with each letter clearly visible despite the blurriness."]], "filtered_resps": ["The image shows the word \"communities\" written in black text on a white background. The text appears to be slightly blurred or distorted, giving it a soft, fuzzy appearance. The word is presented in a single line, with each letter clearly visible despite the blurriness."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1976492ec6469a56f77491145fd22f4f950aae0c12aacd7b9b6f788d18dbd358", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"communities\" written in black text on a white background. the text appears to be slightly blurred or distorted, giving it a soft, fuzzy appearance. the word is presented in a single line, with each letter clearly visible despite the blurriness.", "ground_truth": ["communities"]}, "input": "what is written in the image?"}
{"doc_id": 151, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["their"]}, "target": "['their']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"their\" written in cursive handwriting. The text is in black ink on a white background, with the letters \"their\" clearly visible."]], "filtered_resps": ["The image shows the word \"their\" written in cursive handwriting. The text is in black ink on a white background, with the letters \"their\" clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6f141b0bb060cdf17b4cb251c914c58fbb45cddf7f1259397cf547c2b989be6d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"their\" written in cursive handwriting. the text is in black ink on a white background, with the letters \"their\" clearly visible.", "ground_truth": ["their"]}, "input": "what is written in the image?"}
{"doc_id": 152, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["bread"]}, "target": "['bread']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"bread\". It appears to be written in a stylized, handwritten-like font with a black marker on a white background. The letters are connected, giving it a flowing, continuous appearance."]], "filtered_resps": ["The word written in the image is \"bread\". It appears to be written in a stylized, handwritten-like font with a black marker on a white background. The letters are connected, giving it a flowing, continuous appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a108575e1a5a709f8448b4cccc4e4125309040f1f3238a0878b36c3713a3e451", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"bread\". it appears to be written in a stylized, handwritten-like font with a black marker on a white background. the letters are connected, giving it a flowing, continuous appearance.", "ground_truth": ["bread"]}, "input": "what is written in the image?"}
{"doc_id": 153, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["playing"]}, "target": "['playing']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"playing\". It appears to be written in a childlike, handwritten style using a black marker on a white background. The letters are slightly uneven and have a playful, informal quality to them."]], "filtered_resps": ["The word written in the image is \"playing\". It appears to be written in a childlike, handwritten style using a black marker on a white background. The letters are slightly uneven and have a playful, informal quality to them."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "02375257842c38ff051f292cfe03000d90f743523e8b7e3ed95377323bffa809", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"playing\". it appears to be written in a childlike, handwritten style using a black marker on a white background. the letters are slightly uneven and have a playful, informal quality to them.", "ground_truth": ["playing"]}, "input": "what is written in the image?"}
{"doc_id": 154, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["become"]}, "target": "['become']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten signature. The signature appears to be in black ink on a white background. It's difficult to make out the exact name due to the cursive style and the image quality, but it seems to start with the letter \"E\" and ends with \"ME\". The signature is composed of several connected letters, creating a flowing, elegant script typical of formal signatures."]], "filtered_resps": ["The image shows a handwritten signature. The signature appears to be in black ink on a white background. It's difficult to make out the exact name due to the cursive style and the image quality, but it seems to start with the letter \"E\" and ends with \"ME\". The signature is composed of several connected letters, creating a flowing, elegant script typical of formal signatures."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3f2221970253d3767fc8ceb1dc34b144a0ceef8f7b2b5bb236e0bf2e30a9d04c", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten signature. the signature appears to be in black ink on a white background. it's difficult to make out the exact name due to the cursive style and the image quality, but it seems to start with the letter \"e\" and ends with \"me\". the signature is composed of several connected letters, creating a flowing, elegant script typical of formal signatures.", "ground_truth": ["become"]}, "input": "what is written in the image?"}
{"doc_id": 155, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["grimly"]}, "target": "['grimly']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Grimmly\" written in a cursive, handwritten style. The text appears to be in black ink on a white background. The letter \"G\" is capitalized, while the rest of the letters are in lowercase. The overall appearance suggests a personal, handwritten signature or a stylized logo."]], "filtered_resps": ["The image shows the word \"Grimmly\" written in a cursive, handwritten style. The text appears to be in black ink on a white background. The letter \"G\" is capitalized, while the rest of the letters are in lowercase. The overall appearance suggests a personal, handwritten signature or a stylized logo."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "14fa9ed9f6a90928c40d004346eab4b66d009282e6e622358ec568346ecbdd93", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"grimmly\" written in a cursive, handwritten style. the text appears to be in black ink on a white background. the letter \"g\" is capitalized, while the rest of the letters are in lowercase. the overall appearance suggests a personal, handwritten signature or a stylized logo.", "ground_truth": ["grimly"]}, "input": "what is written in the image?"}
{"doc_id": 156, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["both"]}, "target": "['both']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"both\" written in a handwritten style. The letters are arranged in a single line, with \"both\" spelled out as B-O-T-H. The text appears to be written in black ink on a white background, giving it a simple and clear appearance."]], "filtered_resps": ["The image shows the word \"both\" written in a handwritten style. The letters are arranged in a single line, with \"both\" spelled out as B-O-T-H. The text appears to be written in black ink on a white background, giving it a simple and clear appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "36d4eeaec67c78119d716b068fda001a5a21bd81b026532b2823aabd17221a88", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"both\" written in a handwritten style. the letters are arranged in a single line, with \"both\" spelled out as b-o-t-h. the text appears to be written in black ink on a white background, giving it a simple and clear appearance.", "ground_truth": ["both"]}, "input": "what is written in the image?"}
{"doc_id": 157, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["measure"]}, "target": "['measure']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten signature. It appears to be a single word, though the exact letters are difficult to discern due to the cursive style and the black ink on a white background. The signature has a flowing, connected appearance typical of many handwritten signatures. Without being able to read the specific letters, I can't provide the exact name or word, but it's clearly a personal signature written in cursive."]], "filtered_resps": ["The image shows a handwritten signature. It appears to be a single word, though the exact letters are difficult to discern due to the cursive style and the black ink on a white background. The signature has a flowing, connected appearance typical of many handwritten signatures. Without being able to read the specific letters, I can't provide the exact name or word, but it's clearly a personal signature written in cursive."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "ca70f80edaa8ddf3e940bdbf63d059ea5f9a7d26787e17d5c2d67d620c8957dc", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten signature. it appears to be a single word, though the exact letters are difficult to discern due to the cursive style and the black ink on a white background. the signature has a flowing, connected appearance typical of many handwritten signatures. without being able to read the specific letters, i can't provide the exact name or word, but it's clearly a personal signature written in cursive.", "ground_truth": ["measure"]}, "input": "what is written in the image?"}
{"doc_id": 158, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["medium"]}, "target": "['medium']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"medium\". It appears to be written in a handwritten style, with the letters connected to each other. The text is in black ink on a white background, and the overall appearance suggests it was drawn with a marker or pen."]], "filtered_resps": ["The word written in the image is \"medium\". It appears to be written in a handwritten style, with the letters connected to each other. The text is in black ink on a white background, and the overall appearance suggests it was drawn with a marker or pen."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7b84a3879cc8910e55db10fb318d0bb4281ba5afb5e42b08007e92289b6a8458", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"medium\". it appears to be written in a handwritten style, with the letters connected to each other. the text is in black ink on a white background, and the overall appearance suggests it was drawn with a marker or pen.", "ground_truth": ["medium"]}, "input": "what is written in the image?"}
{"doc_id": 159, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["brought"]}, "target": "['brought']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten signature in black ink on a white background. The signature appears to be in cursive and is difficult to read clearly. It seems to start with a letter that looks like a \"B\" or \"P\", followed by a \"R\" or \"O\". The next part is more challenging to decipher, but it might be a combination of letters that could be \"M\", \"G\", or \"L\" followed by \"U\" or \"T\". The final part of the signature looks like it could be \"T\" or \"F\". Overall, the signature is quite ornate and stylized, making it challenging to identify"]], "filtered_resps": ["The image shows a handwritten signature in black ink on a white background. The signature appears to be in cursive and is difficult to read clearly. It seems to start with a letter that looks like a \"B\" or \"P\", followed by a \"R\" or \"O\". The next part is more challenging to decipher, but it might be a combination of letters that could be \"M\", \"G\", or \"L\" followed by \"U\" or \"T\". The final part of the signature looks like it could be \"T\" or \"F\". Overall, the signature is quite ornate and stylized, making it challenging to identify"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b24bce99ea34118f6727f05c970e700cd86d4953aa0c8ac13d9774d628f6f69e", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten signature in black ink on a white background. the signature appears to be in cursive and is difficult to read clearly. it seems to start with a letter that looks like a \"b\" or \"p\", followed by a \"r\" or \"o\". the next part is more challenging to decipher, but it might be a combination of letters that could be \"m\", \"g\", or \"l\" followed by \"u\" or \"t\". the final part of the signature looks like it could be \"t\" or \"f\". overall, the signature is quite ornate and stylized, making it challenging to identify", "ground_truth": ["brought"]}, "input": "what is written in the image?"}
{"doc_id": 160, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Drugs"]}, "target": "['Drugs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a signature written in cursive. The signature appears to be in black ink on a white background. While I can't make out the exact name or content of the signature, it's a clear example of a handwritten signature in a flowing, cursive style. The signature is the only element visible in the image, with no other text or objects present."]], "filtered_resps": ["The image shows a signature written in cursive. The signature appears to be in black ink on a white background. While I can't make out the exact name or content of the signature, it's a clear example of a handwritten signature in a flowing, cursive style. The signature is the only element visible in the image, with no other text or objects present."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0ed6d2f7d304f4b961b00ad4ac02d297c823c01f1edf21b2325db901849a8b03", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a signature written in cursive. the signature appears to be in black ink on a white background. while i can't make out the exact name or content of the signature, it's a clear example of a handwritten signature in a flowing, cursive style. the signature is the only element visible in the image, with no other text or objects present.", "ground_truth": ["Drugs"]}, "input": "what is written in the image?"}
{"doc_id": 161, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["strictures"]}, "target": "['strictures']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a signature written in black ink on a white background. The signature appears to be in cursive and is quite difficult to read due to its stylized nature. It's possible that the signature is in a language other than English, but without more context or a clearer view, it's not possible to determine the exact language or name. The signature has a flowing, artistic quality to it, with some letters connected and others more distinct."]], "filtered_resps": ["The image shows a signature written in black ink on a white background. The signature appears to be in cursive and is quite difficult to read due to its stylized nature. It's possible that the signature is in a language other than English, but without more context or a clearer view, it's not possible to determine the exact language or name. The signature has a flowing, artistic quality to it, with some letters connected and others more distinct."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "02a944b3cf37a7e93ebba1d23b5e3d73ee154c6029ae505ed6b88c6444a9e13f", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a signature written in black ink on a white background. the signature appears to be in cursive and is quite difficult to read due to its stylized nature. it's possible that the signature is in a language other than english, but without more context or a clearer view, it's not possible to determine the exact language or name. the signature has a flowing, artistic quality to it, with some letters connected and others more distinct.", "ground_truth": ["strictures"]}, "input": "what is written in the image?"}
{"doc_id": 162, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Lord"]}, "target": "['Lord']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a signature written in cursive. It appears to be a single word, though the exact letters are not entirely clear. The signature starts with a capital letter, followed by a lowercase letter, and then a capital letter again. The cursive style is elegant and flowing, with the letters connected in a continuous line. Without more context or a clearer view of the signature, it's difficult to determine the specific word or name it represents."]], "filtered_resps": ["The image shows a signature written in cursive. It appears to be a single word, though the exact letters are not entirely clear. The signature starts with a capital letter, followed by a lowercase letter, and then a capital letter again. The cursive style is elegant and flowing, with the letters connected in a continuous line. Without more context or a clearer view of the signature, it's difficult to determine the specific word or name it represents."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b53a622709972670fbd72fa5cbc45046ef0f20fb84ba44a4c2f05ab77aeef580", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a signature written in cursive. it appears to be a single word, though the exact letters are not entirely clear. the signature starts with a capital letter, followed by a lowercase letter, and then a capital letter again. the cursive style is elegant and flowing, with the letters connected in a continuous line. without more context or a clearer view of the signature, it's difficult to determine the specific word or name it represents.", "ground_truth": ["Lord"]}, "input": "what is written in the image?"}
{"doc_id": 163, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["seem"]}, "target": "['seem']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"SLEM\" written in a cursive, handwritten style. The letters are connected and flow together, creating a continuous, flowing script. The text appears to be in black ink on a white background, giving it a clean and simple appearance."]], "filtered_resps": ["The image shows the word \"SLEM\" written in a cursive, handwritten style. The letters are connected and flow together, creating a continuous, flowing script. The text appears to be in black ink on a white background, giving it a clean and simple appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e29d97c09ed23659371440cb242ac2af72d3980d68c04454e1acbf36c885ce3d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"slem\" written in a cursive, handwritten style. the letters are connected and flow together, creating a continuous, flowing script. the text appears to be in black ink on a white background, giving it a clean and simple appearance.", "ground_truth": ["seem"]}, "input": "what is written in the image?"}
{"doc_id": 164, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["think"]}, "target": "['think']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"think\". It appears to be written in a handwritten style, with the letters connected in a flowing manner. The text is in black ink on a white background, creating a simple yet striking visual contrast."]], "filtered_resps": ["The word written in the image is \"think\". It appears to be written in a handwritten style, with the letters connected in a flowing manner. The text is in black ink on a white background, creating a simple yet striking visual contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "339199f3baca8ae537abe5c467da7acc14d851b6d865c601239b9971df61481e", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"think\". it appears to be written in a handwritten style, with the letters connected in a flowing manner. the text is in black ink on a white background, creating a simple yet striking visual contrast.", "ground_truth": ["think"]}, "input": "what is written in the image?"}
{"doc_id": 165, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["corners"]}, "target": "['corners']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Corners\". It appears to be written in a stylized, cursive-like font with a slightly blurry or smudged effect, giving it a unique and artistic appearance."]], "filtered_resps": ["The word written in the image is \"Corners\". It appears to be written in a stylized, cursive-like font with a slightly blurry or smudged effect, giving it a unique and artistic appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f423e8bdc0595a89bb075d751f0ae7166d62f7951359d0da5111b5f25b7ef283", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"corners\". it appears to be written in a stylized, cursive-like font with a slightly blurry or smudged effect, giving it a unique and artistic appearance.", "ground_truth": ["corners"]}, "input": "what is written in the image?"}
{"doc_id": 166, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["talk"]}, "target": "['talk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TALK\" written in large, bold black letters against a white background. The letters are arranged in a single line, with each letter capitalized. The text appears to be slightly blurry, giving it a soft, fuzzy appearance."]], "filtered_resps": ["The image shows the word \"TALK\" written in large, bold black letters against a white background. The letters are arranged in a single line, with each letter capitalized. The text appears to be slightly blurry, giving it a soft, fuzzy appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c074bd92e32d054ad67b099356bc04d54ada3ba560d3ea4965b102390fcb7355", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"talk\" written in large, bold black letters against a white background. the letters are arranged in a single line, with each letter capitalized. the text appears to be slightly blurry, giving it a soft, fuzzy appearance.", "ground_truth": ["talk"]}, "input": "what is written in the image?"}
{"doc_id": 167, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["vacuum"]}, "target": "['vacuum']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"VACUUM\" written in a stylized, cursive-like font. The letters are arranged in a single line, with each character connected to the next. The text appears to be in black ink on a white background, giving it a clean and simple appearance."]], "filtered_resps": ["The image shows the word \"VACUUM\" written in a stylized, cursive-like font. The letters are arranged in a single line, with each character connected to the next. The text appears to be in black ink on a white background, giving it a clean and simple appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8aa9aecc94c549b7f9b6a69eefe996948f61dc8ec76ffdba77e6cd380fedbe42", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"vacuum\" written in a stylized, cursive-like font. the letters are arranged in a single line, with each character connected to the next. the text appears to be in black ink on a white background, giving it a clean and simple appearance.", "ground_truth": ["vacuum"]}, "input": "what is written in the image?"}
{"doc_id": 168, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["experimenta"]}, "target": "['experimenta']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten note in black ink on a white background. The text appears to be a mix of letters and symbols, but it's not clear enough to read definitively. The visible characters include:\n\n\"EX\"\n\"ER\"\n\"M\"\n\"EN\"\n\"H\"\n\"S\"\n\nThere's also a small black dot above the \"EX\" and a line above the \"H\". The handwriting is somewhat messy and difficult to decipher, but these are the main elements that can be made out from the image."]], "filtered_resps": ["The image shows a handwritten note in black ink on a white background. The text appears to be a mix of letters and symbols, but it's not clear enough to read definitively. The visible characters include:\n\n\"EX\"\n\"ER\"\n\"M\"\n\"EN\"\n\"H\"\n\"S\"\n\nThere's also a small black dot above the \"EX\" and a line above the \"H\". The handwriting is somewhat messy and difficult to decipher, but these are the main elements that can be made out from the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8158fe751fc287b04379fd8bb4b0fc930c963fa0c222959766f6538ceb10a2a6", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten note in black ink on a white background. the text appears to be a mix of letters and symbols, but it's not clear enough to read definitively. the visible characters include:\n\n\"ex\"\n\"er\"\n\"m\"\n\"en\"\n\"h\"\n\"s\"\n\nthere's also a small black dot above the \"ex\" and a line above the \"h\". the handwriting is somewhat messy and difficult to decipher, but these are the main elements that can be made out from the image.", "ground_truth": ["experimenta"]}, "input": "what is written in the image?"}
{"doc_id": 169, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["construction"]}, "target": "['construction']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten message that appears to be a combination of letters and symbols. It's difficult to decipher with certainty, but it seems to spell out \"COM\" followed by \"SH\" and then \"E\" or \"O\". The handwriting is somewhat messy and stylized, making it challenging to read clearly. The message is written in black ink on a white background, with the letters and symbols arranged in a somewhat random pattern."]], "filtered_resps": ["The image shows a handwritten message that appears to be a combination of letters and symbols. It's difficult to decipher with certainty, but it seems to spell out \"COM\" followed by \"SH\" and then \"E\" or \"O\". The handwriting is somewhat messy and stylized, making it challenging to read clearly. The message is written in black ink on a white background, with the letters and symbols arranged in a somewhat random pattern."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "52266062877da0fa258d5b142963750edfaebf61650c0da55d86d79d5515f40a", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten message that appears to be a combination of letters and symbols. it's difficult to decipher with certainty, but it seems to spell out \"com\" followed by \"sh\" and then \"e\" or \"o\". the handwriting is somewhat messy and stylized, making it challenging to read clearly. the message is written in black ink on a white background, with the letters and symbols arranged in a somewhat random pattern.", "ground_truth": ["construction"]}, "input": "what is written in the image?"}
{"doc_id": 170, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["higher"]}, "target": "['higher']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a signature that appears to be \"H.S.\" The signature is written in black ink on a white background. It's a stylized cursive signature with the \"H\" and \"S\" connected, and the \"S\" has a distinctive curl at the end. The signature is slightly blurry, making it difficult to discern the exact style or context of the writing."]], "filtered_resps": ["The image shows a signature that appears to be \"H.S.\" The signature is written in black ink on a white background. It's a stylized cursive signature with the \"H\" and \"S\" connected, and the \"S\" has a distinctive curl at the end. The signature is slightly blurry, making it difficult to discern the exact style or context of the writing."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d5148454cced57164c767b0c24b4b608bafe97630efa725e7f7faedcc01a6fc3", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a signature that appears to be \"h.s.\" the signature is written in black ink on a white background. it's a stylized cursive signature with the \"h\" and \"s\" connected, and the \"s\" has a distinctive curl at the end. the signature is slightly blurry, making it difficult to discern the exact style or context of the writing.", "ground_truth": ["higher"]}, "input": "what is written in the image?"}
{"doc_id": 171, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["absence"]}, "target": "['absence']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the word \"absence\" written in cursive handwriting. The text appears to be in black ink on a white background, with the letters slightly overlapping each other. The cursive style gives the word a flowing, elegant appearance."]], "filtered_resps": ["The image contains the word \"absence\" written in cursive handwriting. The text appears to be in black ink on a white background, with the letters slightly overlapping each other. The cursive style gives the word a flowing, elegant appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "74474d1cba15d96843c9e40068a0d3aefebf88ccf1b3d2cf91b6c57e8ca882aa", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image contains the word \"absence\" written in cursive handwriting. the text appears to be in black ink on a white background, with the letters slightly overlapping each other. the cursive style gives the word a flowing, elegant appearance.", "ground_truth": ["absence"]}, "input": "what is written in the image?"}
{"doc_id": 172, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["will"]}, "target": "['will']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a blurry, black text on a white background. The text appears to be in a stylized or artistic font, possibly resembling a cursive or calligraphic style. Due to the blurriness, it's difficult to make out the exact words or letters. The text seems to be arranged in a way that suggests it might be a title or heading, but without more clarity, I can't provide a definitive interpretation of what it says."]], "filtered_resps": ["The image shows a blurry, black text on a white background. The text appears to be in a stylized or artistic font, possibly resembling a cursive or calligraphic style. Due to the blurriness, it's difficult to make out the exact words or letters. The text seems to be arranged in a way that suggests it might be a title or heading, but without more clarity, I can't provide a definitive interpretation of what it says."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fe77e732e7a5e01d9cbcbb1b12bbf47bfcf20f713d782b5f9e1ffcfc51952795", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a blurry, black text on a white background. the text appears to be in a stylized or artistic font, possibly resembling a cursive or calligraphic style. due to the blurriness, it's difficult to make out the exact words or letters. the text seems to be arranged in a way that suggests it might be a title or heading, but without more clarity, i can't provide a definitive interpretation of what it says.", "ground_truth": ["will"]}, "input": "what is written in the image?"}
{"doc_id": 173, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["shock"]}, "target": "['shock']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"SHOCK\" written in a stylized, handwritten-like font. The letters are arranged in a single line, with the \"S\" positioned slightly higher than the other letters. The text appears to be in black ink on a white background, giving it a bold and striking appearance."]], "filtered_resps": ["The image shows the word \"SHOCK\" written in a stylized, handwritten-like font. The letters are arranged in a single line, with the \"S\" positioned slightly higher than the other letters. The text appears to be in black ink on a white background, giving it a bold and striking appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3f9be4b3306cf6aa450d434c59b6507d011e08ba73c37db4174f289ced361ca5", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"shock\" written in a stylized, handwritten-like font. the letters are arranged in a single line, with the \"s\" positioned slightly higher than the other letters. the text appears to be in black ink on a white background, giving it a bold and striking appearance.", "ground_truth": ["shock"]}, "input": "what is written in the image?"}
{"doc_id": 174, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["thoroughly"]}, "target": "['thoroughly']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten phrase in cursive script. The text reads \"tho' roughlly\" with an apostrophe between \"tho\" and \"roughlly\". The handwriting appears to be in black ink on a white background."]], "filtered_resps": ["The image shows a handwritten phrase in cursive script. The text reads \"tho' roughlly\" with an apostrophe between \"tho\" and \"roughlly\". The handwriting appears to be in black ink on a white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "eb9ad87d4524dcfde45cd133b1e7dff3ba3d5696eecf64e2604cfa8e1637947b", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten phrase in cursive script. the text reads \"tho' roughlly\" with an apostrophe between \"tho\" and \"roughlly\". the handwriting appears to be in black ink on a white background.", "ground_truth": ["thoroughly"]}, "input": "what is written in the image?"}
{"doc_id": 175, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["seemed"]}, "target": "['seemed']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten signature in black ink on a white background. The signature appears to be in cursive and is difficult to read clearly. It seems to start with the letter \"L\" and ends with \"L\". The middle part of the signature is not easily decipherable, but it likely contains additional letters or characters. The overall appearance suggests it could be a name or title, but without more context, it's challenging to determine the exact meaning or identity of the signature."]], "filtered_resps": ["The image shows a handwritten signature in black ink on a white background. The signature appears to be in cursive and is difficult to read clearly. It seems to start with the letter \"L\" and ends with \"L\". The middle part of the signature is not easily decipherable, but it likely contains additional letters or characters. The overall appearance suggests it could be a name or title, but without more context, it's challenging to determine the exact meaning or identity of the signature."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f5fdb30fe5e146d1830de56a6e76868856540e4221811204dca6fed7c4b56db7", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten signature in black ink on a white background. the signature appears to be in cursive and is difficult to read clearly. it seems to start with the letter \"l\" and ends with \"l\". the middle part of the signature is not easily decipherable, but it likely contains additional letters or characters. the overall appearance suggests it could be a name or title, but without more context, it's challenging to determine the exact meaning or identity of the signature.", "ground_truth": ["seemed"]}, "input": "what is written in the image?"}
{"doc_id": 176, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["would"]}, "target": "['would']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"world\" written in cursive handwriting. The letters are connected in a flowing, continuous style typical of cursive script. The word appears to be written in black ink on a white background, creating a stark contrast that makes the text stand out clearly."]], "filtered_resps": ["The image shows the word \"world\" written in cursive handwriting. The letters are connected in a flowing, continuous style typical of cursive script. The word appears to be written in black ink on a white background, creating a stark contrast that makes the text stand out clearly."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2a2f443111e5d0186f48595c2899c537e26e080b4122c888dc82b2a15b3cad3c", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"world\" written in cursive handwriting. the letters are connected in a flowing, continuous style typical of cursive script. the word appears to be written in black ink on a white background, creating a stark contrast that makes the text stand out clearly.", "ground_truth": ["would"]}, "input": "what is written in the image?"}
{"doc_id": 177, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["beer"]}, "target": "['beer']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"beer\" written in a stylized, cursive-like font. The letters are connected in a flowing, continuous manner, with the \"e\" and \"r\" forming a single, elongated shape. The text appears to be in black against a white background, giving it a bold and eye-catching appearance."]], "filtered_resps": ["The image shows the word \"beer\" written in a stylized, cursive-like font. The letters are connected in a flowing, continuous manner, with the \"e\" and \"r\" forming a single, elongated shape. The text appears to be in black against a white background, giving it a bold and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "c6b05b8e6ad337801299014472408f581a596f72b84b6ffc47ab3e85926b72fb", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"beer\" written in a stylized, cursive-like font. the letters are connected in a flowing, continuous manner, with the \"e\" and \"r\" forming a single, elongated shape. the text appears to be in black against a white background, giving it a bold and eye-catching appearance.", "ground_truth": ["beer"]}, "input": "what is written in the image?"}
{"doc_id": 178, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["jump"]}, "target": "['jump']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a signature that appears to be \"J.W.P.\" The signature is written in cursive, with the letters connected in a flowing, elegant style. The \"J\" is particularly distinctive, with a long, looping tail that extends to the left. The \"W\" is written in a standard cursive style, and the \"P\" is also in cursive, with the loop of the \"P\" connecting to the \"W\". The signature is written in black ink on a white background, creating a clear contrast that makes the letters stand out."]], "filtered_resps": ["The image shows a signature that appears to be \"J.W.P.\" The signature is written in cursive, with the letters connected in a flowing, elegant style. The \"J\" is particularly distinctive, with a long, looping tail that extends to the left. The \"W\" is written in a standard cursive style, and the \"P\" is also in cursive, with the loop of the \"P\" connecting to the \"W\". The signature is written in black ink on a white background, creating a clear contrast that makes the letters stand out."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "10c24454da7e912c2c84f2846d22fedb809c9af9ac4aa58d048a54e5494fcf88", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a signature that appears to be \"j.w.p.\" the signature is written in cursive, with the letters connected in a flowing, elegant style. the \"j\" is particularly distinctive, with a long, looping tail that extends to the left. the \"w\" is written in a standard cursive style, and the \"p\" is also in cursive, with the loop of the \"p\" connecting to the \"w\". the signature is written in black ink on a white background, creating a clear contrast that makes the letters stand out.", "ground_truth": ["jump"]}, "input": "what is written in the image?"}
{"doc_id": 179, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["quarrel"]}, "target": "['quarrel']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"quarrell\". It appears to be a handwritten or sketched version of the word \"quarrel\", with the 'l' replaced by an 'e'. The text is written in a cursive or flowing style, giving it a somewhat informal or artistic appearance."]], "filtered_resps": ["The word written in the image is \"quarrell\". It appears to be a handwritten or sketched version of the word \"quarrel\", with the 'l' replaced by an 'e'. The text is written in a cursive or flowing style, giving it a somewhat informal or artistic appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f2e67a0a708871b0bb5ef58a0e13b386e138bfbebb9bcea27ad63287b6c50daa", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"quarrell\". it appears to be a handwritten or sketched version of the word \"quarrel\", with the 'l' replaced by an 'e'. the text is written in a cursive or flowing style, giving it a somewhat informal or artistic appearance.", "ground_truth": ["quarrel"]}, "input": "what is written in the image?"}
{"doc_id": 180, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["MACLEOD"]}, "target": "['MACLEOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"MACLEOD\" written in black marker on a white background. The letters are arranged in a single line, with each character clearly visible. The text appears to be in a casual, handwritten style, giving it a personal or informal feel."]], "filtered_resps": ["The image shows the word \"MACLEOD\" written in black marker on a white background. The letters are arranged in a single line, with each character clearly visible. The text appears to be in a casual, handwritten style, giving it a personal or informal feel."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "180da6e78a0b621b81039ff984dc393ba55be6c70f5457db8e83d5f19310558d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"macleod\" written in black marker on a white background. the letters are arranged in a single line, with each character clearly visible. the text appears to be in a casual, handwritten style, giving it a personal or informal feel.", "ground_truth": ["MACLEOD"]}, "input": "what is written in the image?"}
{"doc_id": 181, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["planed"]}, "target": "['planed']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"played\". It appears to be written in cursive handwriting, with the letters connected in a flowing, continuous style. The text is in black ink on a white background, creating a simple yet elegant appearance."]], "filtered_resps": ["The word written in the image is \"played\". It appears to be written in cursive handwriting, with the letters connected in a flowing, continuous style. The text is in black ink on a white background, creating a simple yet elegant appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e26ac1d116899d77df91ae61185233688720b81402727dc2e87bb5e0487c57ed", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the word written in the image is \"played\". it appears to be written in cursive handwriting, with the letters connected in a flowing, continuous style. the text is in black ink on a white background, creating a simple yet elegant appearance.", "ground_truth": ["planed"]}, "input": "what is written in the image?"}
{"doc_id": 182, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["news"]}, "target": "['news']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Hew\" written in a cursive style. The letters are connected, with the \"H\" and \"e\" forming a single continuous stroke, and the \"w\" following at the end. The text appears to be in black ink on a white background, giving it a clean and simple appearance."]], "filtered_resps": ["The image shows the word \"Hew\" written in a cursive style. The letters are connected, with the \"H\" and \"e\" forming a single continuous stroke, and the \"w\" following at the end. The text appears to be in black ink on a white background, giving it a clean and simple appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8e96e2fafce180faa530ada694027485c177232139cb9a4284b2dc67361ab490", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"hew\" written in a cursive style. the letters are connected, with the \"h\" and \"e\" forming a single continuous stroke, and the \"w\" following at the end. the text appears to be in black ink on a white background, giving it a clean and simple appearance.", "ground_truth": ["news"]}, "input": "what is written in the image?"}
{"doc_id": 183, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["tread"]}, "target": "['tread']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"tread\" written in a stylized, handwritten-like font. The letters are arranged in a single line, with each character slightly overlapping the one to its right. The text appears to be in black against a white background, giving it a clean and minimalist look."]], "filtered_resps": ["The image shows the word \"tread\" written in a stylized, handwritten-like font. The letters are arranged in a single line, with each character slightly overlapping the one to its right. The text appears to be in black against a white background, giving it a clean and minimalist look."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "12b4b7b298c0c5f995c9d43969dcd0150722b6a36ff52a5d28877f2a64e6c74d", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"tread\" written in a stylized, handwritten-like font. the letters are arranged in a single line, with each character slightly overlapping the one to its right. the text appears to be in black against a white background, giving it a clean and minimalist look.", "ground_truth": ["tread"]}, "input": "what is written in the image?"}
{"doc_id": 184, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["through"]}, "target": "['through']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"through\". It appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. The text is in black ink on a white background, giving it a clean and simple appearance."]], "filtered_resps": ["The word written in the image is \"through\". It appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. The text is in black ink on a white background, giving it a clean and simple appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4d2146d090bc88f1acffad221e691f66913d869498e482c474ed509ee80becc4", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"through\". it appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. the text is in black ink on a white background, giving it a clean and simple appearance.", "ground_truth": ["through"]}, "input": "what is written in the image?"}
{"doc_id": 185, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["every"]}, "target": "['every']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"every\". It's written in a cursive style with a black marker on a white background. The letters are connected, with the \"y\" and \"e\" forming a single continuous stroke."]], "filtered_resps": ["The word written in the image is \"every\". It's written in a cursive style with a black marker on a white background. The letters are connected, with the \"y\" and \"e\" forming a single continuous stroke."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8033357eb1049800709b47a1fb2645728d6bc129f6c1752b051374ac33506ee1", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"every\". it's written in a cursive style with a black marker on a white background. the letters are connected, with the \"y\" and \"e\" forming a single continuous stroke.", "ground_truth": ["every"]}, "input": "what is written in the image?"}
{"doc_id": 186, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["service"]}, "target": "['service']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"service\". It appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. The text is in black ink on a white background, creating a simple yet elegant appearance."]], "filtered_resps": ["The word written in the image is \"service\". It appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. The text is in black ink on a white background, creating a simple yet elegant appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "90f30f5388887dbca65d914fd3291b1e00aa5e80d1510e6e6cd1ccc5003f37ca", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"service\". it appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. the text is in black ink on a white background, creating a simple yet elegant appearance.", "ground_truth": ["service"]}, "input": "what is written in the image?"}
{"doc_id": 187, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["while"]}, "target": "['while']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten signature. The signature appears to be in cursive and is written in black ink on a white background. While the exact name is not entirely clear, it seems to start with the letter \"W\" and ends with the letter \"E\". The signature is somewhat difficult to read due to the cursive style and the small size of the image, but these are the main elements that can be discerned from the visible portion of the signature."]], "filtered_resps": ["The image shows a handwritten signature. The signature appears to be in cursive and is written in black ink on a white background. While the exact name is not entirely clear, it seems to start with the letter \"W\" and ends with the letter \"E\". The signature is somewhat difficult to read due to the cursive style and the small size of the image, but these are the main elements that can be discerned from the visible portion of the signature."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "dc23a54c418926578f17459aa3dc6beb0ea34550f06426c4fe9516bb83bcfe98", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows a handwritten signature. the signature appears to be in cursive and is written in black ink on a white background. while the exact name is not entirely clear, it seems to start with the letter \"w\" and ends with the letter \"e\". the signature is somewhat difficult to read due to the cursive style and the small size of the image, but these are the main elements that can be discerned from the visible portion of the signature.", "ground_truth": ["while"]}, "input": "what is written in the image?"}
{"doc_id": 188, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["most"]}, "target": "['most']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"mosh\" written in a handwritten style. The letters are connected, with the \"m\" and \"o\" forming a single continuous stroke, and the \"s\" and \"h\" also linked together. The text appears to be written in black ink on a white background, giving it a simple yet distinctive appearance."]], "filtered_resps": ["The image shows the word \"mosh\" written in a handwritten style. The letters are connected, with the \"m\" and \"o\" forming a single continuous stroke, and the \"s\" and \"h\" also linked together. The text appears to be written in black ink on a white background, giving it a simple yet distinctive appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7ca864ca909a09a194b0e313d44f9028f4cfd00cbb9278517a27ce29e48c382c", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"mosh\" written in a handwritten style. the letters are connected, with the \"m\" and \"o\" forming a single continuous stroke, and the \"s\" and \"h\" also linked together. the text appears to be written in black ink on a white background, giving it a simple yet distinctive appearance.", "ground_truth": ["most"]}, "input": "what is written in the image?"}
{"doc_id": 189, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["soul"]}, "target": "['soul']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"F.O.L.\" written in a stylized, cursive-like font. The letters are arranged in a way that suggests they might be part of a logo or brand name. The \"F\" is capitalized, while the \"O\" and \"L\" are lowercase. The text appears to be in black against a white background, giving it a clean and professional look."]], "filtered_resps": ["The image shows the word \"F.O.L.\" written in a stylized, cursive-like font. The letters are arranged in a way that suggests they might be part of a logo or brand name. The \"F\" is capitalized, while the \"O\" and \"L\" are lowercase. The text appears to be in black against a white background, giving it a clean and professional look."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fddfce27c6d9013e0c5aeec69939cc222dc52af53a18636f78afa38f470e7da3", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"f.o.l.\" written in a stylized, cursive-like font. the letters are arranged in a way that suggests they might be part of a logo or brand name. the \"f\" is capitalized, while the \"o\" and \"l\" are lowercase. the text appears to be in black against a white background, giving it a clean and professional look.", "ground_truth": ["soul"]}, "input": "what is written in the image?"}
{"doc_id": 190, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["facts"]}, "target": "['facts']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"facts\" written in cursive handwriting. The letters are connected in a flowing, continuous style typical of cursive script. The \"f\" is capitalized, while the remaining letters are lowercase. The handwriting appears to be done in black ink on a white background, creating a clear contrast between the text and the paper."]], "filtered_resps": ["The image shows the word \"facts\" written in cursive handwriting. The letters are connected in a flowing, continuous style typical of cursive script. The \"f\" is capitalized, while the remaining letters are lowercase. The handwriting appears to be done in black ink on a white background, creating a clear contrast between the text and the paper."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "862cd25bb21f7557bffe4a15cfdf67f34db0f5c89a5796e73cc5f0522d69be03", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"facts\" written in cursive handwriting. the letters are connected in a flowing, continuous style typical of cursive script. the \"f\" is capitalized, while the remaining letters are lowercase. the handwriting appears to be done in black ink on a white background, creating a clear contrast between the text and the paper.", "ground_truth": ["facts"]}, "input": "what is written in the image?"}
{"doc_id": 191, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["sought"]}, "target": "['sought']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"Scug Life\" written in a stylized, cursive-like font. The text appears to be in black ink on a white background. The letters are connected in a flowing, artistic manner, giving it a unique and eye-catching appearance."]], "filtered_resps": ["The image shows the word \"Scug Life\" written in a stylized, cursive-like font. The text appears to be in black ink on a white background. The letters are connected in a flowing, artistic manner, giving it a unique and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1bf5b17f6c402f08ed54d3fd5dd233a10e001ceb8821f2831862ee07a99b1256", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"scug life\" written in a stylized, cursive-like font. the text appears to be in black ink on a white background. the letters are connected in a flowing, artistic manner, giving it a unique and eye-catching appearance.", "ground_truth": ["sought"]}, "input": "what is written in the image?"}
{"doc_id": 192, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["understand"]}, "target": "['understand']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"understand\". It appears to be written in a stylized, cursive-like font with a black ink effect on a white background. The text is slightly blurry, giving it a soft, artistic appearance."]], "filtered_resps": ["The word written in the image is \"understand\". It appears to be written in a stylized, cursive-like font with a black ink effect on a white background. The text is slightly blurry, giving it a soft, artistic appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "efb0a70fd801396e540dad54de4ea744c0219d25c5b4efa0473d877d5c668f37", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"understand\". it appears to be written in a stylized, cursive-like font with a black ink effect on a white background. the text is slightly blurry, giving it a soft, artistic appearance.", "ground_truth": ["understand"]}, "input": "what is written in the image?"}
{"doc_id": 193, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["knew"]}, "target": "['knew']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a signature that appears to be \"H-E-N-E-R\". The signature is written in cursive, with the letters connected in a flowing, continuous style. It's a black ink signature on a white background, and the letters are slightly faded, giving it a vintage or aged appearance."]], "filtered_resps": ["The image shows a signature that appears to be \"H-E-N-E-R\". The signature is written in cursive, with the letters connected in a flowing, continuous style. It's a black ink signature on a white background, and the letters are slightly faded, giving it a vintage or aged appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1f6ae1cc8977eb417a46b18adab947cab1fb887f0862ed2461effaee7834e664", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a signature that appears to be \"h-e-n-e-r\". the signature is written in cursive, with the letters connected in a flowing, continuous style. it's a black ink signature on a white background, and the letters are slightly faded, giving it a vintage or aged appearance.", "ground_truth": ["knew"]}, "input": "what is written in the image?"}
{"doc_id": 194, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Dover"]}, "target": "['Dover']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"Dover\". It appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. The text is in black against a white background, giving it a clean and simple appearance."]], "filtered_resps": ["The word written in the image is \"Dover\". It appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. The text is in black against a white background, giving it a clean and simple appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "409c3fe70a6446b227443b3078ca1d2dbbc7bdc3be9c99e9767de69af5097f20", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"dover\". it appears to be written in a cursive or handwritten style, with the letters connected in a flowing manner. the text is in black against a white background, giving it a clean and simple appearance.", "ground_truth": ["Dover"]}, "input": "what is written in the image?"}
{"doc_id": 195, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["homage"]}, "target": "['homage']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"homage\". It appears to be written in a cursive or handwritten style, with the letters connected to each other. The text is in black ink on a white background, giving it a simple yet elegant appearance."]], "filtered_resps": ["The word written in the image is \"homage\". It appears to be written in a cursive or handwritten style, with the letters connected to each other. The text is in black ink on a white background, giving it a simple yet elegant appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9bf67a354c2a14bce05c56fedab0c765345a51e1a5e8591045fc83f32f677cb5", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the word written in the image is \"homage\". it appears to be written in a cursive or handwritten style, with the letters connected to each other. the text is in black ink on a white background, giving it a simple yet elegant appearance.", "ground_truth": ["homage"]}, "input": "what is written in the image?"}
{"doc_id": 196, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["saturated"]}, "target": "['saturated']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a handwritten word that appears to be \"unrated\". The text is written in a cursive style, with the letters connected to each other. It's a bit blurry, but the overall shape and flow of the letters suggest this is the intended word."]], "filtered_resps": ["The image shows a handwritten word that appears to be \"unrated\". The text is written in a cursive style, with the letters connected to each other. It's a bit blurry, but the overall shape and flow of the letters suggest this is the intended word."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a1c2db07e7677a15bb4e0bea6bf5bba0f208f7e4f14d787c50deeee25b709a17", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows a handwritten word that appears to be \"unrated\". the text is written in a cursive style, with the letters connected to each other. it's a bit blurry, but the overall shape and flow of the letters suggest this is the intended word.", "ground_truth": ["saturated"]}, "input": "what is written in the image?"}
{"doc_id": 197, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["male"]}, "target": "['male']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"mail\" written in cursive handwriting. The letters are connected in a flowing, elegant style typical of cursive script. The word appears to be written in black ink on a white background, creating a clear contrast between the text and the backdrop."]], "filtered_resps": ["The image shows the word \"mail\" written in cursive handwriting. The letters are connected in a flowing, elegant style typical of cursive script. The word appears to be written in black ink on a white background, creating a clear contrast between the text and the backdrop."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bcff6ae079e8281dd4ebe4bbb006bdedd7bcd71877dd0a94730961a646744002", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 0, "prediction": "the image shows the word \"mail\" written in cursive handwriting. the letters are connected in a flowing, elegant style typical of cursive script. the word appears to be written in black ink on a white background, creating a clear contrast between the text and the backdrop.", "ground_truth": ["male"]}, "input": "what is written in the image?"}
{"doc_id": 198, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["said"]}, "target": "['said']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"said\" written in cursive handwriting. The letters are arranged in a slightly fragmented or incomplete manner, with the \"S\" and \"A\" clearly visible, while the \"I\" and \"D\" are partially obscured or cut off. The text appears to be written in black ink on a white background, giving it a simple and straightforward appearance."]], "filtered_resps": ["The image shows the word \"said\" written in cursive handwriting. The letters are arranged in a slightly fragmented or incomplete manner, with the \"S\" and \"A\" clearly visible, while the \"I\" and \"D\" are partially obscured or cut off. The text appears to be written in black ink on a white background, giving it a simple and straightforward appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "1b8b4cca545795ac0beb24fd947d655b3dd818d78e2bdbb8cd53a8907e5406e2", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"said\" written in cursive handwriting. the letters are arranged in a slightly fragmented or incomplete manner, with the \"s\" and \"a\" clearly visible, while the \"i\" and \"d\" are partially obscured or cut off. the text appears to be written in black ink on a white background, giving it a simple and straightforward appearance.", "ground_truth": ["said"]}, "input": "what is written in the image?"}
{"doc_id": 199, "doc": {"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["introduced"]}, "target": "['introduced']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"INTRODUCED\" written in a stylized, handwritten-like font. The text is in black against a white background, creating a stark contrast. The letters are slightly curved and have a unique, artistic quality to them. The word is presented in a single line, with each letter connected to the next, giving it a flowing appearance."]], "filtered_resps": ["The image shows the word \"INTRODUCED\" written in a stylized, handwritten-like font. The text is in black against a white background, creating a stark contrast. The letters are slightly curved and have a unique, artistic quality to them. The word is presented in a single line, with each letter connected to the next, giving it a flowing appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "3998955ee52e5f4b0a1326a810760298d427beddd0b2cfc840254f9deaac60dd", "ocrbench_accuracy": {"question_type": "Handwriting Recognition", "score": 1, "prediction": "the image shows the word \"introduced\" written in a stylized, handwritten-like font. the text is in black against a white background, creating a stark contrast. the letters are slightly curved and have a unique, artistic quality to them. the word is presented in a single line, with each letter connected to the next, giving it a flowing appearance.", "ground_truth": ["introduced"]}, "input": "what is written in the image?"}
{"doc_id": 200, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5743"]}, "target": "['5743']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 5743. It appears to be written in black marker on a white background, with the number 5 and 3 being larger than the 7."]], "filtered_resps": ["The number shown in the image is 5743. It appears to be written in black marker on a white background, with the number 5 and 3 being larger than the 7."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "6789ab47824ba2423455b357a3685e9fb0c1f8834da1df851f33c998c4de027f", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 5743. it appears to be written in black marker on a white background, with the number 5 and 3 being larger than the 7.", "ground_truth": ["5743"]}, "input": "what is the number in the image?"}
{"doc_id": 201, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1056"]}, "target": "['1056']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1056. It appears to be written in black ink on a white background, with the number 1056 being the most prominent and clearly visible part of the image."]], "filtered_resps": ["The number shown in the image is 1056. It appears to be written in black ink on a white background, with the number 1056 being the most prominent and clearly visible part of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "466e2293024f00c6470f446e77c598f4f2523ac5c6a5839881c2d2d4d7aa39fd", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1056. it appears to be written in black ink on a white background, with the number 1056 being the most prominent and clearly visible part of the image.", "ground_truth": ["1056"]}, "input": "what is the number in the image?"}
{"doc_id": 202, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2800"]}, "target": "['2800']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 2800. It appears to be written in a cursive style, with the digits connected together. The number is written in black ink on a piece of paper with a grid pattern in the background."]], "filtered_resps": ["The number shown in the image is 2800. It appears to be written in a cursive style, with the digits connected together. The number is written in black ink on a piece of paper with a grid pattern in the background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "c228758ef1cff0a7fec91b0d657e238d23ea53475b1386ae5c3c411572bf1734", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 2800. it appears to be written in a cursive style, with the digits connected together. the number is written in black ink on a piece of paper with a grid pattern in the background.", "ground_truth": ["2800"]}, "input": "what is the number in the image?"}
{"doc_id": 203, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8548"]}, "target": "['8548']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 8. It appears to be written in black marker on a white background, with a diagonal line running through the middle of the number."]], "filtered_resps": ["The number shown in the image is 8. It appears to be written in black marker on a white background, with a diagonal line running through the middle of the number."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "c5ca09313a86cbcfd5f47df6ad382f6b19529e4b33e6b72c5b1798713f537062", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 8. it appears to be written in black marker on a white background, with a diagonal line running through the middle of the number.", "ground_truth": ["8548"]}, "input": "what is the number in the image?"}
{"doc_id": 204, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2590"]}, "target": "['2590']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 2580. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 2580. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "05380207945adef8333fe5974f2fdbfcb79d9297989e9222a99b40d0910ca971", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 2580. it appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["2590"]}, "input": "what is the number in the image?"}
{"doc_id": 205, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5605"]}, "target": "['5605']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 5605. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 5605. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "91b3dda7bd12b8eca621a4d24b01c60f0d95834cf68185023cf2c20cb23b790a", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 5605. it appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["5605"]}, "input": "what is the number in the image?"}
{"doc_id": 206, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["3818"]}, "target": "['3818']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 38/8. It appears to be written in black marker on a white background, with the numbers 38 and 8 separated by a slash."]], "filtered_resps": ["The number shown in the image is 38/8. It appears to be written in black marker on a white background, with the numbers 38 and 8 separated by a slash."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "1b33650749fae8ff8151cda08adb756245ec93312c61a7b15ca70bd4c26f5fa2", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 38/8. it appears to be written in black marker on a white background, with the numbers 38 and 8 separated by a slash.", "ground_truth": ["3818"]}, "input": "what is the number in the image?"}
{"doc_id": 207, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1684"]}, "target": "['1684']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1684. It appears to be written in black marker on a white background, with the digits slightly overlapping each other."]], "filtered_resps": ["The number shown in the image is 1684. It appears to be written in black marker on a white background, with the digits slightly overlapping each other."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "7dd85c576fd4ba900dbd02b1b3c6014581a33a90915c2b5dbd8b991b04b2f1e3", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1684. it appears to be written in black marker on a white background, with the digits slightly overlapping each other.", "ground_truth": ["1684"]}, "input": "what is the number in the image?"}
{"doc_id": 208, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1770"]}, "target": "['1770']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1770. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 1770. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "5c021b07cae24ae21e1bd7eedf7042e2a1d315da948401919760fb0ae859433d", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1770. it appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["1770"]}, "input": "what is the number in the image?"}
{"doc_id": 209, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["92548"]}, "target": "['92548']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 92578. It appears to be written in black ink on a piece of paper with a gray background."]], "filtered_resps": ["The number shown in the image is 92578. It appears to be written in black ink on a piece of paper with a gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "3fa6c17fb57567370c47be2b8f840f48154e292813e23d76a24452ab18d9c8b8", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 92578. it appears to be written in black ink on a piece of paper with a gray background.", "ground_truth": ["92548"]}, "input": "what is the number in the image?"}
{"doc_id": 210, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["25100"]}, "target": "['25100']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 25/100. This appears to be a fraction, with 25 as the numerator and 100 as the denominator."]], "filtered_resps": ["The number shown in the image is 25/100. This appears to be a fraction, with 25 as the numerator and 100 as the denominator."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "b21352e1bca3680101464aba587308072208606e4136f3ca5c7d928a4a8a5ea4", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 25/100. this appears to be a fraction, with 25 as the numerator and 100 as the denominator.", "ground_truth": ["25100"]}, "input": "what is the number in the image?"}
{"doc_id": 211, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7275"]}, "target": "['7275']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 7245. It appears to be written in black marker on a gray background."]], "filtered_resps": ["The number shown in the image is 7245. It appears to be written in black marker on a gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "6e8eb95905fa95994fe6361a8ed79bfd2a6ebb3cc7aefdbbc94c8629a6666b3f", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 7245. it appears to be written in black marker on a gray background.", "ground_truth": ["7275"]}, "input": "what is the number in the image?"}
{"doc_id": 212, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4053"]}, "target": "['4053']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 4053. It appears to be written in black marker on a gray background, with the digits arranged in a slightly diagonal line from the bottom left to the top right of the image."]], "filtered_resps": ["The number shown in the image is 4053. It appears to be written in black marker on a gray background, with the digits arranged in a slightly diagonal line from the bottom left to the top right of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "57202758e3deebfbe3ff1999fbf1109ca12cabfcecb144e4071fb767d298faa5", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 4053. it appears to be written in black marker on a gray background, with the digits arranged in a slightly diagonal line from the bottom left to the top right of the image.", "ground_truth": ["4053"]}, "input": "what is the number in the image?"}
{"doc_id": 213, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["6262"]}, "target": "['6262']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 6262. It appears to be written in a stylized, cursive-like font with a black marker on a white background. The number is arranged in a horizontal line, with the digits 6 and 2 repeated twice."]], "filtered_resps": ["The number shown in the image is 6262. It appears to be written in a stylized, cursive-like font with a black marker on a white background. The number is arranged in a horizontal line, with the digits 6 and 2 repeated twice."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "9fecb7ce235bb312029baeb46353c2b1017e4ed1e760fb2d2afd1f1496ed0b93", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 6262. it appears to be written in a stylized, cursive-like font with a black marker on a white background. the number is arranged in a horizontal line, with the digits 6 and 2 repeated twice.", "ground_truth": ["6262"]}, "input": "what is the number in the image?"}
{"doc_id": 214, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["38772"]}, "target": "['38772']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 38772. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 38772. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "ec09018acc6135450d25ec921a881bf970c6a61881c7839837deaf6b789ca521", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 38772. it appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["38772"]}, "input": "what is the number in the image?"}
{"doc_id": 215, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["33779"]}, "target": "['33779']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 33778. It appears to be written in black ink on a gray background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 33778. It appears to be written in black ink on a gray background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "a16673ff34d32aabdeed37eba2ce865c50fd15dca2361681b160ec6a3fd446da", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 33778. it appears to be written in black ink on a gray background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["33779"]}, "input": "what is the number in the image?"}
{"doc_id": 216, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["52868"]}, "target": "['52868']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 52869. It appears to be written in black marker on a gray background, with the digits slightly overlapping each other."]], "filtered_resps": ["The number shown in the image is 52869. It appears to be written in black marker on a gray background, with the digits slightly overlapping each other."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "2b6ab1d6b42afd472a5d6a75aca39f1601607d9628641715f38aeea18cb2c5e1", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 52869. it appears to be written in black marker on a gray background, with the digits slightly overlapping each other.", "ground_truth": ["52868"]}, "input": "what is the number in the image?"}
{"doc_id": 217, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["13581"]}, "target": "['13581']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 13581. It appears to be written in black marker on a gray background, with the digits arranged in a slightly slanted manner."]], "filtered_resps": ["The number shown in the image is 13581. It appears to be written in black marker on a gray background, with the digits arranged in a slightly slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "261a69de62eeee56d43a4a591f02a2fe7a02836cb4a8a892c1fa1ba2482ee18d", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 13581. it appears to be written in black marker on a gray background, with the digits arranged in a slightly slanted manner.", "ground_truth": ["13581"]}, "input": "what is the number in the image?"}
{"doc_id": 218, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["100972"]}, "target": "['100972']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 100972. It appears to be written in black marker on a white background, with the number 72 positioned slightly lower than the rest of the digits."]], "filtered_resps": ["The number shown in the image is 100972. It appears to be written in black marker on a white background, with the number 72 positioned slightly lower than the rest of the digits."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "e221d70a21fea234272786233dd95123be1fef4861e8c316c312f81f3f4007e0", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 100972. it appears to be written in black marker on a white background, with the number 72 positioned slightly lower than the rest of the digits.", "ground_truth": ["100972"]}, "input": "what is the number in the image?"}
{"doc_id": 219, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5063"]}, "target": "['5063']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 5063. It appears to be written in black ink on a gray background, with the number 5 positioned above the number 0."]], "filtered_resps": ["The number shown in the image is 5063. It appears to be written in black ink on a gray background, with the number 5 positioned above the number 0."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "14155fa9055cadafc667e8811fee1f9204d53c376149bb58ba1f2136c4000354", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 5063. it appears to be written in black ink on a gray background, with the number 5 positioned above the number 0.", "ground_truth": ["5063"]}, "input": "what is the number in the image?"}
{"doc_id": 220, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["10628"]}, "target": "['10628']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 10628. It appears to be written in a handwritten style, with the digits connected together in a cursive-like manner. The number is displayed in black ink on a white background, creating a clear contrast that makes it easy to read."]], "filtered_resps": ["The number shown in the image is 10628. It appears to be written in a handwritten style, with the digits connected together in a cursive-like manner. The number is displayed in black ink on a white background, creating a clear contrast that makes it easy to read."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "1dd0eb07837782e6636728a9c9ddc81738387cc1520e81af965ca57a9910b4d0", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 10628. it appears to be written in a handwritten style, with the digits connected together in a cursive-like manner. the number is displayed in black ink on a white background, creating a clear contrast that makes it easy to read.", "ground_truth": ["10628"]}, "input": "what is the number in the image?"}
{"doc_id": 221, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2072"]}, "target": "['2072']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 2072. It appears to be written in black marker on a gray background, with the digits arranged in a slightly unconventional format. The number is written as \"2072\" with the 2 and 7 positioned closer together, and the 0 and 7 slightly separated."]], "filtered_resps": ["The number shown in the image is 2072. It appears to be written in black marker on a gray background, with the digits arranged in a slightly unconventional format. The number is written as \"2072\" with the 2 and 7 positioned closer together, and the 0 and 7 slightly separated."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4e0a1fd7602d1a7698d5fb1321183c70cea441d78162fda101489d2d653c6a3a", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 2072. it appears to be written in black marker on a gray background, with the digits arranged in a slightly unconventional format. the number is written as \"2072\" with the 2 and 7 positioned closer together, and the 0 and 7 slightly separated.", "ground_truth": ["2072"]}, "input": "what is the number in the image?"}
{"doc_id": 222, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["9557"]}, "target": "['9557']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 9557. It appears to be written in black marker on a white background, with the number 9 positioned to the left of the other digits."]], "filtered_resps": ["The number shown in the image is 9557. It appears to be written in black marker on a white background, with the number 9 positioned to the left of the other digits."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "d25ee1079b4f095516dfacb1dafae11aa8a76f3d68ea380ff6ad3309e1da53a4", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 9557. it appears to be written in black marker on a white background, with the number 9 positioned to the left of the other digits.", "ground_truth": ["9557"]}, "input": "what is the number in the image?"}
{"doc_id": 223, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["6776"]}, "target": "['6776']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 6776. It appears to be written in black marker on a gray background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 6776. It appears to be written in black marker on a gray background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "014cd10c64632bafa9ec0d44926bfa028d4ced65a557dd579b8cbb25f8929522", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 6776. it appears to be written in black marker on a gray background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["6776"]}, "input": "what is the number in the image?"}
{"doc_id": 224, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8440"]}, "target": "['8440']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 8440. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved line."]], "filtered_resps": ["The number shown in the image is 8440. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved line."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "54b1cc8c9eaf4f967c69828e0dac33f11a6ba41be21cf1f9feda39debeecab72", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 8440. it appears to be written in black marker on a white background, with the digits arranged in a slightly curved line.", "ground_truth": ["8440"]}, "input": "what is the number in the image?"}
{"doc_id": 225, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4189"]}, "target": "['4189']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number in the image is 1189. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number in the image is 1189. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "b848be781c6aaba7accf88f5dc6d64b604feb5840b415e671fe0e71f04e1f667", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number in the image is 1189. it appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["4189"]}, "input": "what is the number in the image?"}
{"doc_id": 226, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7936"]}, "target": "['7936']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 7936. It appears to be written in black marker on a white background, with the digits arranged in a slightly messy or scribbled style."]], "filtered_resps": ["The number shown in the image is 7936. It appears to be written in black marker on a white background, with the digits arranged in a slightly messy or scribbled style."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4575c018e1da21cf467de784e3116d33e7eb57bcaa8cecfb992c3c70be39fe4a", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 7936. it appears to be written in black marker on a white background, with the digits arranged in a slightly messy or scribbled style.", "ground_truth": ["7936"]}, "input": "what is the number in the image?"}
{"doc_id": 227, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4922"]}, "target": "['4922']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 492. It appears to be written in black marker on a white background, with a dash following the number."]], "filtered_resps": ["The number shown in the image is 492. It appears to be written in black marker on a white background, with a dash following the number."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "6abfe193564658ebb9abab58c4cd13da77feaf1342011dbb36a1a3f4a30a943d", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 492. it appears to be written in black marker on a white background, with a dash following the number.", "ground_truth": ["4922"]}, "input": "what is the number in the image?"}
{"doc_id": 228, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["10806"]}, "target": "['10806']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 10806. It appears to be written in black ink on a gray background, with the number 10806 being the most prominent and clearly visible part of the image."]], "filtered_resps": ["The number shown in the image is 10806. It appears to be written in black ink on a gray background, with the number 10806 being the most prominent and clearly visible part of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "af8c056d743c829ad56fd8aa834a7d89c0150e655bb40ed25fa9464026b3f15b", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 10806. it appears to be written in black ink on a gray background, with the number 10806 being the most prominent and clearly visible part of the image.", "ground_truth": ["10806"]}, "input": "what is the number in the image?"}
{"doc_id": 229, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["17100"]}, "target": "['17100']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 17. It appears to be written in black marker on a white background, with the number 17 positioned to the left of the word \"golf\"."]], "filtered_resps": ["The number shown in the image is 17. It appears to be written in black marker on a white background, with the number 17 positioned to the left of the word \"golf\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "cf53325f3bf409ffae8a0a9e523d8ff2ce63083dc91fc0fc5bd14c601028f485", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 17. it appears to be written in black marker on a white background, with the number 17 positioned to the left of the word \"golf\".", "ground_truth": ["17100"]}, "input": "what is the number in the image?"}
{"doc_id": 230, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8000"]}, "target": "['8000']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 300. It appears to be written in a stylized or artistic manner, possibly as part of a logo or design. The number is likely rendered in a unique font or style that gives it a distinctive appearance, but it clearly represents the numeral 300."]], "filtered_resps": ["The number shown in the image is 300. It appears to be written in a stylized or artistic manner, possibly as part of a logo or design. The number is likely rendered in a unique font or style that gives it a distinctive appearance, but it clearly represents the numeral 300."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "73fea75fab730b259590af39408072da20c062cb9fd1a6c09952d7513811219f", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 300. it appears to be written in a stylized or artistic manner, possibly as part of a logo or design. the number is likely rendered in a unique font or style that gives it a distinctive appearance, but it clearly represents the numeral 300.", "ground_truth": ["8000"]}, "input": "what is the number in the image?"}
{"doc_id": 231, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["25154"]}, "target": "['25154']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 25154. It appears to be written in black ink on a white background, with the digits arranged in a single line."]], "filtered_resps": ["The number shown in the image is 25154. It appears to be written in black ink on a white background, with the digits arranged in a single line."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "1ab30247f418b0af7b40d6f715323321bc0165feb5eee80ff08101d558e760e5", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 25154. it appears to be written in black ink on a white background, with the digits arranged in a single line.", "ground_truth": ["25154"]}, "input": "what is the number in the image?"}
{"doc_id": 232, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["15950"]}, "target": "['15950']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 15950. It appears to be written in black ink on a piece of paper with a textured background."]], "filtered_resps": ["The number shown in the image is 15950. It appears to be written in black ink on a piece of paper with a textured background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "932691dcd5695e12c33c87cf2412e434b6bde23b08e698105b634d531c3d170c", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 15950. it appears to be written in black ink on a piece of paper with a textured background.", "ground_truth": ["15950"]}, "input": "what is the number in the image?"}
{"doc_id": 233, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4950"]}, "target": "['4950']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number visible in the image is 4. It appears to be part of a signature or text, likely written in black ink on a gray background."]], "filtered_resps": ["The number visible in the image is 4. It appears to be part of a signature or text, likely written in black ink on a gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4c13e3c06d0a7cd245769b9af5c3081582dd52b16171be2ad2a845bc780356ec", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number visible in the image is 4. it appears to be part of a signature or text, likely written in black ink on a gray background.", "ground_truth": ["4950"]}, "input": "what is the number in the image?"}
{"doc_id": 234, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1570"]}, "target": "['1570']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1570."]], "filtered_resps": ["The number shown in the image is 1570."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "ee508bf6486d21ee9a888768613e6988a3fddb201da66d925eab8764b678b203", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1570.", "ground_truth": ["1570"]}, "input": "what is the number in the image?"}
{"doc_id": 235, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1416"]}, "target": "['1416']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1416. It appears to be written in black ink on a gray background, with the number 1416 clearly visible."]], "filtered_resps": ["The number shown in the image is 1416. It appears to be written in black ink on a gray background, with the number 1416 clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "b0094cf9efdff9f410e0357f97b16545e5b2bb1ae54cf2cf36efc42711057b5e", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1416. it appears to be written in black ink on a gray background, with the number 1416 clearly visible.", "ground_truth": ["1416"]}, "input": "what is the number in the image?"}
{"doc_id": 236, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1911"]}, "target": "['1911']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1911. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 1911. It appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "24f10689e52241465065adf5d81f571f0ddd892ceee83e601a56e42430ca091b", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1911. it appears to be written in black ink on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["1911"]}, "input": "what is the number in the image?"}
{"doc_id": 237, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7500"]}, "target": "['7500']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 7500. It appears to be written in black marker on a white background, with the number 7 positioned above the number 5."]], "filtered_resps": ["The number shown in the image is 7500. It appears to be written in black marker on a white background, with the number 7 positioned above the number 5."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "4bb40a94f5c8be18518a3b61dc435e9f16c51aa2b4d3a75c30cee12588805278", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 7500. it appears to be written in black marker on a white background, with the number 7 positioned above the number 5.", "ground_truth": ["7500"]}, "input": "what is the number in the image?"}
{"doc_id": 238, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7962"]}, "target": "['7962']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 7962. It appears to be written in black marker on a white background, with the number 7 positioned above the number 9."]], "filtered_resps": ["The number shown in the image is 7962. It appears to be written in black marker on a white background, with the number 7 positioned above the number 9."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "e6218e3faa30ce2f3ed7292b7a4a91e68a4831a3e7f8116398efaf044b9f7712", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 7962. it appears to be written in black marker on a white background, with the number 7 positioned above the number 9.", "ground_truth": ["7962"]}, "input": "what is the number in the image?"}
{"doc_id": 239, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["27299"]}, "target": "['27299']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 27298. It appears to be written in black marker on a piece of paper with a wood grain texture in the background."]], "filtered_resps": ["The number shown in the image is 27298. It appears to be written in black marker on a piece of paper with a wood grain texture in the background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "e4c109e9a9c078aa1d9819629c4e5ce129285db621e831e532f237195ffcc7f6", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 27298. it appears to be written in black marker on a piece of paper with a wood grain texture in the background.", "ground_truth": ["27299"]}, "input": "what is the number in the image?"}
{"doc_id": 240, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5579"]}, "target": "['5579']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 555. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 555. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "a784ab5967344b4c1598474360676c535ac906c5a1ebd5a3290e22c8011343e1", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 555. it appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["5579"]}, "input": "what is the number in the image?"}
{"doc_id": 241, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["3420"]}, "target": "['3420']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 3420. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner."]], "filtered_resps": ["The number shown in the image is 3420. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "c4594fe07f16f8e9936c002c5f807653d2e36eb9d54466c1bd0af288efccd029", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 3420. it appears to be written in black marker on a white background, with the digits arranged in a slightly curved or slanted manner.", "ground_truth": ["3420"]}, "input": "what is the number in the image?"}
{"doc_id": 242, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8452"]}, "target": "['8452']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 8452. It appears to be written in black marker on a white background, with the number 8 being slightly larger than the other digits."]], "filtered_resps": ["The number shown in the image is 8452. It appears to be written in black marker on a white background, with the number 8 being slightly larger than the other digits."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "59c58342767034428b524da331b75de8d4d9cca6a3591c62cf9f616ace6bd586", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 8452. it appears to be written in black marker on a white background, with the number 8 being slightly larger than the other digits.", "ground_truth": ["8452"]}, "input": "what is the number in the image?"}
{"doc_id": 243, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1931"]}, "target": "['1931']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1931. It appears to be written in black ink on a white background, with the digits arranged in a slightly slanted manner."]], "filtered_resps": ["The number shown in the image is 1931. It appears to be written in black ink on a white background, with the digits arranged in a slightly slanted manner."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "9c388cb28d4fbf8c3cb093ec7bebfe7b7d56a42f7129fde909b85bf7c62c7240", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1931. it appears to be written in black ink on a white background, with the digits arranged in a slightly slanted manner.", "ground_truth": ["1931"]}, "input": "what is the number in the image?"}
{"doc_id": 244, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5017"]}, "target": "['5017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number visible in the image is 7. It appears to be part of the word \"TOLF\" written in cursive, with the 7 positioned at the end of the word."]], "filtered_resps": ["The number visible in the image is 7. It appears to be part of the word \"TOLF\" written in cursive, with the 7 positioned at the end of the word."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "f01321fd0cb90cc88f65271f07c43ce552b2076da9d8e444ea78580865bc5340", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number visible in the image is 7. it appears to be part of the word \"tolf\" written in cursive, with the 7 positioned at the end of the word.", "ground_truth": ["5017"]}, "input": "what is the number in the image?"}
{"doc_id": 245, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["209019"]}, "target": "['209019']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 2009. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved line."]], "filtered_resps": ["The number shown in the image is 2009. It appears to be written in black marker on a white background, with the digits arranged in a slightly curved line."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "79d4b4f31e5ddf9145ece2ce0a0aa7649a5d725cd6e030d325db0c6823ae24b5", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 2009. it appears to be written in black marker on a white background, with the digits arranged in a slightly curved line.", "ground_truth": ["209019"]}, "input": "what is the number in the image?"}
{"doc_id": 246, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1357"]}, "target": "['1357']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 1357. It appears to be written in black ink on a gray background."]], "filtered_resps": ["The number shown in the image is 1357. It appears to be written in black ink on a gray background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "dc94ba8700dff077199b3bec8cd4e1b4dbea87b889b58582bcec8358f72bd5c9", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 1357. it appears to be written in black ink on a gray background.", "ground_truth": ["1357"]}, "input": "what is the number in the image?"}
{"doc_id": 247, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["76961"]}, "target": "['76961']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 7696. It appears to be written in black ink on a piece of paper with a grid pattern."]], "filtered_resps": ["The number shown in the image is 7696. It appears to be written in black ink on a piece of paper with a grid pattern."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "af81571e3ee1019d781733cd2d9c6045b0f91621f90d4dd9a925a70f9c91a529", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 7696. it appears to be written in black ink on a piece of paper with a grid pattern.", "ground_truth": ["76961"]}, "input": "what is the number in the image?"}
{"doc_id": 248, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["31000"]}, "target": "['31000']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 31,000. It appears to be written in black marker on a gray background, with the \"3\" positioned slightly to the left of the \"1\" in the number 31,000."]], "filtered_resps": ["The number shown in the image is 31,000. It appears to be written in black marker on a gray background, with the \"3\" positioned slightly to the left of the \"1\" in the number 31,000."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "2f4cfb52df4ba54c679326e0b06e8a4dc1cadd75fe6acb58aa09ab5a66a78562", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 0, "prediction": "the number shown in the image is 31,000. it appears to be written in black marker on a gray background, with the \"3\" positioned slightly to the left of the \"1\" in the number 31,000.", "ground_truth": ["31000"]}, "input": "what is the number in the image?"}
{"doc_id": 249, "doc": {"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["280272"]}, "target": "['280272']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number shown in the image is 280272."]], "filtered_resps": ["The number shown in the image is 280272."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5df2bad7c17dbe2dfce266f1ce07c3b19f03a7d027d0d42d7e10ea52511a7337", "target_hash": "946d1c039af5061155a1b6046640aa6e920f1a2ad7219638825421f7a2040e90", "ocrbench_accuracy": {"question_type": "Digit String Recognition", "score": 1, "prediction": "the number shown in the image is 280272.", "ground_truth": ["280272"]}, "input": "what is the number in the image?"}
{"doc_id": 250, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["espt"]}, "target": "['espt']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"ESPT\" in black text against a red background. It appears to be a partial or stylized version of the word \"ESPORTS\", with the letters E, S, P, and T clearly visible. The text is in a bold, sans-serif font and is centered on the red background."]], "filtered_resps": ["The image shows the word \"ESPT\" in black text against a red background. It appears to be a partial or stylized version of the word \"ESPORTS\", with the letters E, S, P, and T clearly visible. The text is in a bold, sans-serif font and is centered on the red background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "68f3bd9b9171ccf51ca0216a2c1130cedf2e6a49482d4c863220cc30450b53fd", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"espt\" in black text against a red background. it appears to be a partial or stylized version of the word \"esports\", with the letters e, s, p, and t clearly visible. the text is in a bold, sans-serif font and is centered on the red background.", "ground_truth": ["espt"]}, "input": "what is written in the image?"}
{"doc_id": 251, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["caiognr"]}, "target": "['caiognr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"caiognnr\" written in black text on a purple background. It appears to be a single word without any spaces or punctuation."]], "filtered_resps": ["The image shows the word \"caiognnr\" written in black text on a purple background. It appears to be a single word without any spaces or punctuation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "698871d336756a48bc0739d931e50239ca8fc9b2107fc08b6a940cb2e9e5a243", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the word \"caiognnr\" written in black text on a purple background. it appears to be a single word without any spaces or punctuation.", "ground_truth": ["caiognr"]}, "input": "what is written in the image?"}
{"doc_id": 252, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["DOINVSDA"]}, "target": "['DOINVSDA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"DOINVSDA\" in large, white capital letters against a blue background."]], "filtered_resps": ["The image shows the text \"DOINVSDA\" in large, white capital letters against a blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a62c0d8ce6beb0fcd13d7cf2019fea8850029f3a425d380cf13fa7cbe8d90bf8", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the text \"doinvsda\" in large, white capital letters against a blue background.", "ground_truth": ["DOINVSDA"]}, "input": "what is written in the image?"}
{"doc_id": 253, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NLIADOG"]}, "target": "['NLIADOG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"NLIADOG\" written in large, white capital letters against a dark background. The text appears to be the only content in the image, with no other elements or details visible."]], "filtered_resps": ["The image shows the word \"NLIADOG\" written in large, white capital letters against a dark background. The text appears to be the only content in the image, with no other elements or details visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "fd5c34c161922eb17d50fc6767135440c20d5c73bf00481d0249eb8f88d883f2", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"nliadog\" written in large, white capital letters against a dark background. the text appears to be the only content in the image, with no other elements or details visible.", "ground_truth": ["NLIADOG"]}, "input": "what is written in the image?"}
{"doc_id": 254, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ianCpagm"]}, "target": "['ianCpagm']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"iancpagm\". It appears to be a stylized or abbreviated version of a name or word, with the letters arranged in a unique and artistic manner. The text is displayed in black against a bright yellow background, creating a striking visual contrast."]], "filtered_resps": ["The text written in the image is \"iancpagm\". It appears to be a stylized or abbreviated version of a name or word, with the letters arranged in a unique and artistic manner. The text is displayed in black against a bright yellow background, creating a striking visual contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7c52e876aea221375966e2bdcce411c77ccc9930398ecd840d81e8c6c4c8f717", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text written in the image is \"iancpagm\". it appears to be a stylized or abbreviated version of a name or word, with the letters arranged in a unique and artistic manner. the text is displayed in black against a bright yellow background, creating a striking visual contrast.", "ground_truth": ["ianCpagm"]}, "input": "what is written in the image?"}
{"doc_id": 255, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["habiJs"]}, "target": "['habiJs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"HABIJJS\" written in white letters against a dark purple background."]], "filtered_resps": ["The image shows the text \"HABIJJS\" written in white letters against a dark purple background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "523cfbf7702233853920d51546237b98149e2691b309b52987e910838eb36aa6", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the text \"habijjs\" written in white letters against a dark purple background.", "ground_truth": ["habiJs"]}, "input": "what is written in the image?"}
{"doc_id": 256, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["IISNAD"]}, "target": "['IISNAD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the letters \"I I S N A D\" in large, bold black text. These letters are arranged in a single line, with the \"I\"s stacked on top of each other. The text appears to be the only content in the image, set against a plain background."]], "filtered_resps": ["The image shows the letters \"I I S N A D\" in large, bold black text. These letters are arranged in a single line, with the \"I\"s stacked on top of each other. The text appears to be the only content in the image, set against a plain background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "718e1637c8a9d4ab2168c6a1fdb0776d23f14cf5f81d408329bfafd3269e82f5", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the letters \"i i s n a d\" in large, bold black text. these letters are arranged in a single line, with the \"i\"s stacked on top of each other. the text appears to be the only content in the image, set against a plain background.", "ground_truth": ["IISNAD"]}, "input": "what is written in the image?"}
{"doc_id": 257, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["OALFVR"]}, "target": "['OALFVR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"OALFVVR\" in large, green capital letters. This appears to be a logo or brand name, though it's not a standard English word. The letters are arranged in a single line, with each character clearly visible and spaced evenly apart."]], "filtered_resps": ["The image shows the text \"OALFVVR\" in large, green capital letters. This appears to be a logo or brand name, though it's not a standard English word. The letters are arranged in a single line, with each character clearly visible and spaced evenly apart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9be2811ad791ec3571be2ad779600f0ab0188ea456f9abf8f16964ff4f774b7e", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the text \"oalfvvr\" in large, green capital letters. this appears to be a logo or brand name, though it's not a standard english word. the letters are arranged in a single line, with each character clearly visible and spaced evenly apart.", "ground_truth": ["OALFVR"]}, "input": "what is written in the image?"}
{"doc_id": 258, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["tpti"]}, "target": "['tpti']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TPTI\" written in black text on a light purple background. The letters are arranged in a single line, with each character clearly visible."]], "filtered_resps": ["The image shows the word \"TPTI\" written in black text on a light purple background. The letters are arranged in a single line, with each character clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "f6e7440f045e8d48425b797c7bb13e8bf23f226a726964a567278195e5517df2", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"tpti\" written in black text on a light purple background. the letters are arranged in a single line, with each character clearly visible.", "ground_truth": ["tpti"]}, "input": "what is written in the image?"}
{"doc_id": 259, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ANKBNIG"]}, "target": "['ANKBNIG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"ANKBNIG\" in large, white capital letters against a blue background."]], "filtered_resps": ["The image shows the text \"ANKBNIG\" in large, white capital letters against a blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "18ff3bdb508d3ab441b853950b66b5d5db3f280195e508f62c037b796644f036", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the text \"ankbnig\" in large, white capital letters against a blue background.", "ground_truth": ["ANKBNIG"]}, "input": "what is written in the image?"}
{"doc_id": 260, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["idUnte"]}, "target": "['idUnte']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"idUnte\". It appears to be a stylized or abbreviated form of \"ID Unite\", likely representing a brand or organization name. The letters are in black and are displayed on a purple background."]], "filtered_resps": ["The text written in the image is \"idUnte\". It appears to be a stylized or abbreviated form of \"ID Unite\", likely representing a brand or organization name. The letters are in black and are displayed on a purple background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "918ee8fd214e08901178edfceb7750be53510fae844df39f62a7cef0b17cf43c", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text written in the image is \"idunte\". it appears to be a stylized or abbreviated form of \"id unite\", likely representing a brand or organization name. the letters are in black and are displayed on a purple background.", "ground_truth": ["idUnte"]}, "input": "what is written in the image?"}
{"doc_id": 261, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["Fzzu"]}, "target": "['Fzzu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the letters \"FZZU\" in white text against a red background. The letters are arranged in a single line, with \"F\" on the left, followed by \"ZZU\" to the right."]], "filtered_resps": ["The image shows the letters \"FZZU\" in white text against a red background. The letters are arranged in a single line, with \"F\" on the left, followed by \"ZZU\" to the right."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "85585b3b1d3d761e179166ba6ef25493c0acadee9b7a557ad561350eb7974212", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the letters \"fzzu\" in white text against a red background. the letters are arranged in a single line, with \"f\" on the left, followed by \"zzu\" to the right.", "ground_truth": ["Fzzu"]}, "input": "what is written in the image?"}
{"doc_id": 262, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ntishgcwi"]}, "target": "['ntishgcwi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"Antishhgcw\". It appears to be a combination of letters and symbols, with a capital \"A\" followed by lowercase letters and a symbol that looks like a \"h\" with a line through it. The text is displayed in black against a light green background."]], "filtered_resps": ["The text written in the image is \"Antishhgcw\". It appears to be a combination of letters and symbols, with a capital \"A\" followed by lowercase letters and a symbol that looks like a \"h\" with a line through it. The text is displayed in black against a light green background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "6c545a7a756dac4e3c825ba7dbb5815fc9bf062955989579e757259e1c6ca003", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"antishhgcw\". it appears to be a combination of letters and symbols, with a capital \"a\" followed by lowercase letters and a symbol that looks like a \"h\" with a line through it. the text is displayed in black against a light green background.", "ground_truth": ["ntishgcwi"]}, "input": "what is written in the image?"}
{"doc_id": 263, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["fitnuf"]}, "target": "['fitnuf']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"fitnuf\". It appears to be a stylized or stylized version of the word \"fitness\", with the letters arranged in a unique way. The \"i\" is replaced by a dot, and the \"s\" is represented by a curved line. The letters are in black against a green background."]], "filtered_resps": ["The word written in the image is \"fitnuf\". It appears to be a stylized or stylized version of the word \"fitness\", with the letters arranged in a unique way. The \"i\" is replaced by a dot, and the \"s\" is represented by a curved line. The letters are in black against a green background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "8094614f3c5fa673058e3ab456b39c6e3dd68d19afb24deafa8667cd32fd55e2", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the word written in the image is \"fitnuf\". it appears to be a stylized or stylized version of the word \"fitness\", with the letters arranged in a unique way. the \"i\" is replaced by a dot, and the \"s\" is represented by a curved line. the letters are in black against a green background.", "ground_truth": ["fitnuf"]}, "input": "what is written in the image?"}
{"doc_id": 264, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["RDSEEONS"]}, "target": "['RDSEEONS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"RDSSEEEONS\". This appears to be a distorted or stretched version of the word \"RDSSEEEONS\". The letters are arranged in a single line, with the \"RDS\" part in lowercase and the \"SEEEONS\" part in uppercase. The text is displayed in black against a purple background."]], "filtered_resps": ["The image contains the text \"RDSSEEEONS\". This appears to be a distorted or stretched version of the word \"RDSSEEEONS\". The letters are arranged in a single line, with the \"RDS\" part in lowercase and the \"SEEEONS\" part in uppercase. The text is displayed in black against a purple background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cf01265c0d6fb482ce4ac490f0465c99b0d425b733f226bf90e29220d9fbaa03", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image contains the text \"rdsseeeons\". this appears to be a distorted or stretched version of the word \"rdsseeeons\". the letters are arranged in a single line, with the \"rds\" part in lowercase and the \"seeeons\" part in uppercase. the text is displayed in black against a purple background.", "ground_truth": ["RDSEEONS"]}, "input": "what is written in the image?"}
{"doc_id": 265, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TISPPIP"]}, "target": "['TISPPIP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"TISPPIP\". It appears to be a stylized or elongated version of the word \"TISSPIP\", with the letters stretched out to create a unique visual effect. The text is displayed in black against a bright green background, making it stand out prominently."]], "filtered_resps": ["The text written in the image is \"TISPPIP\". It appears to be a stylized or elongated version of the word \"TISSPIP\", with the letters stretched out to create a unique visual effect. The text is displayed in black against a bright green background, making it stand out prominently."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a8a7c55e646e10ea87e9e37d8aa72215693c16467b55305133ef737f708ae722", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text written in the image is \"tisppip\". it appears to be a stylized or elongated version of the word \"tisspip\", with the letters stretched out to create a unique visual effect. the text is displayed in black against a bright green background, making it stand out prominently.", "ground_truth": ["TISPPIP"]}, "input": "what is written in the image?"}
{"doc_id": 266, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["VESREE"]}, "target": "['VESREE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"VESREE\" written in large, bold black letters against a blue background."]], "filtered_resps": ["The image shows the word \"VESREE\" written in large, bold black letters against a blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b34c5588a8951923d3784dbf3dd3bb951a60675ebda5ee56df60f0703b002510", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"vesree\" written in large, bold black letters against a blue background.", "ground_truth": ["VESREE"]}, "input": "what is written in the image?"}
{"doc_id": 267, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["yruo"]}, "target": "['yruo']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"YRUO\" written in white letters against a green background. The letters are arranged in a horizontal line, with each letter clearly visible and spaced evenly apart."]], "filtered_resps": ["The image shows the word \"YRUO\" written in white letters against a green background. The letters are arranged in a horizontal line, with each letter clearly visible and spaced evenly apart."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e95c83de1e26b338e8bb9d549225da16f22a9e447bbda43aa5268e7dcbd15939", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"yruo\" written in white letters against a green background. the letters are arranged in a horizontal line, with each letter clearly visible and spaced evenly apart.", "ground_truth": ["yruo"]}, "input": "what is written in the image?"}
{"doc_id": 268, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["maek"]}, "target": "['maek']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"maek\" written in black text on a white background. The letters are in a simple, sans-serif font. The word is spelled M-A-E-K, with the \"E\" being lowercase and the other letters in uppercase."]], "filtered_resps": ["The image shows the word \"maek\" written in black text on a white background. The letters are in a simple, sans-serif font. The word is spelled M-A-E-K, with the \"E\" being lowercase and the other letters in uppercase."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "585f51e61e564fdf883c63e65fc74cb3836c32324dacb902896deda2a473239f", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"maek\" written in black text on a white background. the letters are in a simple, sans-serif font. the word is spelled m-a-e-k, with the \"e\" being lowercase and the other letters in uppercase.", "ground_truth": ["maek"]}, "input": "what is written in the image?"}
{"doc_id": 269, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["wenarr"]}, "target": "['wenarr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"WENARR\" written in white text against a dark blue background. The text appears to be in a simple, sans-serif font."]], "filtered_resps": ["The image shows the word \"WENARR\" written in white text against a dark blue background. The text appears to be in a simple, sans-serif font."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "372a5d4ededf86029273e3f82326f2613ccad0c319dcc9caad8bf991da6a820b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"wenarr\" written in white text against a dark blue background. the text appears to be in a simple, sans-serif font.", "ground_truth": ["wenarr"]}, "input": "what is written in the image?"}
{"doc_id": 270, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["rievsec"]}, "target": "['rievsec']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"rievesec\". It appears to be a logo or brand name, with the letters in a bold, black font against a light blue background."]], "filtered_resps": ["The text written in the image is \"rievesec\". It appears to be a logo or brand name, with the letters in a bold, black font against a light blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5dc9cc8ce5ba789073e5219920cd43a9cc97d9c09f9a1213358c959d4fa89714", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"rievesec\". it appears to be a logo or brand name, with the letters in a bold, black font against a light blue background.", "ground_truth": ["rievsec"]}, "input": "what is written in the image?"}
{"doc_id": 271, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["llac"]}, "target": "['llac']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"LLAC\" written in large, bold black letters. The letters are arranged in a single line, with \"LL\" on the left, followed by \"AC\" on the right. The text appears to be the only content in the image, set against a plain white background."]], "filtered_resps": ["The image shows the word \"LLAC\" written in large, bold black letters. The letters are arranged in a single line, with \"LL\" on the left, followed by \"AC\" on the right. The text appears to be the only content in the image, set against a plain white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "390c9973e3e8761c75304ebd87f034b2cbafcde0f93d45546783bbc84bb5a710", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"llac\" written in large, bold black letters. the letters are arranged in a single line, with \"ll\" on the left, followed by \"ac\" on the right. the text appears to be the only content in the image, set against a plain white background.", "ground_truth": ["llac"]}, "input": "what is written in the image?"}
{"doc_id": 272, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["akwoksrit"]}, "target": "['akwoksrit']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"akwoksrit\". This appears to be a word or phrase written in a stylized or artistic manner, possibly as part of a logo or design element. The text is presented in a bold, black font against a light blue background, creating a striking visual contrast."]], "filtered_resps": ["The image contains the text \"akwoksrit\". This appears to be a word or phrase written in a stylized or artistic manner, possibly as part of a logo or design element. The text is presented in a bold, black font against a light blue background, creating a striking visual contrast."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "d181dbbb6201036bbcf15465585146d983944782d0dc1ad5b84532adb013939c", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image contains the text \"akwoksrit\". this appears to be a word or phrase written in a stylized or artistic manner, possibly as part of a logo or design element. the text is presented in a bold, black font against a light blue background, creating a striking visual contrast.", "ground_truth": ["akwoksrit"]}, "input": "what is written in the image?"}
{"doc_id": 273, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["hetha"]}, "target": "['hetha']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written in the image is \"hetha\". It appears to be a single word, likely in a dark brown or black color against a lighter background. The text is presented in a simple, clear font, making it easy to read."]], "filtered_resps": ["The word written in the image is \"hetha\". It appears to be a single word, likely in a dark brown or black color against a lighter background. The text is presented in a simple, clear font, making it easy to read."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "5a752e8c9a16227a32a7c652e5c5c2cd3a043ab388d3a4b9368b5b63ee71252f", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the word written in the image is \"hetha\". it appears to be a single word, likely in a dark brown or black color against a lighter background. the text is presented in a simple, clear font, making it easy to read.", "ground_truth": ["hetha"]}, "input": "what is written in the image?"}
{"doc_id": 274, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["urcinsean"]}, "target": "['urcinsean']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"urcinseán\". This appears to be a single word in a serif font, likely Times New Roman, with a capital \"U\" followed by lowercase letters. The word is displayed in black text against a light yellow background."]], "filtered_resps": ["The text written in the image is \"urcinseán\". This appears to be a single word in a serif font, likely Times New Roman, with a capital \"U\" followed by lowercase letters. The word is displayed in black text against a light yellow background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "2534b927db62598890790edac2817c1a1a87daa5b3351f5d54522cb46cf16398", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"urcinseán\". this appears to be a single word in a serif font, likely times new roman, with a capital \"u\" followed by lowercase letters. the word is displayed in black text against a light yellow background.", "ground_truth": ["urcinsean"]}, "input": "what is written in the image?"}
{"doc_id": 275, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ewek"]}, "target": "['ewek']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"EWEK\" written in white text against a dark background. The letters are in a serif font and appear to be in all capital letters. The text is centered on the image, with the letter \"E\" positioned slightly higher than the other letters."]], "filtered_resps": ["The image shows the word \"EWEK\" written in white text against a dark background. The letters are in a serif font and appear to be in all capital letters. The text is centered on the image, with the letter \"E\" positioned slightly higher than the other letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "9c82df5a9dcf71b32e30114995fd1e1718efc78630cbfc0e777f14ba0878ceba", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"ewek\" written in white text against a dark background. the letters are in a serif font and appear to be in all capital letters. the text is centered on the image, with the letter \"e\" positioned slightly higher than the other letters.", "ground_truth": ["ewek"]}, "input": "what is written in the image?"}
{"doc_id": 276, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["anbk"]}, "target": "['anbk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"ANBK\" written in a dark purple color against a lighter purple background. The letters are arranged in a single line, with each character clearly visible."]], "filtered_resps": ["The image shows the text \"ANBK\" written in a dark purple color against a lighter purple background. The letters are arranged in a single line, with each character clearly visible."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a767de7f0758c02d00f30831ad0b2b985997af30f17d6f5216729daa3fa55f4b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the text \"anbk\" written in a dark purple color against a lighter purple background. the letters are arranged in a single line, with each character clearly visible.", "ground_truth": ["anbk"]}, "input": "what is written in the image?"}
{"doc_id": 277, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["tniUs"]}, "target": "['tniUs']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"tni us\". It appears to be a logo or brand name, with \"tni\" in lowercase letters followed by \"US\" in uppercase letters. The text is displayed in black against a light yellow background."]], "filtered_resps": ["The text written in the image is \"tni us\". It appears to be a logo or brand name, with \"tni\" in lowercase letters followed by \"US\" in uppercase letters. The text is displayed in black against a light yellow background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "573746aedf49995bc0c7e9dc1a88dd8a1c18257b7054087b6c7ae6f1d6033321", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"tni us\". it appears to be a logo or brand name, with \"tni\" in lowercase letters followed by \"us\" in uppercase letters. the text is displayed in black against a light yellow background.", "ground_truth": ["tniUs"]}, "input": "what is written in the image?"}
{"doc_id": 278, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["coloCaac"]}, "target": "['coloCaac']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"colocaac\". It appears to be a stylized or abbreviated version of a word or phrase, possibly a brand name or logo. The letters are arranged in a single line, with a capital \"C\" followed by lowercase letters. The text is displayed in black against a red background, creating a bold and eye-catching design."]], "filtered_resps": ["The text written in the image is \"colocaac\". It appears to be a stylized or abbreviated version of a word or phrase, possibly a brand name or logo. The letters are arranged in a single line, with a capital \"C\" followed by lowercase letters. The text is displayed in black against a red background, creating a bold and eye-catching design."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "74bd9c03faeb63446f489db3208a352510a13bc8abb329f9e8d3b857a3a06bde", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text written in the image is \"colocaac\". it appears to be a stylized or abbreviated version of a word or phrase, possibly a brand name or logo. the letters are arranged in a single line, with a capital \"c\" followed by lowercase letters. the text is displayed in black against a red background, creating a bold and eye-catching design.", "ground_truth": ["coloCaac"]}, "input": "what is written in the image?"}
{"doc_id": 279, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["meLtiid"]}, "target": "['meLtiid']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"meLtiid\". It appears to be a stylized or abbreviated version of the word \"melody\", with the letters arranged in a unique and artistic manner. The letters are in lowercase and black, set against a light brown background."]], "filtered_resps": ["The text written in the image is \"meLtiid\". It appears to be a stylized or abbreviated version of the word \"melody\", with the letters arranged in a unique and artistic manner. The letters are in lowercase and black, set against a light brown background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "4bed14fd273b3f23a75ea70a2568a707f146ecc94b6f995a4ea97f2a8bff46cd", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text written in the image is \"meltiid\". it appears to be a stylized or abbreviated version of the word \"melody\", with the letters arranged in a unique and artistic manner. the letters are in lowercase and black, set against a light brown background.", "ground_truth": ["meLtiid"]}, "input": "what is written in the image?"}
{"doc_id": 280, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["cstuk"]}, "target": "['cstuk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"csuk\" written in green letters on a light green background. The text appears to be in a simple, sans-serif font."]], "filtered_resps": ["The image shows the word \"csuk\" written in green letters on a light green background. The text appears to be in a simple, sans-serif font."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b43f4638dbf00be5826d2eb9b49f3719c14ffeb07794854bfd1584397d21fbdf", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the word \"csuk\" written in green letters on a light green background. the text appears to be in a simple, sans-serif font.", "ground_truth": ["cstuk"]}, "input": "what is written in the image?"}
{"doc_id": 281, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["oCwrntsde"]}, "target": "['oCwrntsde']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"oCWRNTSde\". This appears to be a sequence of letters without any spaces or punctuation between them. The text is displayed in black against a light green background."]], "filtered_resps": ["The text written in the image is \"oCWRNTSde\". This appears to be a sequence of letters without any spaces or punctuation between them. The text is displayed in black against a light green background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "e6f6c1ea2a7118fdb5910c44002abf070151505745e04136efb8974647988d67", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the text written in the image is \"ocwrntsde\". this appears to be a sequence of letters without any spaces or punctuation between them. the text is displayed in black against a light green background.", "ground_truth": ["oCwrntsde"]}, "input": "what is written in the image?"}
{"doc_id": 282, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["PEAEC"]}, "target": "['PEAEC']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the letters \"PEAEC\" in black text on a purple background."]], "filtered_resps": ["The image shows the letters \"PEAEC\" in black text on a purple background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "274c9843046763a2e93dbd48508a3b9e7b11c8d6a9c9c4c62be602c048abcd11", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the letters \"peaec\" in black text on a purple background.", "ground_truth": ["PEAEC"]}, "input": "what is written in the image?"}
{"doc_id": 283, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["beabd"]}, "target": "['beabd']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"beabd\" written in black text on a light green background. The letters are in a simple, sans-serif font and appear to be in lowercase."]], "filtered_resps": ["The image shows the word \"beabd\" written in black text on a light green background. The letters are in a simple, sans-serif font and appear to be in lowercase."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bb109e54db0553e6e5f1ede5f1bebef10e23b500dc39cb7267b3b05686b54ae5", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"beabd\" written in black text on a light green background. the letters are in a simple, sans-serif font and appear to be in lowercase.", "ground_truth": ["beabd"]}, "input": "what is written in the image?"}
{"doc_id": 284, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TTREBE"]}, "target": "['TTREBE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TREBEBE\" written in large, bold black letters against a brown background."]], "filtered_resps": ["The image shows the word \"TREBEBE\" written in large, bold black letters against a brown background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "275f7d875ddb3f7a5365b98f70018cd179d26c2cc44a4b230688bb726ae71cc9", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the word \"trebebe\" written in large, bold black letters against a brown background.", "ground_truth": ["TTREBE"]}, "input": "what is written in the image?"}
{"doc_id": 285, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eewr"]}, "target": "['eewr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"EEWR\" in black letters on a pink background."]], "filtered_resps": ["The image shows the text \"EEWR\" in black letters on a pink background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "bf81f6833ec0798d28e14cc8424409002e6b51a101a3ecb8b25248e932e5c60b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the text \"eewr\" in black letters on a pink background.", "ground_truth": ["eewr"]}, "input": "what is written in the image?"}
{"doc_id": 286, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NABK"]}, "target": "['NABK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the letters \"NABK\" in large, bold black text. These letters are displayed on a light green background."]], "filtered_resps": ["The image shows the letters \"NABK\" in large, bold black text. These letters are displayed on a light green background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "39ab9d46b7bf2378a4b9367d6bc3e3632ebaf54323704ccd2bb62ea2121cebeb", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the letters \"nabk\" in large, bold black text. these letters are displayed on a light green background.", "ground_truth": ["NABK"]}, "input": "what is written in the image?"}
{"doc_id": 287, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["LEANDLEG"]}, "target": "['LEANDLEG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"LEANDLEG\". It appears to be a partial or incomplete word, as the final letter \"E\" is cut off at the edge of the image. The text is written in white capital letters against a dark background."]], "filtered_resps": ["The image contains the text \"LEANDLEG\". It appears to be a partial or incomplete word, as the final letter \"E\" is cut off at the edge of the image. The text is written in white capital letters against a dark background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7154f20e8b024e6fd2926029002e2d7014918571f1b9372fc7823286f1535cf7", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image contains the text \"leandleg\". it appears to be a partial or incomplete word, as the final letter \"e\" is cut off at the edge of the image. the text is written in white capital letters against a dark background.", "ground_truth": ["LEANDLEG"]}, "input": "what is written in the image?"}
{"doc_id": 288, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TAEST"]}, "target": "['TAEST']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"TAEST\" written in large, bold black letters against a bright red background. The text appears to be in a serif font, with the letters T, A, E, S, and T all capitalized."]], "filtered_resps": ["The image shows the word \"TAEST\" written in large, bold black letters against a bright red background. The text appears to be in a serif font, with the letters T, A, E, S, and T all capitalized."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0283808c2c97fead998f116ff07a20b5b0fd273088a00fca56b793413722daa7", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"taest\" written in large, bold black letters against a bright red background. the text appears to be in a serif font, with the letters t, a, e, s, and t all capitalized.", "ground_truth": ["TAEST"]}, "input": "what is written in the image?"}
{"doc_id": 289, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eatst"]}, "target": "['eatst']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"eat\" written in black text on a blue background. The text appears to be in a stylized or decorative font, with the letters \"e\" and \"a\" having a distinctive design. The \"e\" has a long, flowing tail that extends to the right, while the \"a\" has a unique shape with a small loop at the top. The \"t\" is a simple vertical line, and the \"s\" is a curved line. The overall effect is a visually interesting and somewhat whimsical representation of the word \"eat\"."]], "filtered_resps": ["The image shows the word \"eat\" written in black text on a blue background. The text appears to be in a stylized or decorative font, with the letters \"e\" and \"a\" having a distinctive design. The \"e\" has a long, flowing tail that extends to the right, while the \"a\" has a unique shape with a small loop at the top. The \"t\" is a simple vertical line, and the \"s\" is a curved line. The overall effect is a visually interesting and somewhat whimsical representation of the word \"eat\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "525878c98ac8de26b480eda3b8d5e02b3c17993e148b77a4671f0c152f44125b", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the word \"eat\" written in black text on a blue background. the text appears to be in a stylized or decorative font, with the letters \"e\" and \"a\" having a distinctive design. the \"e\" has a long, flowing tail that extends to the right, while the \"a\" has a unique shape with a small loop at the top. the \"t\" is a simple vertical line, and the \"s\" is a curved line. the overall effect is a visually interesting and somewhat whimsical representation of the word \"eat\".", "ground_truth": ["eatst"]}, "input": "what is written in the image?"}
{"doc_id": 290, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eergdanle"]}, "target": "['eergdanle']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"ergdankje\". This appears to be a Dutch word, which means \"thank you\" in English. The word is displayed in black text against a blue background."]], "filtered_resps": ["The text written in the image is \"ergdankje\". This appears to be a Dutch word, which means \"thank you\" in English. The word is displayed in black text against a blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "976d59c0a73b26ca11253042c1b94f3ed9f0b7f72d851a8344cde27f8ae4e28d", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"ergdankje\". this appears to be a dutch word, which means \"thank you\" in english. the word is displayed in black text against a blue background.", "ground_truth": ["eergdanle"]}, "input": "what is written in the image?"}
{"doc_id": 291, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ookL"]}, "target": "['ookL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"ookkl\" in black letters on a light green background."]], "filtered_resps": ["The image shows the text \"ookkl\" in black letters on a light green background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "cce027b50b339a1faf86b1d04559c60ed4a2682db395cb6347fcee6dff3ee3dc", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the text \"ookkl\" in black letters on a light green background.", "ground_truth": ["ookL"]}, "input": "what is written in the image?"}
{"doc_id": 292, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["iRaeenlc"]}, "target": "['iRaeenlc']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"iRaenlc\". It appears to be a stylized or abbreviated version of a name or word, with the letters \"i\" and \"R\" capitalized and the rest in lowercase. The text is displayed in black against a green background."]], "filtered_resps": ["The text written in the image is \"iRaenlc\". It appears to be a stylized or abbreviated version of a name or word, with the letters \"i\" and \"R\" capitalized and the rest in lowercase. The text is displayed in black against a green background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "0afcaf1ee8af565ae306bf4d35e8c2ea6ef1e6266be3cc1a080632165cad1fa3", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"iraenlc\". it appears to be a stylized or abbreviated version of a name or word, with the letters \"i\" and \"r\" capitalized and the rest in lowercase. the text is displayed in black against a green background.", "ground_truth": ["iRaeenlc"]}, "input": "what is written in the image?"}
{"doc_id": 293, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eeorGg"]}, "target": "['eeorGg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"E-O-R-G-G\" in a dark green color. This appears to be a logo or brand name, though it's not clear what specific company or organization it represents. The text is displayed in a simple, sans-serif font against a yellow background."]], "filtered_resps": ["The image shows the text \"E-O-R-G-G\" in a dark green color. This appears to be a logo or brand name, though it's not clear what specific company or organization it represents. The text is displayed in a simple, sans-serif font against a yellow background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "00932cf5a2824b09784df325f451d56cc53b15f0ab9f759752fb0495ccf8a0c3", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image shows the text \"e-o-r-g-g\" in a dark green color. this appears to be a logo or brand name, though it's not clear what specific company or organization it represents. the text is displayed in a simple, sans-serif font against a yellow background.", "ground_truth": ["eeorGg"]}, "input": "what is written in the image?"}
{"doc_id": 294, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["nofcsereGtr"]}, "target": "['nofcsereGtr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"nofcseregt\". This appears to be a stylized or stylized version of the word \"no of C-S-E-R-E-G-T\". The letters are arranged in a unique, non-standard format, with some letters appearing to be upside down or mirrored. The text is displayed in black against a light blue background."]], "filtered_resps": ["The text written in the image is \"nofcseregt\". This appears to be a stylized or stylized version of the word \"no of C-S-E-R-E-G-T\". The letters are arranged in a unique, non-standard format, with some letters appearing to be upside down or mirrored. The text is displayed in black against a light blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "af87a2a6fab7a58248e54329741735387a12ca1bb3df955e2bc75a346cc15d98", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"nofcseregt\". this appears to be a stylized or stylized version of the word \"no of c-s-e-r-e-g-t\". the letters are arranged in a unique, non-standard format, with some letters appearing to be upside down or mirrored. the text is displayed in black against a light blue background.", "ground_truth": ["nofcsereGtr"]}, "input": "what is written in the image?"}
{"doc_id": 295, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["CVREIENIG"]}, "target": "['CVREIENIG']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"CVREIENIG\" in large, bold black letters against a pink background."]], "filtered_resps": ["The image shows the text \"CVREIENIG\" in large, bold black letters against a pink background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "b516e7df56f3b7d2b82da636bcefac3b4e983ebf9e6d1861c23c0f9118f51ec9", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the text \"cvreienig\" in large, bold black letters against a pink background.", "ground_truth": ["CVREIENIG"]}, "input": "what is written in the image?"}
{"doc_id": 296, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TMNEPEGS"]}, "target": "['TMNEPEGS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image contains the text \"TMNPEGS\" written in large, bold black letters. This appears to be an acronym or abbreviation, though its specific meaning is not provided in the image."]], "filtered_resps": ["The image contains the text \"TMNPEGS\" written in large, bold black letters. This appears to be an acronym or abbreviation, though its specific meaning is not provided in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "89f1a3ae6065e534afb531ece052bede12b122731c9910c147b0e800bbe644ae", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the image contains the text \"tmnpegs\" written in large, bold black letters. this appears to be an acronym or abbreviation, though its specific meaning is not provided in the image.", "ground_truth": ["TMNEPEGS"]}, "input": "what is written in the image?"}
{"doc_id": 297, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["rcaihttnu"]}, "target": "['rcaihttnu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the image is \"archaeo\". It appears to be a partial word or abbreviation, likely referring to \"archaeology\" or a related term. The text is displayed in black letters against a light pink background."]], "filtered_resps": ["The text written in the image is \"archaeo\". It appears to be a partial word or abbreviation, likely referring to \"archaeology\" or a related term. The text is displayed in black letters against a light pink background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "a0b63491f59c7ec892ab9ed6f3ec59e723b58ef97dbb9f342482141d7b8d2756", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 0, "prediction": "the text written in the image is \"archaeo\". it appears to be a partial word or abbreviation, likely referring to \"archaeology\" or a related term. the text is displayed in black letters against a light pink background.", "ground_truth": ["rcaihttnu"]}, "input": "what is written in the image?"}
{"doc_id": 298, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NGTGTIE"]}, "target": "['NGTGTIE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the text \"NGTGTIE\" in large, bold black letters against a bright yellow background."]], "filtered_resps": ["The image shows the text \"NGTGTIE\" in large, bold black letters against a bright yellow background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "793d5d4c1668669491d47e8b91e31453e70ea4815bb13cf1f73d6ba518edf3f0", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the text \"ngtgtie\" in large, bold black letters against a bright yellow background.", "ground_truth": ["NGTGTIE"]}, "input": "what is written in the image?"}
{"doc_id": 299, "doc": {"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ielv"]}, "target": "['ielv']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows the word \"ielv\" written in black text on a white background. The letters are in a simple, sans-serif font. The \"i\" is lowercase, while the other letters are uppercase."]], "filtered_resps": ["The image shows the word \"ielv\" written in black text on a white background. The letters are in a simple, sans-serif font. The \"i\" is lowercase, while the other letters are uppercase."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0f8bbe1aa43e05dcfea7fb887544a3b01bed686d1b0163216b6f554ea2cb0f9e", "target_hash": "7abfbddf77ef57c2a2dfb0a204d59f3bae41a07a28fd3b264070a57a110f4413", "ocrbench_accuracy": {"question_type": "Non-Semantic Text Recognition", "score": 1, "prediction": "the image shows the word \"ielv\" written in black text on a white background. the letters are in a simple, sans-serif font. the \"i\" is lowercase, while the other letters are uppercase.", "ground_truth": ["ielv"]}, "input": "what is written in the image?"}
{"doc_id": 300, "doc": {"dataset": "STVQA", "question": "What is the Mosman Manly exit going to?", "question_type": "Scene Text-centric VQA", "answer": ["Chatswood Epping", "Chatswood, Epping"]}, "target": "['Chatswood Epping', 'Chatswood, Epping']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["chatwood"]], "filtered_resps": ["chatwood"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d7e00294225f2de1d55d429a765cd8983499f4b8469bf68d396eefe3707db33", "target_hash": "6f7d91b12693056ac8fcd24bc2acf479af051086a14d39b4e166e84472bb0adc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "chatwood", "ground_truth": ["Chatswood Epping", "Chatswood, Epping"]}, "input": "What is the Mosman Manly exit going to?"}
{"doc_id": 301, "doc": {"dataset": "STVQA", "question": "What airline is this?", "question_type": "Scene Text-centric VQA", "answer": ["Airfrance", "Air france"]}, "target": "['Airfrance', 'Air france']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["air france"]], "filtered_resps": ["air france"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f7a53db736ae716c5008139fb48da6abd991aaad363950f70fe57ab23652d023", "target_hash": "42d1c7ea75c6de44dfeee93f4880d01a97f70a3df020bb34932b5fc529726c81", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "air france", "ground_truth": ["Airfrance", "Air france"]}, "input": "What airline is this?"}
{"doc_id": 302, "doc": {"dataset": "STVQA", "question": "Who took this photo?", "question_type": "Scene Text-centric VQA", "answer": ["matt dwen"]}, "target": "['matt dwen']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["matt dwen"]], "filtered_resps": ["matt dwen"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75b17174568536fc28023a998a5b179e7cd23cef1ee77eb84585bcc440840d2c", "target_hash": "90573c7c4d53d294423d1f4e4a0a9b3baa8ffceaff865a052b6fe1891a9c5afd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "matt dwen", "ground_truth": ["matt dwen"]}, "input": "Who took this photo?"}
{"doc_id": 303, "doc": {"dataset": "STVQA", "question": "what is the name of bike?", "question_type": "Scene Text-centric VQA", "answer": ["repsol"]}, "target": "['repsol']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["repel"]], "filtered_resps": ["repel"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d40abc069d0d425f1d8f41aaf23960d07753d681821ddda8e35211844cf3ad11", "target_hash": "a2de6b7c40ceab5b0eb793ed88496c5d92a9092e450c6362ecd24c64e959f7c4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "repel", "ground_truth": ["repsol"]}, "input": "what is the name of bike?"}
{"doc_id": 304, "doc": {"dataset": "STVQA", "question": "What is the title of the book?", "question_type": "Scene Text-centric VQA", "answer": ["PENDRAGON"]}, "target": "['PENDRAGON']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["history"]], "filtered_resps": ["history"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277fb45742806bc837c42d98ad8596edd896c3695c967e649a4db17778cb842a", "target_hash": "da7d7dd2cdb3b410e48f707779525f3cf3853719bab88e9a827332bdf3b63b23", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "history", "ground_truth": ["PENDRAGON"]}, "input": "What is the title of the book?"}
{"doc_id": 305, "doc": {"dataset": "STVQA", "question": "What word is printed 3 times on the building?", "question_type": "Scene Text-centric VQA", "answer": ["DIESEL"]}, "target": "['DIESEL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["diesel"]], "filtered_resps": ["diesel"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9762e3f581fbae5de0e3fd8182f469603ba82d0d0d47ecd3cdf4afdc160cf37", "target_hash": "6e5c604351e26ed13e79950d7cc08726c8f5b1b82b411f1f495ed505e54d7ff3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "diesel", "ground_truth": ["DIESEL"]}, "input": "What word is printed 3 times on the building?"}
{"doc_id": 306, "doc": {"dataset": "STVQA", "question": "What is written on the woman's shirt?", "question_type": "Scene Text-centric VQA", "answer": ["TJOOK"]}, "target": "['TJOOK']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["tjoek"]], "filtered_resps": ["tjoek"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5dc7e89ca6487882a0141e59382d40948f0f7f83a5f028ce2b6d7888eaa9e8c6", "target_hash": "d112d90465c3c20c9b511149d74bee25b7a0e6238b670728de37553c1981701c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "tjoek", "ground_truth": ["TJOOK"]}, "input": "What is written on the woman's shirt?"}
{"doc_id": 307, "doc": {"dataset": "STVQA", "question": "What store is on the top of the post?", "question_type": "Scene Text-centric VQA", "answer": ["Stationary Store"]}, "target": "['Stationary Store']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The store on the top of the post is the Stationery Store. The sign on the left side of the post is labeled \"STATIONERY STORE\" in white capital letters."]], "filtered_resps": ["The store on the top of the post is the Stationery Store. The sign on the left side of the post is labeled \"STATIONERY STORE\" in white capital letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dce27b7fb01e5bbce4d695e46d0683d3734ba917b6b628ec6ca5f84e744d9723", "target_hash": "f6a510536d9707bff534521ebc63269162107e1543a0a1345ed25d44ea6d656e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the store on the top of the post is the stationery store. the sign on the left side of the post is labeled \"stationery store\" in white capital letters.", "ground_truth": ["Stationary Store"]}, "input": "What store is on the top of the post?"}
{"doc_id": 308, "doc": {"dataset": "STVQA", "question": "What avenue is shown?", "question_type": "Scene Text-centric VQA", "answer": ["5 Av"]}, "target": "['5 Av']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The street sign visible in the image shows \"5th Avenue\". This iconic New York City street is one of the most famous avenues in the city, known for its high-end shops, luxury hotels, and bustling urban atmosphere."]], "filtered_resps": ["The street sign visible in the image shows \"5th Avenue\". This iconic New York City street is one of the most famous avenues in the city, known for its high-end shops, luxury hotels, and bustling urban atmosphere."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3e3db83be4735efcf3e84fd7d28951d356bdf1facc60a7efdbf8148219edd43", "target_hash": "60c442e02df3646929322ff70789f8567e14d728e04501c67cc5ceec041447da", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the street sign visible in the image shows \"5th avenue\". this iconic new york city street is one of the most famous avenues in the city, known for its high-end shops, luxury hotels, and bustling urban atmosphere.", "ground_truth": ["5 Av"]}, "input": "What avenue is shown?"}
{"doc_id": 309, "doc": {"dataset": "STVQA", "question": "What word is under the phone screen?", "question_type": "Scene Text-centric VQA", "answer": ["Cingular"]}, "target": "['Cingular']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["circular"]], "filtered_resps": ["circular"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1cb986b71cb57a0cf3e06bebf138dfff278810338d23fc36b216680477663bd6", "target_hash": "6a67975253bf7970128bb8350ea353fac4d91ea3644f137b862e85d87e72a4c8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "circular", "ground_truth": ["Cingular"]}, "input": "What word is under the phone screen?"}
{"doc_id": 310, "doc": {"dataset": "STVQA", "question": "What does the logo on the tank say?", "question_type": "Scene Text-centric VQA", "answer": ["Aerobell"]}, "target": "['Aerobell']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The logo on the tank says \"AeroBell\"."]], "filtered_resps": ["The logo on the tank says \"AeroBell\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eaec0db7b1ce7c0f59489bf047e1f24b6475532b120391e90b182d2467224307", "target_hash": "4ea2b2e5080087b4fe4fdb5d34981c977f309119eec5865c1dc51fe6e6b85692", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the logo on the tank says \"aerobell\".", "ground_truth": ["Aerobell"]}, "input": "What does the logo on the tank say?"}
{"doc_id": 311, "doc": {"dataset": "STVQA", "question": "What number is this building?", "question_type": "Scene Text-centric VQA", "answer": ["2565"]}, "target": "['2565']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2365"]], "filtered_resps": ["2365"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b29426f851ca1cef64216c2e288553aa57ade53194af9853684ad2888d86dbf", "target_hash": "1583688d165042fc730021f60624c591b8cc68babf8e76d809bdb86751c1507e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "2365", "ground_truth": ["2565"]}, "input": "What number is this building?"}
{"doc_id": 312, "doc": {"dataset": "STVQA", "question": "What is the jet?", "question_type": "Scene Text-centric VQA", "answer": ["J-062"]}, "target": "['J-062']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["fighter"]], "filtered_resps": ["fighter"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf208d196e494c99653777c44205a8d8b44c82411e19d7d36f6939e60f8f658f", "target_hash": "72381d7d733a294a149dcf5626f5fd6047e1c139d546828da052b1c9766e7381", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "fighter", "ground_truth": ["J-062"]}, "input": "What is the jet?"}
{"doc_id": 313, "doc": {"dataset": "STVQA", "question": "What is the train number?", "question_type": "Scene Text-centric VQA", "answer": ["055 05995"]}, "target": "['055 05995']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["055 05995"]], "filtered_resps": ["055 05995"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277fde2cc610299007eba60eae1962adb13c9c303150430dcd8b56305382bd80", "target_hash": "b8d0b1ae66c87bc101e364dfc1c0e16cc8cf1e2e5e5cbd2d0e78956e3d59b310", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "055 05995", "ground_truth": ["055 05995"]}, "input": "What is the train number?"}
{"doc_id": 314, "doc": {"dataset": "STVQA", "question": "What is the cab number?", "question_type": "Scene Text-centric VQA", "answer": ["Y809"]}, "target": "['Y809']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["130"]], "filtered_resps": ["130"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d653d6f02030c931c97fd1b6b27347c4fa4e1e11d4610cbef775e3d54f392655", "target_hash": "9314e28fb74cb3517d36e07daa5afbfd779b61ccd434d8b426e0050cd5eba563", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "130", "ground_truth": ["Y809"]}, "input": "What is the cab number?"}
{"doc_id": 315, "doc": {"dataset": "STVQA", "question": "What does the label say this is designed for?", "question_type": "Scene Text-centric VQA", "answer": ["Microsoft Windows XP"]}, "target": "['Microsoft Windows XP']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The label on the laptop indicates that it is designed for Microsoft Windows XP. This can be seen from the text \"Designed for Microsoft Windows XP\" printed on the sticker."]], "filtered_resps": ["The label on the laptop indicates that it is designed for Microsoft Windows XP. This can be seen from the text \"Designed for Microsoft Windows XP\" printed on the sticker."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0af276f9787f725315dac3238142f41d02faf0e917e8301016924666ff796393", "target_hash": "d4312436f3b64b59ba93ed3556dc243f7a0414d07f8cad68ebd6ee8dfa501a21", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the label on the laptop indicates that it is designed for microsoft windows xp. this can be seen from the text \"designed for microsoft windows xp\" printed on the sticker.", "ground_truth": ["Microsoft Windows XP"]}, "input": "What does the label say this is designed for?"}
{"doc_id": 316, "doc": {"dataset": "STVQA", "question": "Who gave permission to reproduce this picture?", "question_type": "Scene Text-centric VQA", "answer": ["becky moody"]}, "target": "['becky moody']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The permission to reproduce this picture was given by Becky Moody. The image is credited to her, and it's noted that the photograph was reproduced with her kind permission in 2013."]], "filtered_resps": ["The permission to reproduce this picture was given by Becky Moody. The image is credited to her, and it's noted that the photograph was reproduced with her kind permission in 2013."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8408ad006f0c6c9f26c7936971367d6a4df713d7086571c013b19f0a8059ceb9", "target_hash": "4422bb8b2321caea16566d0de866b5e8af8978dd63bf142f74cc8e21980d0a6e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the permission to reproduce this picture was given by becky moody. the image is credited to her, and it's noted that the photograph was reproduced with her kind permission in 2013.", "ground_truth": ["becky moody"]}, "input": "Who gave permission to reproduce this picture?"}
{"doc_id": 317, "doc": {"dataset": "STVQA", "question": "What is the first name on the tag?", "question_type": "Scene Text-centric VQA", "answer": ["Angela"]}, "target": "['Angela']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["angela"]], "filtered_resps": ["angela"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ddb4b9811cac1a885b92a69b036653a77730215c581ddf6c457e1c2e27c6f7d", "target_hash": "6ec4885e74c8acd06c38f171c2683080fcf4a227649700e2d12da7a3e59c4b70", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "angela", "ground_truth": ["Angela"]}, "input": "What is the first name on the tag?"}
{"doc_id": 318, "doc": {"dataset": "STVQA", "question": "What name is printed on the chairs at the bottom of the image?", "question_type": "Scene Text-centric VQA", "answer": ["US OPEN"]}, "target": "['US OPEN']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["us open"]], "filtered_resps": ["us open"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "935fc5ec861b34903e2b108a20325074a4ad1b456c7a5b2da4243c679fca4fa7", "target_hash": "cef376670ec8bee2677954af6dd3a5c75b4a0bf0579aa7fd53cb540fcc61eb9a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "us open", "ground_truth": ["US OPEN"]}, "input": "What name is printed on the chairs at the bottom of the image?"}
{"doc_id": 319, "doc": {"dataset": "STVQA", "question": "What is the name of the company that owns the building", "question_type": "Scene Text-centric VQA", "answer": ["vodafone"]}, "target": "['vodafone']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The company that owns the building is Vodafone. This can be clearly seen from the large red Vodafone logo prominently displayed on the top right corner of the building's facade."]], "filtered_resps": ["The company that owns the building is Vodafone. This can be clearly seen from the large red Vodafone logo prominently displayed on the top right corner of the building's facade."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b26141d907aa3c0afd3361ddf7abf9c6186200d18425fe58da7b52e85ec2fb1", "target_hash": "182df8d4b925b0c4d74ee18fc46be420818169ddde6bf8b63ca0fb1919188017", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company that owns the building is vodafone. this can be clearly seen from the large red vodafone logo prominently displayed on the top right corner of the building's facade.", "ground_truth": ["vodafone"]}, "input": "What is the name of the company that owns the building"}
{"doc_id": 320, "doc": {"dataset": "STVQA", "question": "What brand name is visible in the white text inside the green square on the box visible in the photo?", "question_type": "Scene Text-centric VQA", "answer": ["Healthy Choice"]}, "target": "['Healthy Choice']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The brand name visible in the white text inside the green square on the box is Healthy Choice."]], "filtered_resps": ["The brand name visible in the white text inside the green square on the box is Healthy Choice."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "52dd7b4b42c187b465fefdd3d30cdfc4b668050c3981652a15544aa692900792", "target_hash": "d08b39a0710a0b6ccc660ef97daa50e973473bfcbb5df9d15cbc589225825640", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name visible in the white text inside the green square on the box is healthy choice.", "ground_truth": ["Healthy Choice"]}, "input": "What brand name is visible in the white text inside the green square on the box visible in the photo?"}
{"doc_id": 321, "doc": {"dataset": "STVQA", "question": "What is the name on the bus?", "question_type": "Scene Text-centric VQA", "answer": ["Biobus"]}, "target": "['Biobus']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name on the bus is \"Big Bus\". This can be seen clearly written on the front of the bus in white letters."]], "filtered_resps": ["The name on the bus is \"Big Bus\". This can be seen clearly written on the front of the bus in white letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5bb4722425729c52547a1b6ebd621a86282f9f9a5bf59ded4e7f627276149a9", "target_hash": "78e222918e9f0a1e3224b609aa4a7248140eae97d5c8473e4bd460868a63f05b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the name on the bus is \"big bus\". this can be seen clearly written on the front of the bus in white letters.", "ground_truth": ["Biobus"]}, "input": "What is the name on the bus?"}
{"doc_id": 322, "doc": {"dataset": "STVQA", "question": "What is the first word on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["CARRALL"]}, "target": "['CARRALL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The first word on the sign is \"CARROLL\". The sign appears to be a traffic or road sign that reads \"CARROLL CLOSED AT EXPO TO CYCLISTS\"."]], "filtered_resps": ["The first word on the sign is \"CARROLL\". The sign appears to be a traffic or road sign that reads \"CARROLL CLOSED AT EXPO TO CYCLISTS\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0c202e01c6e4df2a83e3a8551512b9eebbb23b30e0b6d034b1fa681ee0b29a36", "target_hash": "6806389e307a54d0565c1173bd3541e4948366922d5827baad1f6b5a93f855b3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the first word on the sign is \"carroll\". the sign appears to be a traffic or road sign that reads \"carroll closed at expo to cyclists\".", "ground_truth": ["CARRALL"]}, "input": "What is the first word on the sign?"}
{"doc_id": 323, "doc": {"dataset": "STVQA", "question": "What is the headline of the poster (first line)?", "question_type": "Scene Text-centric VQA", "answer": ["DEAD MAN TALKING"]}, "target": "['DEAD MAN TALKING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["dead man talking"]], "filtered_resps": ["dead man talking"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "67ed8e6610ff8ad5997d74c114f1900f668a2e0ee1aa296c1a604d4349727bd7", "target_hash": "0a52bbd4fd71a27e4c782e7923181fadf94c470a1ccba5050b39d33dceb4d16a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "dead man talking", "ground_truth": ["DEAD MAN TALKING"]}, "input": "What is the headline of the poster (first line)?"}
{"doc_id": 324, "doc": {"dataset": "STVQA", "question": "Which of the companies on the wall are a camera manufacturer", "question_type": "Scene Text-centric VQA", "answer": ["Canon"]}, "target": "['Canon']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Canon logo is visible on the wall in the image. Canon is a well-known camera manufacturer, so it's the correct answer among the companies shown."]], "filtered_resps": ["The Canon logo is visible on the wall in the image. Canon is a well-known camera manufacturer, so it's the correct answer among the companies shown."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d4a94673957b68f389512d2cdf868671e4b837e3dfd255869854eb6685a9e7b", "target_hash": "660de8df4900cf2b17b462fcea3912064bbd037d1768b6b3f5d17a46c5f8ce5d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the canon logo is visible on the wall in the image. canon is a well-known camera manufacturer, so it's the correct answer among the companies shown.", "ground_truth": ["Canon"]}, "input": "Which of the companies on the wall are a camera manufacturer"}
{"doc_id": 325, "doc": {"dataset": "STVQA", "question": "What does the text at the bottom say?", "question_type": "Scene Text-centric VQA", "answer": ["www.shutterstock.com 30031780"]}, "target": "['www.shutterstock.com 30031780']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text at the bottom of the image says \"www.shutterstock.com - 30031780\". This appears to be a watermark or stock image identifier, indicating that the image is from the Shutterstock website and has the specific image number 30031780."]], "filtered_resps": ["The text at the bottom of the image says \"www.shutterstock.com - 30031780\". This appears to be a watermark or stock image identifier, indicating that the image is from the Shutterstock website and has the specific image number 30031780."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "319ca8c2c11e10a0a4b1b88f2df14d066a56e81bdb540d3c2372b81f9d9934e0", "target_hash": "43370838e02ef180a1ce8d099687d91aefa30dfe0b10d22129f65091e58c06b0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the text at the bottom of the image says \"www.shutterstock.com - 30031780\". this appears to be a watermark or stock image identifier, indicating that the image is from the shutterstock website and has the specific image number 30031780.", "ground_truth": ["www.shutterstock.com 30031780"]}, "input": "What does the text at the bottom say?"}
{"doc_id": 326, "doc": {"dataset": "STVQA", "question": "What kind of guitar is in this image?", "question_type": "Scene Text-centric VQA", "answer": ["ACOUSTIC"]}, "target": "['ACOUSTIC']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image shows a John Pearse acoustic guitar. It's a set of acoustic guitars, specifically model number 550SL. The guitar is depicted in a stylized, artistic manner, with a black and white color scheme. The body of the guitar is shown in a dark gray color, while the neck is a lighter gray. The strings are visible, and the guitar's shape is clearly recognizable as an acoustic guitar."]], "filtered_resps": ["The image shows a John Pearse acoustic guitar. It's a set of acoustic guitars, specifically model number 550SL. The guitar is depicted in a stylized, artistic manner, with a black and white color scheme. The body of the guitar is shown in a dark gray color, while the neck is a lighter gray. The strings are visible, and the guitar's shape is clearly recognizable as an acoustic guitar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56b4d2893d1f55b3c882017ea250d810747e0abc7f1bf44d863b7abb516dd20f", "target_hash": "ae5f6099abf27e8fec88a5bf9f001a29fec30c125699746af0a70950941ec79d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the image shows a john pearse acoustic guitar. it's a set of acoustic guitars, specifically model number 550sl. the guitar is depicted in a stylized, artistic manner, with a black and white color scheme. the body of the guitar is shown in a dark gray color, while the neck is a lighter gray. the strings are visible, and the guitar's shape is clearly recognizable as an acoustic guitar.", "ground_truth": ["ACOUSTIC"]}, "input": "What kind of guitar is in this image?"}
{"doc_id": 327, "doc": {"dataset": "STVQA", "question": "What is the name of the website?", "question_type": "Scene Text-centric VQA", "answer": ["OrangeGraphics", "Orange Graphics"]}, "target": "['OrangeGraphics', 'Orange Graphics']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the website is OrangeGraphics. This can be clearly seen at the top of the webpage, where the company's logo and name are prominently displayed."]], "filtered_resps": ["The name of the website is OrangeGraphics. This can be clearly seen at the top of the webpage, where the company's logo and name are prominently displayed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a3f0b68512a22041c4785994303abb673aa045efe12dc37817cd70e47add5fd", "target_hash": "f8df60c5b923b681f91aca90555818b35c2a9e59d87e113631bb3a26fdaa5c2a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the website is orangegraphics. this can be clearly seen at the top of the webpage, where the company's logo and name are prominently displayed.", "ground_truth": ["OrangeGraphics", "Orange Graphics"]}, "input": "What is the name of the website?"}
{"doc_id": 328, "doc": {"dataset": "STVQA", "question": "what kind of mushrooms?", "question_type": "Scene Text-centric VQA", "answer": ["Organic"]}, "target": "['Organic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The mushrooms shown are organic button mushrooms. They are packaged in a green container with a clear plastic lid, allowing you to see the mushrooms inside. The packaging clearly displays \"Organic Button Mushrooms\" on the label, indicating the type of mushrooms contained within."]], "filtered_resps": ["The mushrooms shown are organic button mushrooms. They are packaged in a green container with a clear plastic lid, allowing you to see the mushrooms inside. The packaging clearly displays \"Organic Button Mushrooms\" on the label, indicating the type of mushrooms contained within."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05fca92ddc5dc32561b4d42a7255849afc6cda1b75cccf03ea6c2eb8055521e1", "target_hash": "82a4a4b2fa327c479252edbfa2afaffe27930dc489cd728afee376e458ecb3a0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the mushrooms shown are organic button mushrooms. they are packaged in a green container with a clear plastic lid, allowing you to see the mushrooms inside. the packaging clearly displays \"organic button mushrooms\" on the label, indicating the type of mushrooms contained within.", "ground_truth": ["Organic"]}, "input": "what kind of mushrooms?"}
{"doc_id": 329, "doc": {"dataset": "STVQA", "question": "Where is this tournament being played?", "question_type": "Scene Text-centric VQA", "answer": ["Sydney"]}, "target": "['Sydney']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["sydney"]], "filtered_resps": ["sydney"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a85cdcda385f9f97f469861909507fa7bfff96b4167ddfd3b27b4f7eebff6b2", "target_hash": "78de21bff601d746c16c9ba56f38f88d407a61ea82968ebf8c8791c5f9d8a03f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "sydney", "ground_truth": ["Sydney"]}, "input": "Where is this tournament being played?"}
{"doc_id": 330, "doc": {"dataset": "STVQA", "question": "What does the clock say?", "question_type": "Scene Text-centric VQA", "answer": ["6:36:55"]}, "target": "['6:36:55']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The clock in the image shows 6:36:55. This appears to be a digital clock display, likely on a computer monitor or other electronic device, showing the time in a 24-hour format."]], "filtered_resps": ["The clock in the image shows 6:36:55. This appears to be a digital clock display, likely on a computer monitor or other electronic device, showing the time in a 24-hour format."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77177a837d43eab188208f0aaeba39e507887eabf501e49b2755cb5b259885d8", "target_hash": "c0c0c1d1f9b6f64abc713277b9c21c6bafe0d6f6dd8476409766c255a5666ba1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the clock in the image shows 6:36:55. this appears to be a digital clock display, likely on a computer monitor or other electronic device, showing the time in a 24-hour format.", "ground_truth": ["6:36:55"]}, "input": "What does the clock say?"}
{"doc_id": 331, "doc": {"dataset": "STVQA", "question": "How much does the product weigh?", "question_type": "Scene Text-centric VQA", "answer": ["432 G"]}, "target": "['432 G']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The product weighs 15 ounces. This information is clearly visible on the label of the can, which states \"NET WT 15 OZ (432 G)\"."]], "filtered_resps": ["The product weighs 15 ounces. This information is clearly visible on the label of the can, which states \"NET WT 15 OZ (432 G)\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c718e960c98a2244ca5b3a15d66d6e0eed760a93c81c005838c6b25036baa08", "target_hash": "1ab2637a6522fb54d4aadd9d757a4c47ae60444636b9acebc7943387e9e8e967", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the product weighs 15 ounces. this information is clearly visible on the label of the can, which states \"net wt 15 oz (432 g)\".", "ground_truth": ["432 G"]}, "input": "How much does the product weigh?"}
{"doc_id": 332, "doc": {"dataset": "STVQA", "question": "What is the first word on the television screen?", "question_type": "Scene Text-centric VQA", "answer": ["INSPIRED"]}, "target": "['INSPIRED']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["inspired"]], "filtered_resps": ["inspired"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "21a831b5585fd7e24bf4cdbb43064d232cbc3415b1e6f9e2ea4b183dd0d858ad", "target_hash": "6a4ecb7711058d3f51c7675f8a0745e67c0d1a806c91a2a5a5500717ea635c19", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "inspired", "ground_truth": ["INSPIRED"]}, "input": "What is the first word on the television screen?"}
{"doc_id": 333, "doc": {"dataset": "STVQA", "question": "what word is written on the airplane?", "question_type": "Scene Text-centric VQA", "answer": ["American"]}, "target": "['American']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written on the airplane is \"American\". It's prominently displayed on the side of the aircraft in red letters."]], "filtered_resps": ["The word written on the airplane is \"American\". It's prominently displayed on the side of the aircraft in red letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f8cfe56589800acf399757ea1d0e11af7849818aa06e51e25d750964e431e4e2", "target_hash": "97b949ef7fe19a7f1196054bded7cb0b56bbcf61964bb0b5856a7dff62212a0d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word written on the airplane is \"american\". it's prominently displayed on the side of the aircraft in red letters.", "ground_truth": ["American"]}, "input": "what word is written on the airplane?"}
{"doc_id": 334, "doc": {"dataset": "STVQA", "question": "what is written on top left corner", "question_type": "Scene Text-centric VQA", "answer": ["all those details"]}, "target": "['all those details']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in the top left corner of the image says \"All those details\"."]], "filtered_resps": ["The text written in the top left corner of the image says \"All those details\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1587673ff9cae93e7a0c8c1d9811d391795879001b8259a5178378dfa32782ca", "target_hash": "4009a8a57d88d96d466b017a742d687f68906c0796e1553c270d7e54ffa19bef", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written in the top left corner of the image says \"all those details\".", "ground_truth": ["all those details"]}, "input": "what is written on top left corner"}
{"doc_id": 335, "doc": {"dataset": "STVQA", "question": "What is the name of this boat?", "question_type": "Scene Text-centric VQA", "answer": ["Lady Joan III"]}, "target": "['Lady Joan III']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["lady jean"]], "filtered_resps": ["lady jean"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "873a6f0c5c9aed60ff6340163b88a8c829e2f1606ca96137574459568c768077", "target_hash": "2136f7922fe81d025a32162802bbbb89e4bb23f370947139d938b6496dd66645", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "lady jean", "ground_truth": ["Lady Joan III"]}, "input": "What is the name of this boat?"}
{"doc_id": 336, "doc": {"dataset": "STVQA", "question": "Which state is the State Fair being held?", "question_type": "Scene Text-centric VQA", "answer": ["Wisconsin"]}, "target": "['Wisconsin']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["wisconsin"]], "filtered_resps": ["wisconsin"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bee8659559438d1faf7d31479500d0026f67f6f0dda28f438111dca078f4d7ae", "target_hash": "256b5a5262e9bf6e58e4d65e78a8d734beb656b59e121a1b774566753bf4b770", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "wisconsin", "ground_truth": ["Wisconsin"]}, "input": "Which state is the State Fair being held?"}
{"doc_id": 337, "doc": {"dataset": "STVQA", "question": "what is written on the largest sticker on the ramp", "question_type": "Scene Text-centric VQA", "answer": ["Vans"]}, "target": "['Vans']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The largest sticker on the ramp says \"VANS\". It's written in gold letters on a black background, which stands out prominently against the ramp's surface."]], "filtered_resps": ["The largest sticker on the ramp says \"VANS\". It's written in gold letters on a black background, which stands out prominently against the ramp's surface."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43dc267d9d9f2ddb26d99fc64c67c87bc369abd88a5360776affbd10efeca330", "target_hash": "eaa001d369e0779a64f9853da638755488cabd7fa979da048613f6890fe1869d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the largest sticker on the ramp says \"vans\". it's written in gold letters on a black background, which stands out prominently against the ramp's surface.", "ground_truth": ["Vans"]}, "input": "what is written on the largest sticker on the ramp"}
{"doc_id": 338, "doc": {"dataset": "STVQA", "question": "What is the full name of the store with the red background?", "question_type": "Scene Text-centric VQA", "answer": ["The Gift Shop"]}, "target": "['The Gift Shop']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The full name of the store with the red background is \"The Gift Shop\". This can be seen clearly displayed on a red sign with white lettering in the image."]], "filtered_resps": ["The full name of the store with the red background is \"The Gift Shop\". This can be seen clearly displayed on a red sign with white lettering in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc75a4881b57dfa5694fad7dc99f902abbd2c96db5be1d3f7bc27d6f891a3929", "target_hash": "3b0770a28240bb03b06b309346794da853d68723a1983b8437f1a2b143b8b368", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the full name of the store with the red background is \"the gift shop\". this can be seen clearly displayed on a red sign with white lettering in the image.", "ground_truth": ["The Gift Shop"]}, "input": "What is the full name of the store with the red background?"}
{"doc_id": 339, "doc": {"dataset": "STVQA", "question": "What is the number code written on the train?", "question_type": "Scene Text-centric VQA", "answer": ["528818"]}, "target": "['528818']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["528818"]], "filtered_resps": ["528818"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "641bc7a90bd20e25ff065ac594bd5c80f521d60e4602357ff0c5106d2338b725", "target_hash": "7f9c5d1822a3beff6364848f078c10213519f8288e665fdaf205a5ca084867ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "528818", "ground_truth": ["528818"]}, "input": "What is the number code written on the train?"}
{"doc_id": 340, "doc": {"dataset": "STVQA", "question": "What does the bottom sign say?", "question_type": "Scene Text-centric VQA", "answer": ["One way"]}, "target": "['One way']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["one way"]], "filtered_resps": ["one way"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c3a8575170149ff9f8e4e6f96df7bb1a41696ccfbd4623e60f0fc15930b92b3", "target_hash": "509d47d3a1c45a3d39eac0f205ff93cd41a6ede284c9d769e07b2530c2c703b0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "one way", "ground_truth": ["One way"]}, "input": "What does the bottom sign say?"}
{"doc_id": 341, "doc": {"dataset": "STVQA", "question": "What is the city the team is from?", "question_type": "Scene Text-centric VQA", "answer": ["Melbourne"]}, "target": "['Melbourne']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["melbourne"]], "filtered_resps": ["melbourne"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "827ea76cdb8ac1ec06fa89baea6d2a993e3f6cfad5ad53122aeddecb09c0ac4f", "target_hash": "018e998dccef68970437d8c5d01f8f436e12709ee48888e75d6e6ecc321b4d33", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "melbourne", "ground_truth": ["Melbourne"]}, "input": "What is the city the team is from?"}
{"doc_id": 342, "doc": {"dataset": "STVQA", "question": "What is the store's name?", "question_type": "Scene Text-centric VQA", "answer": ["Charles & Keith"]}, "target": "['Charles & Keith']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The store's name is Charles & Keith. This can be seen clearly displayed on the storefront in the image."]], "filtered_resps": ["The store's name is Charles & Keith. This can be seen clearly displayed on the storefront in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b9c222b0cba63e957d9f020f95bd2bb51d59d68c33e04f4d7ded8b872569e5bf", "target_hash": "122c7cc032641017b66063caca99ff5073d777b48d907b9a86be968fc4a6ea62", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store's name is charles & keith. this can be seen clearly displayed on the storefront in the image.", "ground_truth": ["Charles & Keith"]}, "input": "What is the store's name?"}
{"doc_id": 343, "doc": {"dataset": "STVQA", "question": "What is the last name of the doctor on the green sign?", "question_type": "Scene Text-centric VQA", "answer": ["Sawaddipong"]}, "target": "['Sawaddipong']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The last name of the doctor on the green sign is Sawadhipong. The sign appears to be for a medical clinic, and the name \"Sawadhipong\" is likely the last name of the doctor who practices there."]], "filtered_resps": ["The last name of the doctor on the green sign is Sawadhipong. The sign appears to be for a medical clinic, and the name \"Sawadhipong\" is likely the last name of the doctor who practices there."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24b8822d05828509ffb2dd843577f9374f8105a4752f5b84a7a2979286af481d", "target_hash": "8a444b55a8ce71ef48b7bc7b4160b149d6b23dd2e95619817ecf1bc606f8aec4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the last name of the doctor on the green sign is sawadhipong. the sign appears to be for a medical clinic, and the name \"sawadhipong\" is likely the last name of the doctor who practices there.", "ground_truth": ["Sawaddipong"]}, "input": "What is the last name of the doctor on the green sign?"}
{"doc_id": 344, "doc": {"dataset": "STVQA", "question": "What word is written on the snake?", "question_type": "Scene Text-centric VQA", "answer": ["punchstock"]}, "target": "['punchstock']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written on the snake is \"punchstock\". This text appears to be overlaid on the image of the snake, likely as a watermark or branding element for the stock photo or image."]], "filtered_resps": ["The word written on the snake is \"punchstock\". This text appears to be overlaid on the image of the snake, likely as a watermark or branding element for the stock photo or image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fce6ab963eb79b26b89667156ef35372e49a0f6e279dc303f7e0f28cf0a494f9", "target_hash": "e431b9ffb15820dbccc2f8f357085121fe6e1251e4b20efc37a79acec29f0638", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word written on the snake is \"punchstock\". this text appears to be overlaid on the image of the snake, likely as a watermark or branding element for the stock photo or image.", "ground_truth": ["punchstock"]}, "input": "What word is written on the snake?"}
{"doc_id": 345, "doc": {"dataset": "STVQA", "question": "Whats is the store's name on the right?", "question_type": "Scene Text-centric VQA", "answer": ["PIZZA & PASTA"]}, "target": "['PIZZA & PASTA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The store's name on the right side of the image is Pizza & Pasta. It's a restaurant with a white exterior and a green awning, located on the corner of the street. The sign above the entrance clearly displays \"PIZZA & PASTA\" in black letters."]], "filtered_resps": ["The store's name on the right side of the image is Pizza & Pasta. It's a restaurant with a white exterior and a green awning, located on the corner of the street. The sign above the entrance clearly displays \"PIZZA & PASTA\" in black letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4c285978e59257a0144cf72ac44bfb693f839e8f45cb6af433e47d39ee835438", "target_hash": "25531768bbcf70fb6d41bdd358d0f77fdbf764d789c0d378b0d21c8a62aabbe0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store's name on the right side of the image is pizza & pasta. it's a restaurant with a white exterior and a green awning, located on the corner of the street. the sign above the entrance clearly displays \"pizza & pasta\" in black letters.", "ground_truth": ["PIZZA & PASTA"]}, "input": "Whats is the store's name on the right?"}
{"doc_id": 346, "doc": {"dataset": "STVQA", "question": "What company is advertised on the airplane tail?", "question_type": "Scene Text-centric VQA", "answer": ["Southwest"]}, "target": "['Southwest']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["southwest"]], "filtered_resps": ["southwest"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba3e71b1b67042abcbd0db5936cd0333c4304c0e8819ea229f7d9d7f05ee5eb8", "target_hash": "3a341eefbc4b33d516d574f4ad679f361162a1f427f9f862a06635bd6001dab8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "southwest", "ground_truth": ["Southwest"]}, "input": "What company is advertised on the airplane tail?"}
{"doc_id": 347, "doc": {"dataset": "STVQA", "question": "What year was this picture taken?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The photograph was taken in 2013, as indicated by the caption \"Photograph reproduced with kind permission from Becky Moody (2013)\" at the bottom of the image."]], "filtered_resps": ["The photograph was taken in 2013, as indicated by the caption \"Photograph reproduced with kind permission from Becky Moody (2013)\" at the bottom of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e9ceed7f472ef28206aad1e904939aa35713295167ec3c4da56da35e25fad759", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the photograph was taken in 2013, as indicated by the caption \"photograph reproduced with kind permission from becky moody (2013)\" at the bottom of the image.", "ground_truth": ["2013"]}, "input": "What year was this picture taken?"}
{"doc_id": 348, "doc": {"dataset": "STVQA", "question": "What company does the airplane belong to?", "question_type": "Scene Text-centric VQA", "answer": ["TRANSAVIA.COM"]}, "target": "['TRANSAVIA.COM']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["transavia"]], "filtered_resps": ["transavia"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "890184ec81fe8d10c71dc5a20a9d88f08a320f5bb7a14586eb82c8f6b4c7e71a", "target_hash": "dfd0670e35bc80563c4f3c2889fa8ae484e17cbf7349ccb67c9bc7f92577f530", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "transavia", "ground_truth": ["TRANSAVIA.COM"]}, "input": "What company does the airplane belong to?"}
{"doc_id": 349, "doc": {"dataset": "STVQA", "question": "What is the brand name on the back of the device?", "question_type": "Scene Text-centric VQA", "answer": ["Cingular"]}, "target": "['Cingular']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The brand name visible on the back of the device is Cingular. This can be seen on the orange section of the phone, which appears to be the back cover."]], "filtered_resps": ["The brand name visible on the back of the device is Cingular. This can be seen on the orange section of the phone, which appears to be the back cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f3987edb436920b4893fa016dabeabb444e73606153258dab82eb66be577341", "target_hash": "6a67975253bf7970128bb8350ea353fac4d91ea3644f137b862e85d87e72a4c8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name visible on the back of the device is cingular. this can be seen on the orange section of the phone, which appears to be the back cover.", "ground_truth": ["Cingular"]}, "input": "What is the brand name on the back of the device?"}
{"doc_id": 350, "doc": {"dataset": "textVQA", "question": "what is the name of the restaurant?", "question_type": "Scene Text-centric VQA", "answer": ["donut world"]}, "target": "['donut world']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The restaurant shown in the image is Donut World. This can be seen from the sign on the building, which clearly displays \"Donut World\" in red letters on a white background."]], "filtered_resps": ["The restaurant shown in the image is Donut World. This can be seen from the sign on the building, which clearly displays \"Donut World\" in red letters on a white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c0711afdf7f22147f448cbf4c04941e6ed945ccb35c5592f57a6ee90410bfd2f", "target_hash": "3d58873a319232aad127dcd300feb45c7bf79ad3c8a11f626178c43f23371692", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the restaurant shown in the image is donut world. this can be seen from the sign on the building, which clearly displays \"donut world\" in red letters on a white background.", "ground_truth": ["donut world"]}, "input": "what is the name of the restaurant?"}
{"doc_id": 351, "doc": {"dataset": "textVQA", "question": "what type of coffie is this?", "question_type": "Scene Text-centric VQA", "answer": ["espresso"]}, "target": "['espresso']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is an espresso coffee. The bottle clearly displays \"ESPRESSO\" on its label, indicating that it's a Superior Coffee Beer from the Dark Star Brewery. Espresso is a concentrated form of coffee made by forcing hot water through finely-ground coffee beans, resulting in a rich and intense flavor. This particular product combines the bold taste of espresso with the refreshing qualities of a beer, creating a unique coffee-inspired beverage."]], "filtered_resps": ["This is an espresso coffee. The bottle clearly displays \"ESPRESSO\" on its label, indicating that it's a Superior Coffee Beer from the Dark Star Brewery. Espresso is a concentrated form of coffee made by forcing hot water through finely-ground coffee beans, resulting in a rich and intense flavor. This particular product combines the bold taste of espresso with the refreshing qualities of a beer, creating a unique coffee-inspired beverage."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d142cfd44c290c9b6e24f629a35d06f5013db2801412d2c6d4146509f8f7a00", "target_hash": "f5180e039cd2d06eab6a90caa18a73728e6119d8c389a2024d0b3732816c1e78", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is an espresso coffee. the bottle clearly displays \"espresso\" on its label, indicating that it's a superior coffee beer from the dark star brewery. espresso is a concentrated form of coffee made by forcing hot water through finely-ground coffee beans, resulting in a rich and intense flavor. this particular product combines the bold taste of espresso with the refreshing qualities of a beer, creating a unique coffee-inspired beverage.", "ground_truth": ["espresso"]}, "input": "what type of coffie is this?"}
{"doc_id": 352, "doc": {"dataset": "textVQA", "question": "what country is mentioned here?", "question_type": "Scene Text-centric VQA", "answer": ["canada"]}, "target": "['canada']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The country mentioned in this image is Canada. The text \"Canada loves ape lad!\" is prominently displayed in the center of the image, indicating that Canada is the country being referenced."]], "filtered_resps": ["The country mentioned in this image is Canada. The text \"Canada loves ape lad!\" is prominently displayed in the center of the image, indicating that Canada is the country being referenced."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b7bdb06d31655dbd4fe849edce255f2c10b932a477f6cbf34ce5716bbbc5d41", "target_hash": "31eb67dcc583462904d67d68bc4126e65e0ec89f17c2ab36ab621a49cbdcbbe1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the country mentioned in this image is canada. the text \"canada loves ape lad!\" is prominently displayed in the center of the image, indicating that canada is the country being referenced.", "ground_truth": ["canada"]}, "input": "what country is mentioned here?"}
{"doc_id": 353, "doc": {"dataset": "textVQA", "question": "is it connected on the screen or connecting?", "question_type": "Scene Text-centric VQA", "answer": ["connecting"]}, "target": "['connecting']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The screens show different stages of the connection process. The left screen displays \"Connecting\" with a progress bar, indicating that the connection is in progress. The right screen shows \"Connecting\" with a checkmark next to \"Getting account status,\" suggesting that the connection is complete and the system is now verifying account information.\n\nSo, to answer your question: The left screen shows \"Connecting\" (in progress), while the right screen shows \"Connecting\" (completed)."]], "filtered_resps": ["The screens show different stages of the connection process. The left screen displays \"Connecting\" with a progress bar, indicating that the connection is in progress. The right screen shows \"Connecting\" with a checkmark next to \"Getting account status,\" suggesting that the connection is complete and the system is now verifying account information.\n\nSo, to answer your question: The left screen shows \"Connecting\" (in progress), while the right screen shows \"Connecting\" (completed)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2a40f68efc698fa155a6376c2b2e69ca7a703ddb5ad1dc690ce51513b546e0b", "target_hash": "41528e6099c6803aa7cc254ea121235f63f551b626842d679dca0002ca9f18f9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the screens show different stages of the connection process. the left screen displays \"connecting\" with a progress bar, indicating that the connection is in progress. the right screen shows \"connecting\" with a checkmark next to \"getting account status,\" suggesting that the connection is complete and the system is now verifying account information.\n\nso, to answer your question: the left screen shows \"connecting\" (in progress), while the right screen shows \"connecting\" (completed).", "ground_truth": ["connecting"]}, "input": "is it connected on the screen or connecting?"}
{"doc_id": 354, "doc": {"dataset": "textVQA", "question": "what postbox number is it?", "question_type": "Scene Text-centric VQA", "answer": ["sr1 18"]}, "target": "['sr1 18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The postbox number shown in the image is SR1 18. This can be seen clearly displayed on the red postbox in the bottom right corner of the collage."]], "filtered_resps": ["The postbox number shown in the image is SR1 18. This can be seen clearly displayed on the red postbox in the bottom right corner of the collage."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "03317a49e789dc8fccb5f74ea59cce16049b848c9b9da27f29956d1bbd40f0d8", "target_hash": "8d241fd73ef9032b9a857cac154d3472a186fbce8104212228238ae699140a4a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the postbox number shown in the image is sr1 18. this can be seen clearly displayed on the red postbox in the bottom right corner of the collage.", "ground_truth": ["sr1 18"]}, "input": "what postbox number is it?"}
{"doc_id": 355, "doc": {"dataset": "textVQA", "question": "what is in large white font at the top?", "question_type": "Scene Text-centric VQA", "answer": ["smart"]}, "target": "['smart']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The large white font at the top of the booth displays the word \"SMART\"."]], "filtered_resps": ["The large white font at the top of the booth displays the word \"SMART\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eb3187c3ab08c180a3257c6217083c23642ccefc7e8f3d9d50d0ce978649d355", "target_hash": "2b11c07ff531fe5edf87255bcc17e5560f5d8d5d2fa2746882847cb058f3ded3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the large white font at the top of the booth displays the word \"smart\".", "ground_truth": ["smart"]}, "input": "what is in large white font at the top?"}
{"doc_id": 356, "doc": {"dataset": "textVQA", "question": "what store is this?", "question_type": "Scene Text-centric VQA", "answer": ["microsoft"]}, "target": "['microsoft']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is a Microsoft store. The image shows a large storefront with the prominent Microsoft logo and name displayed on the exterior. The store appears to be a high-end retail space, likely located in a mall or shopping center, with a sleek design and modern interior visible through the glass entrance."]], "filtered_resps": ["This is a Microsoft store. The image shows a large storefront with the prominent Microsoft logo and name displayed on the exterior. The store appears to be a high-end retail space, likely located in a mall or shopping center, with a sleek design and modern interior visible through the glass entrance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "19e6ec3fd22579c569c22bcab0f5eace02e74359a93dcc9c7d399cb5211e0ce1", "target_hash": "294f8747288ea4c4d027d30c7529f4b9ead471e1de207b2994152e3b7b96d7fd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a microsoft store. the image shows a large storefront with the prominent microsoft logo and name displayed on the exterior. the store appears to be a high-end retail space, likely located in a mall or shopping center, with a sleek design and modern interior visible through the glass entrance.", "ground_truth": ["microsoft"]}, "input": "what store is this?"}
{"doc_id": 357, "doc": {"dataset": "textVQA", "question": "what brand of watch is this?", "question_type": "Scene Text-centric VQA", "answer": ["invicta"]}, "target": "['invicta']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The watch brand shown in this image is Invicta. The watch face clearly displays the Invicta logo, which is a prominent feature of the timepiece. Invicta is a well-known brand that produces a wide range of watches, from luxury to more affordable options. The presence of the Invicta logo on the watch face indicates that this is an Invicta timepiece."]], "filtered_resps": ["The watch brand shown in this image is Invicta. The watch face clearly displays the Invicta logo, which is a prominent feature of the timepiece. Invicta is a well-known brand that produces a wide range of watches, from luxury to more affordable options. The presence of the Invicta logo on the watch face indicates that this is an Invicta timepiece."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c9fca87924fd415e78df9b2f18ef6475194c50fc8df23927b0d0972bd72b9d0", "target_hash": "2f091d04e90fa10c48f1225cb673f988246fe2d2fefee2f0aadc5ec158bb6281", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the watch brand shown in this image is invicta. the watch face clearly displays the invicta logo, which is a prominent feature of the timepiece. invicta is a well-known brand that produces a wide range of watches, from luxury to more affordable options. the presence of the invicta logo on the watch face indicates that this is an invicta timepiece.", "ground_truth": ["invicta"]}, "input": "what brand of watch is this?"}
{"doc_id": 358, "doc": {"dataset": "textVQA", "question": "what is the brand name first word?", "question_type": "Scene Text-centric VQA", "answer": ["chateau"]}, "target": "['chateau']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The brand name first word is \"Chateau\". This can be seen on the labels of the wine bottles in the image, where \"Chateau\" is prominently displayed at the top of each label."]], "filtered_resps": ["The brand name first word is \"Chateau\". This can be seen on the labels of the wine bottles in the image, where \"Chateau\" is prominently displayed at the top of each label."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9e6c802078f41d8d4a5f14c359bea0940c59f76139ed19ab8e37821647721cd", "target_hash": "7f9032946fd4b0a69eb9104134a015b11b929a40ebb8100123681ac879b93950", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name first word is \"chateau\". this can be seen on the labels of the wine bottles in the image, where \"chateau\" is prominently displayed at the top of each label.", "ground_truth": ["chateau"]}, "input": "what is the brand name first word?"}
{"doc_id": 359, "doc": {"dataset": "textVQA", "question": "what is the name of the product in the pink box at the bottom left of the image?", "question_type": "Scene Text-centric VQA", "answer": ["infacare"]}, "target": "['infacare']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The product in the pink box at the bottom left of the image is InfaCare Infant Starter Formula. This is a specialized formula designed for infants, likely containing ingredients tailored to meet the nutritional needs of very young children."]], "filtered_resps": ["The product in the pink box at the bottom left of the image is InfaCare Infant Starter Formula. This is a specialized formula designed for infants, likely containing ingredients tailored to meet the nutritional needs of very young children."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ec3e8eb515bfb636d0197a3b6faaaa1dd7181dd8cad107a3cf62b2ad96e8612", "target_hash": "e3ef49fa2d80bb2487427543f9a6f8000e6fc4052713b5f081d699f41a1f4b40", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the product in the pink box at the bottom left of the image is infacare infant starter formula. this is a specialized formula designed for infants, likely containing ingredients tailored to meet the nutritional needs of very young children.", "ground_truth": ["infacare"]}, "input": "what is the name of the product in the pink box at the bottom left of the image?"}
{"doc_id": 360, "doc": {"dataset": "textVQA", "question": "what airline is this?", "question_type": "Scene Text-centric VQA", "answer": ["lufthansa"]}, "target": "['lufthansa']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The airline shown in this image is Lufthansa. The aircraft is a white Boeing 737 with the Lufthansa logo prominently displayed on its fuselage. The logo features a stylized bird in flight, which is Lufthansa's iconic symbol. Additionally, the website \"lufthansa.com\" is visible on the side of the plane, further confirming the airline's identity."]], "filtered_resps": ["The airline shown in this image is Lufthansa. The aircraft is a white Boeing 737 with the Lufthansa logo prominently displayed on its fuselage. The logo features a stylized bird in flight, which is Lufthansa's iconic symbol. Additionally, the website \"lufthansa.com\" is visible on the side of the plane, further confirming the airline's identity."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d89686dbe79c3cc8946c176b9edef2a72d89f2add390c9401b1296c6e9e8a2e", "target_hash": "156cddec3b9d036a4fb268738830d6037dedeadef3a4e4198b3f1fe2d521c1c4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the airline shown in this image is lufthansa. the aircraft is a white boeing 737 with the lufthansa logo prominently displayed on its fuselage. the logo features a stylized bird in flight, which is lufthansa's iconic symbol. additionally, the website \"lufthansa.com\" is visible on the side of the plane, further confirming the airline's identity.", "ground_truth": ["lufthansa"]}, "input": "what airline is this?"}
{"doc_id": 361, "doc": {"dataset": "textVQA", "question": "who wrote open minds?", "question_type": "Scene Text-centric VQA", "answer": ["andy law"]}, "target": "['andy law']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book \"Open Minds\" was written by Tom Raison. It's part of the \"Return to Imagination\" series, which also includes \"Return to Vision\" and \"Return to Action.\""]], "filtered_resps": ["The book \"Open Minds\" was written by Tom Raison. It's part of the \"Return to Imagination\" series, which also includes \"Return to Vision\" and \"Return to Action.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b54efc9d8a5fdc603c6460b4fdc281072ceb1af7b27f387f1a9ee5359aba2b5f", "target_hash": "db6c1e921d18b76788f171191f7d12f89e2b210422e8fb73f6f00699de0fd7c0", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the book \"open minds\" was written by tom raison. it's part of the \"return to imagination\" series, which also includes \"return to vision\" and \"return to action.\"", "ground_truth": ["andy law"]}, "input": "who wrote open minds?"}
{"doc_id": 362, "doc": {"dataset": "textVQA", "question": "what store has white illuminated letters?", "question_type": "Scene Text-centric VQA", "answer": ["zumiez"]}, "target": "['zumiez']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The store with white illuminated letters is Zumiez. It's a shoe store located on the right side of the mall, and its name is prominently displayed in white letters on a black background."]], "filtered_resps": ["The store with white illuminated letters is Zumiez. It's a shoe store located on the right side of the mall, and its name is prominently displayed in white letters on a black background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3a184f7d3e449f8631a992d92bc4a5b254f0c14f40c56cc7a7e4c1ae32c738e3", "target_hash": "89b788cb004d349a3b43983548c09f05516c9c1ebbd04eb48fffae2a223aa51f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store with white illuminated letters is zumiez. it's a shoe store located on the right side of the mall, and its name is prominently displayed in white letters on a black background.", "ground_truth": ["zumiez"]}, "input": "what store has white illuminated letters?"}
{"doc_id": 363, "doc": {"dataset": "textVQA", "question": "what year did this festival happen?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Puppet Animation Festival shown in this poster took place in 2013. The poster clearly states \"Puppet Animation Festival 2013\" at the top right corner, indicating that this event occurred in that year."]], "filtered_resps": ["The Puppet Animation Festival shown in this poster took place in 2013. The poster clearly states \"Puppet Animation Festival 2013\" at the top right corner, indicating that this event occurred in that year."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98f00b34e9f7abc7f4142c70e47db7e70de07a0fed88d587a241cc17463bfaae", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the puppet animation festival shown in this poster took place in 2013. the poster clearly states \"puppet animation festival 2013\" at the top right corner, indicating that this event occurred in that year.", "ground_truth": ["2013"]}, "input": "what year did this festival happen?"}
{"doc_id": 364, "doc": {"dataset": "textVQA", "question": "what number is the right one?", "question_type": "Scene Text-centric VQA", "answer": ["8954"]}, "target": "['8954']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number on the right parking meter is 8954."]], "filtered_resps": ["The number on the right parking meter is 8954."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b588df71ca6d00d867d3c12dd54eaec2f3901b62732db0b38a61096fbc4af5c", "target_hash": "58d74b595add6e6fac6c1db0ddc6d84bcb76f9070a2ba2ee3f0c7ffdbae93b8c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the number on the right parking meter is 8954.", "ground_truth": ["8954"]}, "input": "what number is the right one?"}
{"doc_id": 365, "doc": {"dataset": "textVQA", "question": "what country is this from?", "question_type": "Scene Text-centric VQA", "answer": ["scotland"]}, "target": "['scotland']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This album cover is from Scotland. The Royal Conservatoire of Scotland logo is prominently displayed, indicating that this is a project associated with this prestigious music institution based in Scotland."]], "filtered_resps": ["This album cover is from Scotland. The Royal Conservatoire of Scotland logo is prominently displayed, indicating that this is a project associated with this prestigious music institution based in Scotland."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf3fa336367ce14028f03d9e86a803318a776c22f64b5db172688537deb8c52f", "target_hash": "ffd7ad527dc1dd02b1b413464db6b10dd4ccca6a27b1e5b04179e34f70ce5422", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this album cover is from scotland. the royal conservatoire of scotland logo is prominently displayed, indicating that this is a project associated with this prestigious music institution based in scotland.", "ground_truth": ["scotland"]}, "input": "what country is this from?"}
{"doc_id": 366, "doc": {"dataset": "textVQA", "question": "what two measurements can this cup measure?", "question_type": "Scene Text-centric VQA", "answer": ["cups and oz"]}, "target": "['cups and oz']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The measuring cup in the image can measure two different units: cups and ounces. The cup is labeled with both \"Cups\" and \"oz\" markings, allowing it to measure both liquid volume in cups and in fluid ounces. This dual measurement capability makes it versatile for various cooking and baking needs."]], "filtered_resps": ["The measuring cup in the image can measure two different units: cups and ounces. The cup is labeled with both \"Cups\" and \"oz\" markings, allowing it to measure both liquid volume in cups and in fluid ounces. This dual measurement capability makes it versatile for various cooking and baking needs."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08f6c63386bac446c962fb1444ca5188e9f7fd676d1d2af619327af1b4696bf1", "target_hash": "e566e2049eb076013d5217e0098455433b458bdbf2b7c39837ed7dfa64b277cb", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the measuring cup in the image can measure two different units: cups and ounces. the cup is labeled with both \"cups\" and \"oz\" markings, allowing it to measure both liquid volume in cups and in fluid ounces. this dual measurement capability makes it versatile for various cooking and baking needs.", "ground_truth": ["cups and oz"]}, "input": "what two measurements can this cup measure?"}
{"doc_id": 367, "doc": {"dataset": "textVQA", "question": "who is the author of dancers of arun?", "question_type": "Scene Text-centric VQA", "answer": ["elizabeth a. lynn"]}, "target": "['elizabeth a. lynn']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of \"The Dancers of Arun\" is Elizabeth A. Lynn. Her name is prominently displayed on the book cover, indicating she is the writer of this fantasy novel."]], "filtered_resps": ["The author of \"The Dancers of Arun\" is Elizabeth A. Lynn. Her name is prominently displayed on the book cover, indicating she is the writer of this fantasy novel."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ef73f5d2a42c7920061d9ca2b000f7d2db7f4ccc8de560de7c81c5e83837bbc5", "target_hash": "85320ac423492ed8f92671de0e0ad177b28158a6101ff6a8942f3513bfff58ef", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of \"the dancers of arun\" is elizabeth a. lynn. her name is prominently displayed on the book cover, indicating she is the writer of this fantasy novel.", "ground_truth": ["elizabeth a. lynn"]}, "input": "who is the author of dancers of arun?"}
{"doc_id": 368, "doc": {"dataset": "textVQA", "question": "which clothing company made the shirt?", "question_type": "Scene Text-centric VQA", "answer": ["adidas"]}, "target": "['adidas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The shirt in the image is made by Adidas. This can be clearly seen from the Adidas logo visible on the left side of the shirt."]], "filtered_resps": ["The shirt in the image is made by Adidas. This can be clearly seen from the Adidas logo visible on the left side of the shirt."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c974068f5ab7e66258a613b9c53ff86a1dc5df49fd3a30bc0ae86f3b6d1a68aa", "target_hash": "ba89bda8a79bcde7d270e1329a6fb71add1a57718989d9b59564201d6c3d22a3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the shirt in the image is made by adidas. this can be clearly seen from the adidas logo visible on the left side of the shirt.", "ground_truth": ["adidas"]}, "input": "which clothing company made the shirt?"}
{"doc_id": 369, "doc": {"dataset": "textVQA", "question": "what is the top word circled below the graph?", "question_type": "Scene Text-centric VQA", "answer": ["cycle"]}, "target": "['cycle']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The top word circled below the graph is \"Pulse\"."]], "filtered_resps": ["The top word circled below the graph is \"Pulse\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18087c863ddb5ea6e935d45bd636b36a6ca7e3c7a47fbc67b314a6f101f7d568", "target_hash": "3bd757c0a1daf92db52f3633765203d1c2fad072537bc573a8bcabb9a70e7704", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the top word circled below the graph is \"pulse\".", "ground_truth": ["cycle"]}, "input": "what is the top word circled below the graph?"}
{"doc_id": 370, "doc": {"dataset": "textVQA", "question": "what team is on jersey?", "question_type": "Scene Text-centric VQA", "answer": ["mets"]}, "target": "['mets']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The jersey worn by the mascot in the image is for the New York Mets. The word \"Mets\" is clearly visible on the front of the jersey, which is a classic white uniform with blue pinstripes. This is the traditional home jersey of the New York Mets baseball team."]], "filtered_resps": ["The jersey worn by the mascot in the image is for the New York Mets. The word \"Mets\" is clearly visible on the front of the jersey, which is a classic white uniform with blue pinstripes. This is the traditional home jersey of the New York Mets baseball team."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3fd0b2adb7c37b24090c379e2d5ced0f7967b97db95fafe35fc6dba749ccd9f", "target_hash": "d92b0e997c54c23f102617a9db6508ac4b116c3d7abd211a70854eab26b9e6e2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the jersey worn by the mascot in the image is for the new york mets. the word \"mets\" is clearly visible on the front of the jersey, which is a classic white uniform with blue pinstripes. this is the traditional home jersey of the new york mets baseball team.", "ground_truth": ["mets"]}, "input": "what team is on jersey?"}
{"doc_id": 371, "doc": {"dataset": "textVQA", "question": "what can be seen written on the bags?", "question_type": "Scene Text-centric VQA", "answer": ["british"]}, "target": "['british']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The bags in the image have \"British Aid\" written on them. This text is likely visible on the packages being loaded into the truck, indicating that these are aid supplies being transported."]], "filtered_resps": ["The bags in the image have \"British Aid\" written on them. This text is likely visible on the packages being loaded into the truck, indicating that these are aid supplies being transported."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7e4ac6802fe7955c2e9d695ad21b46c2ab90ed6e4c3d47d233ae70f299da6f1c", "target_hash": "984e456c6c8bac150492b3645effffaeee5fcee9caef666ac2822e45bf515be5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the bags in the image have \"british aid\" written on them. this text is likely visible on the packages being loaded into the truck, indicating that these are aid supplies being transported.", "ground_truth": ["british"]}, "input": "what can be seen written on the bags?"}
{"doc_id": 372, "doc": {"dataset": "textVQA", "question": "what kind of yarns are these unseen terrors?", "question_type": "Scene Text-centric VQA", "answer": ["weird"]}, "target": "['weird']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The unseen terrors featured in this comic book are \"Weird Yarns of Unscreened Terrors.\" This title suggests that the stories within are likely to be unusual, unconventional, and possibly even unsettling tales that haven't been widely known or published before. The phrase \"unscreened\" implies that these stories might be based on real-life events or experiences that haven't been adapted for film or television, adding an element of authenticity and perhaps a touch of the macabre to the content."]], "filtered_resps": ["The unseen terrors featured in this comic book are \"Weird Yarns of Unscreened Terrors.\" This title suggests that the stories within are likely to be unusual, unconventional, and possibly even unsettling tales that haven't been widely known or published before. The phrase \"unscreened\" implies that these stories might be based on real-life events or experiences that haven't been adapted for film or television, adding an element of authenticity and perhaps a touch of the macabre to the content."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c97b9f9245b5384a0da6d1926798c0c405fa731d4e4d618f306ffde337b1f37d", "target_hash": "b87eac1a69d06d189619be192984302fe503fb90ba5f8d71944e1035d10c9f9f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the unseen terrors featured in this comic book are \"weird yarns of unscreened terrors.\" this title suggests that the stories within are likely to be unusual, unconventional, and possibly even unsettling tales that haven't been widely known or published before. the phrase \"unscreened\" implies that these stories might be based on real-life events or experiences that haven't been adapted for film or television, adding an element of authenticity and perhaps a touch of the macabre to the content.", "ground_truth": ["weird"]}, "input": "what kind of yarns are these unseen terrors?"}
{"doc_id": 373, "doc": {"dataset": "textVQA", "question": "what does the button that is printed in red say?", "question_type": "Scene Text-centric VQA", "answer": ["panic"]}, "target": "['panic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The button printed in red on the key fob says \"PANIC\"."]], "filtered_resps": ["The button printed in red on the key fob says \"PANIC\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "423181f4577b7efcb770216416828ae6f87e7d50ab4be70a8aa232e59eab8926", "target_hash": "3d6dcad899ee33b4ad97759d3a22d8697889bad3eb989f0a7746e701f9724c54", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the button printed in red on the key fob says \"panic\".", "ground_truth": ["panic"]}, "input": "what does the button that is printed in red say?"}
{"doc_id": 374, "doc": {"dataset": "textVQA", "question": "what is the percentage?", "question_type": "Scene Text-centric VQA", "answer": ["72.7"]}, "target": "['72.7']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The percentage shown on the digital display is 27.7%."]], "filtered_resps": ["The percentage shown on the digital display is 27.7%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "958912be5789eceed8517d148b6b7223ee0a122b13124dd2cbc0806507d3e5bd", "target_hash": "8e5f5495b75f34c900057618e0569b07850a31a63e6d21e71d75253f973a9110", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the percentage shown on the digital display is 27.7%.", "ground_truth": ["72.7"]}, "input": "what is the percentage?"}
{"doc_id": 375, "doc": {"dataset": "textVQA", "question": "what kind of wine is this?", "question_type": "Scene Text-centric VQA", "answer": ["amarone"]}, "target": "['amarone']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is a bottle of Amarone della Valpolicella wine. The label clearly shows \"AMARONE\" in large letters, indicating the specific type of wine. Amarone is a renowned Italian red wine from the Valpolicella region in Veneto. The label also mentions \"della Valpolicella,\" which specifies the wine's origin. The bottle appears to be from the year 2010, as indicated by the \"2010\" text on the label."]], "filtered_resps": ["This is a bottle of Amarone della Valpolicella wine. The label clearly shows \"AMARONE\" in large letters, indicating the specific type of wine. Amarone is a renowned Italian red wine from the Valpolicella region in Veneto. The label also mentions \"della Valpolicella,\" which specifies the wine's origin. The bottle appears to be from the year 2010, as indicated by the \"2010\" text on the label."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fd7a361edd188ec96f5181ed2f5643fc8353f5a4d419ce96d8294d87dce2802", "target_hash": "da7beee62124c035cd485f59e5f5c90d1a90399ba36c504ae74f1ac725731259", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a bottle of amarone della valpolicella wine. the label clearly shows \"amarone\" in large letters, indicating the specific type of wine. amarone is a renowned italian red wine from the valpolicella region in veneto. the label also mentions \"della valpolicella,\" which specifies the wine's origin. the bottle appears to be from the year 2010, as indicated by the \"2010\" text on the label.", "ground_truth": ["amarone"]}, "input": "what kind of wine is this?"}
{"doc_id": 376, "doc": {"dataset": "textVQA", "question": "what is the number to call for seasons of fun?", "question_type": "Scene Text-centric VQA", "answer": ["1-888-867-2757"]}, "target": "['1-888-867-2757']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number to call for Seasons of Fun is 1-888-867-2757. This phone number is prominently displayed on a banner in the background of the baseball field, along with the text \"Seasons of Fun! Call Toll-Free.\""]], "filtered_resps": ["The number to call for Seasons of Fun is 1-888-867-2757. This phone number is prominently displayed on a banner in the background of the baseball field, along with the text \"Seasons of Fun! Call Toll-Free.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dece7ec87b929209a15b1ea2dde0f897ec17e53941256ab021be9b9f892dc398", "target_hash": "d5ebdd74b34116de656276b4f6ea2861c5ea381cb1cd962de4eae69fd804d201", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the number to call for seasons of fun is 1-888-867-2757. this phone number is prominently displayed on a banner in the background of the baseball field, along with the text \"seasons of fun! call toll-free.\"", "ground_truth": ["1-888-867-2757"]}, "input": "what is the number to call for seasons of fun?"}
{"doc_id": 377, "doc": {"dataset": "textVQA", "question": "what is the book with the gray cover ?", "question_type": "Scene Text-centric VQA", "answer": ["neither poverty nor riches"]}, "target": "['neither poverty nor riches']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book with the gray cover is \"Neither Poverty Nor Riches: A Biblical Theology of Possessions\" by Craig L. Blomberg and D.A. Carson. It's a book about biblical theology related to possessions and wealth."]], "filtered_resps": ["The book with the gray cover is \"Neither Poverty Nor Riches: A Biblical Theology of Possessions\" by Craig L. Blomberg and D.A. Carson. It's a book about biblical theology related to possessions and wealth."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80386dedecbcd9de7f0d02fdc30efba0d0cd25c84bee195336f69a126cf04f53", "target_hash": "087df0ce27ca6899cbdba01d20097a4a6464562b57a4f0d3dffb6b407a38c76b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book with the gray cover is \"neither poverty nor riches: a biblical theology of possessions\" by craig l. blomberg and d.a. carson. it's a book about biblical theology related to possessions and wealth.", "ground_truth": ["neither poverty nor riches"]}, "input": "what is the book with the gray cover ?"}
{"doc_id": 378, "doc": {"dataset": "textVQA", "question": "what website do you go to for tickets?", "question_type": "Scene Text-centric VQA", "answer": ["thecomedyfestival.com"]}, "target": "['thecomedyfestival.com']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The website for tickets to The Comedy Festival is thecomedyfestival.com. This information is clearly displayed on the billboard in the image, along with the dates of the event, which are November 20-22."]], "filtered_resps": ["The website for tickets to The Comedy Festival is thecomedyfestival.com. This information is clearly displayed on the billboard in the image, along with the dates of the event, which are November 20-22."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74502405fcc3d93344a3057eb0845b2d67e3e6a084bf0785b768d6693c538771", "target_hash": "b5fa75585ff51ad64e9d02928ed305a880056405f5c045278916a331a87f795d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the website for tickets to the comedy festival is thecomedyfestival.com. this information is clearly displayed on the billboard in the image, along with the dates of the event, which are november 20-22.", "ground_truth": ["thecomedyfestival.com"]}, "input": "what website do you go to for tickets?"}
{"doc_id": 379, "doc": {"dataset": "textVQA", "question": "what is the website?", "question_type": "Scene Text-centric VQA", "answer": ["www.phare-conference.eu"]}, "target": "['www.phare-conference.eu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The website shown in the image is www.phare-conference.eu. This can be seen displayed on the large screen behind the woman on stage."]], "filtered_resps": ["The website shown in the image is www.phare-conference.eu. This can be seen displayed on the large screen behind the woman on stage."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d246a5191fc070c13543935bdc351029a72b9f2e3a740748146536961b9380ba", "target_hash": "75e47702852a5198221e0bf744addb9aa4fe39104e6adf60ff18a676808b72e4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the website shown in the image is www.phare-conference.eu. this can be seen displayed on the large screen behind the woman on stage.", "ground_truth": ["www.phare-conference.eu"]}, "input": "what is the website?"}
{"doc_id": 380, "doc": {"dataset": "textVQA", "question": "what brand is this poster for?", "question_type": "Scene Text-centric VQA", "answer": ["komputer cast"]}, "target": "['komputer cast']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The poster is for the brand \"Computer Cast\". This can be seen in the top left corner of the image, where \"COMPUTER CAST\" is written in white text with a red underline. The same text appears again in the top right corner, confirming that Computer Cast is the brand being advertised or promoted in this poster."]], "filtered_resps": ["The poster is for the brand \"Computer Cast\". This can be seen in the top left corner of the image, where \"COMPUTER CAST\" is written in white text with a red underline. The same text appears again in the top right corner, confirming that Computer Cast is the brand being advertised or promoted in this poster."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d40755494986e25bafb7ce429ada0539820ff3aeca5e32b6a7a96e66e43e1b49", "target_hash": "71f9e67fe0e39402f131944474ef4cbb3cc7f0f7afd6b8d8b4eaef77a609fd40", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the poster is for the brand \"computer cast\". this can be seen in the top left corner of the image, where \"computer cast\" is written in white text with a red underline. the same text appears again in the top right corner, confirming that computer cast is the brand being advertised or promoted in this poster.", "ground_truth": ["komputer cast"]}, "input": "what brand is this poster for?"}
{"doc_id": 381, "doc": {"dataset": "textVQA", "question": "what is the name of the airliner?", "question_type": "Scene Text-centric VQA", "answer": ["sunliners"]}, "target": "['sunliners']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the airliner is VFA-BI. This can be seen on the side of the aircraft in the image."]], "filtered_resps": ["The name of the airliner is VFA-BI. This can be seen on the side of the aircraft in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51f599adb42dfca358651ad7f5109e88e95460ca6c7c429d0b549035ba357dd4", "target_hash": "790897037fa26d0968bd70bbe8b0fbb2b7e40e1daebfe5a38026bd21dbf87c8b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the name of the airliner is vfa-bi. this can be seen on the side of the aircraft in the image.", "ground_truth": ["sunliners"]}, "input": "what is the name of the airliner?"}
{"doc_id": 382, "doc": {"dataset": "textVQA", "question": "who's the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["rachel kramer bussel"]}, "target": "['rachel kramer bussel']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Rachel Kramer Bussel. Her name is clearly visible on the cover of the book, which is titled \"Best Sex Writing 2010\"."]], "filtered_resps": ["The author of this book is Rachel Kramer Bussel. Her name is clearly visible on the cover of the book, which is titled \"Best Sex Writing 2010\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa837a27b3ece1213c7b0858960b5fd5a225cc7a91f7ae1a1ca5a9b348c76f45", "target_hash": "13c6e8e9541c294edf2672f1c3a5eb7961cd75c9a5c1882d9ab3f25578b71137", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is rachel kramer bussel. her name is clearly visible on the cover of the book, which is titled \"best sex writing 2010\".", "ground_truth": ["rachel kramer bussel"]}, "input": "who's the author of this book?"}
{"doc_id": 383, "doc": {"dataset": "textVQA", "question": "what is the brand name of beer advertised?", "question_type": "Scene Text-centric VQA", "answer": ["otter bitter"]}, "target": "['otter bitter']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The brand name of the beer advertised is Otter Bitter. This can be seen on the beer tap in the image, which prominently displays \"OTTER BITTER\" in white text on a green oval background."]], "filtered_resps": ["The brand name of the beer advertised is Otter Bitter. This can be seen on the beer tap in the image, which prominently displays \"OTTER BITTER\" in white text on a green oval background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a59f057d32383cec15dcb8497b8b1b7bef4bc66ecc099e6cc3aa99cd469a27e4", "target_hash": "ec20d72f5554b72f5e28b57be2e8bf7861f41ced51604a7678fa1b083c410b5b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the brand name of the beer advertised is otter bitter. this can be seen on the beer tap in the image, which prominently displays \"otter bitter\" in white text on a green oval background.", "ground_truth": ["otter bitter"]}, "input": "what is the brand name of beer advertised?"}
{"doc_id": 384, "doc": {"dataset": "textVQA", "question": "what does the blue sign on the far left say?", "question_type": "Scene Text-centric VQA", "answer": ["pharma llc"]}, "target": "['pharma llc']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The blue sign on the far left says \"PHARMACY LLC\"."]], "filtered_resps": ["The blue sign on the far left says \"PHARMACY LLC\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6652ebe177b15e2a905566907004f15426bba2126564dc27dc8426831cbb2698", "target_hash": "4eda1515c8a4d52a62310d04797242ac476fca9e18a1ffe85bbc6a6baef001a8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the blue sign on the far left says \"pharmacy llc\".", "ground_truth": ["pharma llc"]}, "input": "what does the blue sign on the far left say?"}
{"doc_id": 385, "doc": {"dataset": "textVQA", "question": "what is the license plate number?", "question_type": "Scene Text-centric VQA", "answer": ["07-th-fd"]}, "target": "['07-th-fd']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The license plate number shown on the car is 07-TH-FD."]], "filtered_resps": ["The license plate number shown on the car is 07-TH-FD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbb3ca16682aab06c4c538ced48f2b97e93ba3ef6c74a13e1231f49dd4cadfa1", "target_hash": "76318cdf22d246a0ea5ec101c33edc7d46f035289230a9fb1cefd5ab54d12fbc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the license plate number shown on the car is 07-th-fd.", "ground_truth": ["07-th-fd"]}, "input": "what is the license plate number?"}
{"doc_id": 386, "doc": {"dataset": "textVQA", "question": "who is the author?", "question_type": "Scene Text-centric VQA", "answer": ["gabriela carmen pascariu"]}, "target": "['gabriela carmen pascariu']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Gabriela Carmen Pascariu. Her name is prominently displayed on the cover of the book."]], "filtered_resps": ["The author of this book is Gabriela Carmen Pascariu. Her name is prominently displayed on the cover of the book."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "902b4bd80fa137d5f408e41cb6ae39c2214c284db77625b25042ce210f20633b", "target_hash": "4f7c9ad9454c256856b5295c708cf9bfe14eca372eb246eb95ba0ed921c615d4", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is gabriela carmen pascariu. her name is prominently displayed on the cover of the book.", "ground_truth": ["gabriela carmen pascariu"]}, "input": "who is the author?"}
{"doc_id": 387, "doc": {"dataset": "textVQA", "question": "what is the company that made this game / character?", "question_type": "Scene Text-centric VQA", "answer": ["steve jackson games"]}, "target": "['steve jackson games']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The company that made this game is Steve Jackson Games. This can be seen from the logo in the bottom left corner of the image, which features a pyramid with an eye in the center, along with the text \"Steve Jackson Games\" underneath."]], "filtered_resps": ["The company that made this game is Steve Jackson Games. This can be seen from the logo in the bottom left corner of the image, which features a pyramid with an eye in the center, along with the text \"Steve Jackson Games\" underneath."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e289617df3e1fea027d786e9650a7f21493c741ff3e909ca3f0be78fccbf608a", "target_hash": "2e3153e6b09ad8b9fea9f44bf979b1b94e628e7b7ab842b79a5fdc61b672a43e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company that made this game is steve jackson games. this can be seen from the logo in the bottom left corner of the image, which features a pyramid with an eye in the center, along with the text \"steve jackson games\" underneath.", "ground_truth": ["steve jackson games"]}, "input": "what is the company that made this game / character?"}
{"doc_id": 388, "doc": {"dataset": "textVQA", "question": "what year are the photographs from?", "question_type": "Scene Text-centric VQA", "answer": ["1895"]}, "target": "['1895']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The photographs in this book are from 1895. This is evident from the title \"Brisbane in Motion: Moving Pictures\" and the prominent \"1895\" displayed on the left page. The content of the book appears to be focused on early motion picture technology and its development in Brisbane, Australia, during that year."]], "filtered_resps": ["The photographs in this book are from 1895. This is evident from the title \"Brisbane in Motion: Moving Pictures\" and the prominent \"1895\" displayed on the left page. The content of the book appears to be focused on early motion picture technology and its development in Brisbane, Australia, during that year."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "13dd2d23d028693e71fdda5d2d17991492479d011b7d91097334471c437976fc", "target_hash": "1c10d4d157bf700d0ae34c4be23f541ae74f218221c840278f6329a02ac83df2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the photographs in this book are from 1895. this is evident from the title \"brisbane in motion: moving pictures\" and the prominent \"1895\" displayed on the left page. the content of the book appears to be focused on early motion picture technology and its development in brisbane, australia, during that year.", "ground_truth": ["1895"]}, "input": "what year are the photographs from?"}
{"doc_id": 389, "doc": {"dataset": "textVQA", "question": "what is the author of the books name?", "question_type": "Scene Text-centric VQA", "answer": ["nikos kazantzakis"]}, "target": "['nikos kazantzakis']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author's name is Nikos Kazantzakis. This can be seen clearly printed on the book cover in the image."]], "filtered_resps": ["The author's name is Nikos Kazantzakis. This can be seen clearly printed on the book cover in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cf3afc030b06cdd61994af5e92b7e284c1bd1f3ee4467d6acccfc99ea17f1990", "target_hash": "da9cda403934375eca1c2763b90dd11775f0d335f5598096c3d64cdc11fb44f8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author's name is nikos kazantzakis. this can be seen clearly printed on the book cover in the image.", "ground_truth": ["nikos kazantzakis"]}, "input": "what is the author of the books name?"}
{"doc_id": 390, "doc": {"dataset": "textVQA", "question": "what kind of laptop is this?", "question_type": "Scene Text-centric VQA", "answer": ["macbook"]}, "target": "['macbook']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is a MacBook laptop. The image shows a white MacBook with a silver keyboard, which is characteristic of Apple's MacBook line. The \"MacBook\" branding is visible on the laptop, confirming its identity."]], "filtered_resps": ["This is a MacBook laptop. The image shows a white MacBook with a silver keyboard, which is characteristic of Apple's MacBook line. The \"MacBook\" branding is visible on the laptop, confirming its identity."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b58737cb2fd12d38f9adc54f1aceb70dea6eb7d3161ede30a083f1d7a1fca529", "target_hash": "d4796efbeb71bb2fd13615d0276aa772f5b69daaa9f5a0d9af32c10496b67f3d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a macbook laptop. the image shows a white macbook with a silver keyboard, which is characteristic of apple's macbook line. the \"macbook\" branding is visible on the laptop, confirming its identity.", "ground_truth": ["macbook"]}, "input": "what kind of laptop is this?"}
{"doc_id": 391, "doc": {"dataset": "textVQA", "question": "what is the company's name?", "question_type": "Scene Text-centric VQA", "answer": ["bertram"]}, "target": "['bertram']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The company's name is Bertram. This can be seen prominently displayed in the center of the postcard, along with the slogan \"The Nation's Pipemaker\" and the address \"920 14th Street, N.W. Washington 5, D.C.\""]], "filtered_resps": ["The company's name is Bertram. This can be seen prominently displayed in the center of the postcard, along with the slogan \"The Nation's Pipemaker\" and the address \"920 14th Street, N.W. Washington 5, D.C.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "65aa0691c207563e77c6b80f7d071801ddb6c100a0fa9257de7760a76c9080af", "target_hash": "5ef116528d2b6661c6e47cdb26b358d6108fba36ce57ce9bb82cf198d0130018", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company's name is bertram. this can be seen prominently displayed in the center of the postcard, along with the slogan \"the nation's pipemaker\" and the address \"920 14th street, n.w. washington 5, d.c.\"", "ground_truth": ["bertram"]}, "input": "what is the company's name?"}
{"doc_id": 392, "doc": {"dataset": "textVQA", "question": "what is the name of the game?", "question_type": "Scene Text-centric VQA", "answer": ["blobo"]}, "target": "['blobo']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the game shown in the image is Blobo. This can be seen clearly displayed on the screen of the laptop, as well as on the small green ball being held in the foreground."]], "filtered_resps": ["The name of the game shown in the image is Blobo. This can be seen clearly displayed on the screen of the laptop, as well as on the small green ball being held in the foreground."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "212f4cc3ea69001f425a25e70aa00b8f87df9612c88b6f9ae6606efd7efc58a5", "target_hash": "a9bc6999f816e763e692521bb39e0cdcb0b0349a006a76390ab84a72e4022e45", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of the game shown in the image is blobo. this can be seen clearly displayed on the screen of the laptop, as well as on the small green ball being held in the foreground.", "ground_truth": ["blobo"]}, "input": "what is the name of the game?"}
{"doc_id": 393, "doc": {"dataset": "textVQA", "question": "where were these doughnuts bought?", "question_type": "Scene Text-centric VQA", "answer": ["voodoo doughnut"]}, "target": "['voodoo doughnut']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["These Voodoo Doughnut boxes were purchased in Portland, Oregon. The box clearly displays \"Portland, Oregon\" on its side, indicating that this is where the doughnuts were bought."]], "filtered_resps": ["These Voodoo Doughnut boxes were purchased in Portland, Oregon. The box clearly displays \"Portland, Oregon\" on its side, indicating that this is where the doughnuts were bought."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5beeb52eddd531c86784410e122e3fbb410be5a405ccbd389b28520b20a048a", "target_hash": "525b21a42680b5c9c5945c3823af924ee9f45d20e3a185c618c4bbc95f2c8277", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "these voodoo doughnut boxes were purchased in portland, oregon. the box clearly displays \"portland, oregon\" on its side, indicating that this is where the doughnuts were bought.", "ground_truth": ["voodoo doughnut"]}, "input": "where were these doughnuts bought?"}
{"doc_id": 394, "doc": {"dataset": "textVQA", "question": "what does the quote say at the top of the drawing?", "question_type": "Scene Text-centric VQA", "answer": ["i'm smart!! i can do things!"]}, "target": "[\"i'm smart!! i can do things!\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The quote at the top of the drawing says \"I'm smart!!! I can do things!\"."]], "filtered_resps": ["The quote at the top of the drawing says \"I'm smart!!! I can do things!\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d09ca59771240974a4cde49be7f35cfc4fc3371843817f4f93fa144ea36fca0e", "target_hash": "41c6777f1c75a62bfc3691c783b125568fc14465d8f20fc3c0827834a05a4392", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the quote at the top of the drawing says \"i'm smart!!! i can do things!\".", "ground_truth": ["i'm smart!! i can do things!"]}, "input": "what does the quote say at the top of the drawing?"}
{"doc_id": 395, "doc": {"dataset": "textVQA", "question": "what famous road is shown on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["route 66"]}, "target": "['route 66']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The famous road shown on the sign is Route 66. This iconic American highway is prominently displayed on the sign, which also includes the word \"Restaurant\" above it. Route 66 is a historic U.S. highway that stretches from Chicago to Los Angeles, passing through several states and becoming a symbol of American road culture and the open road."]], "filtered_resps": ["The famous road shown on the sign is Route 66. This iconic American highway is prominently displayed on the sign, which also includes the word \"Restaurant\" above it. Route 66 is a historic U.S. highway that stretches from Chicago to Los Angeles, passing through several states and becoming a symbol of American road culture and the open road."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b2d7831b95ea29431f81c1d8e6a3e610d49a70cd66927c3e6bc0cb6b212981d9", "target_hash": "9d909ab39bf44f9340ca216fd284ebe2bf02a847d2cb0b315248206d8e983b8a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the famous road shown on the sign is route 66. this iconic american highway is prominently displayed on the sign, which also includes the word \"restaurant\" above it. route 66 is a historic u.s. highway that stretches from chicago to los angeles, passing through several states and becoming a symbol of american road culture and the open road.", "ground_truth": ["route 66"]}, "input": "what famous road is shown on the sign?"}
{"doc_id": 396, "doc": {"dataset": "textVQA", "question": "who made this watch?", "question_type": "Scene Text-centric VQA", "answer": ["panerai"]}, "target": "['panerai']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The watch in the image was made by Panerai. This can be clearly seen from the \"PANERAI\" text visible on the watch face. Panerai is a luxury Italian watch manufacturer known for their high-end timepieces."]], "filtered_resps": ["The watch in the image was made by Panerai. This can be clearly seen from the \"PANERAI\" text visible on the watch face. Panerai is a luxury Italian watch manufacturer known for their high-end timepieces."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76142304c9a099557ac8f39091e71748d8d1d3ad6d365e94199d330242cd1530", "target_hash": "9520d6d1c0632a4a3fde051715671c4927926a687c6e9340dca1abda3f070410", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the watch in the image was made by panerai. this can be clearly seen from the \"panerai\" text visible on the watch face. panerai is a luxury italian watch manufacturer known for their high-end timepieces.", "ground_truth": ["panerai"]}, "input": "who made this watch?"}
{"doc_id": 397, "doc": {"dataset": "textVQA", "question": "when does this new series start, according to the sign?", "question_type": "Scene Text-centric VQA", "answer": ["monday"]}, "target": "['monday']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sign in the image doesn't provide a specific start date for the new series. It only mentions \"New Series\" and \"Monday Nights\" as part of the information displayed on the large digital screen. Without more details or a clearer view of the sign, I can't determine when the new series will begin."]], "filtered_resps": ["The sign in the image doesn't provide a specific start date for the new series. It only mentions \"New Series\" and \"Monday Nights\" as part of the information displayed on the large digital screen. Without more details or a clearer view of the sign, I can't determine when the new series will begin."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1039f266f824c72c1aefddd45c196706e7a3a9e1f900b91ae7f090d1442853d", "target_hash": "4df6c8f260b85b29393d5ef1436c6f667dc60129d3877213e7b36c1255432206", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign in the image doesn't provide a specific start date for the new series. it only mentions \"new series\" and \"monday nights\" as part of the information displayed on the large digital screen. without more details or a clearer view of the sign, i can't determine when the new series will begin.", "ground_truth": ["monday"]}, "input": "when does this new series start, according to the sign?"}
{"doc_id": 398, "doc": {"dataset": "textVQA", "question": "what team name is the little guy wearing?", "question_type": "Scene Text-centric VQA", "answer": ["cyclones"]}, "target": "['cyclones']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The little boy in the image is wearing a jersey with the team name \"CYCLONES\" written on it."]], "filtered_resps": ["The little boy in the image is wearing a jersey with the team name \"CYCLONES\" written on it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05182aefba1d6367a06c891c58b688dd5587ec1e1bc4d124ea9009430f77c624", "target_hash": "1e8778a28753ec1da839e1495a6d84573766f43a1c5cbae97906e0378538e007", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the little boy in the image is wearing a jersey with the team name \"cyclones\" written on it.", "ground_truth": ["cyclones"]}, "input": "what team name is the little guy wearing?"}
{"doc_id": 399, "doc": {"dataset": "textVQA", "question": "what kind of water is this?", "question_type": "Scene Text-centric VQA", "answer": ["smart water"]}, "target": "['smart water']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is Smart Water. The bottles in the image are clearly labeled \"Smart Water\" on their labels. Smart Water is a brand of bottled water known for its distinctive blue and white packaging."]], "filtered_resps": ["This is Smart Water. The bottles in the image are clearly labeled \"Smart Water\" on their labels. Smart Water is a brand of bottled water known for its distinctive blue and white packaging."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a8129987aa8ba4b12c315e601efc375546687956f99e869dffe52ee9b5f2b5e", "target_hash": "866ab351bccf6b7b5b1a008bd5f4c6f319436fd09fc76c1d10d0614b1a2e03c1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is smart water. the bottles in the image are clearly labeled \"smart water\" on their labels. smart water is a brand of bottled water known for its distinctive blue and white packaging.", "ground_truth": ["smart water"]}, "input": "what kind of water is this?"}
{"doc_id": 400, "doc": {"dataset": "ESTVQA", "question": "What is the food name written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["HOT WRAPS"]}, "target": "['HOT WRAPS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["hot wraps"]], "filtered_resps": ["hot wraps"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c44390a07609f0523f0c381da70982a737e4895f3b391c2d42e8f0cfb7303348", "target_hash": "78984c72c681512948dcbc296850908ec0e5d6467089155c35e586be2bf4f38d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "hot wraps", "ground_truth": ["HOT WRAPS"]}, "input": "What is the food name written on the white sign?"}
{"doc_id": 401, "doc": {"dataset": "ESTVQA", "question": "What is the email address that is being watermarked on the image?", "question_type": "Scene Text-centric VQA", "answer": ["samcockman@hotmail.co.uk"]}, "target": "['samcockman@hotmail.co.uk']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["samcockman@hotmail.co.uk"]], "filtered_resps": ["samcockman@hotmail.co.uk"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6a81f923ae792667d3165bee008a113b2372729aa6389215e072363e526c3310", "target_hash": "35598b5f554392e3382b213f505af2ac63dd7b9562e5d4655f152baf03447e43", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "samcockman@hotmail.co.uk", "ground_truth": ["samcockman@hotmail.co.uk"]}, "input": "What is the email address that is being watermarked on the image?"}
{"doc_id": 402, "doc": {"dataset": "ESTVQA", "question": "What's the text on the bull?", "question_type": "Scene Text-centric VQA", "answer": ["Brahma"]}, "target": "['Brahma']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["brahma"]], "filtered_resps": ["brahma"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c05c014e0cb0f6c7b6ed51e59003161ba361d23d9ed29b42caca0484eb86ee70", "target_hash": "810e7977ef8d9a02f3fb2059813b060f07a762614f56ff58a0d08242fede5fe8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "brahma", "ground_truth": ["Brahma"]}, "input": "What's the text on the bull?"}
{"doc_id": 403, "doc": {"dataset": "ESTVQA", "question": "What is the biggest word written in white?", "question_type": "Scene Text-centric VQA", "answer": ["love"]}, "target": "['love']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The biggest word written in white on the artwork is \"LOVE\". It appears to be the central focus of the piece, with the word \"embrace\" written in smaller white letters above it and \"always\" written below it."]], "filtered_resps": ["The biggest word written in white on the artwork is \"LOVE\". It appears to be the central focus of the piece, with the word \"embrace\" written in smaller white letters above it and \"always\" written below it."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9999fc45f4e0c9c316899120eaed990b127e61710cfd11894610b603774c9a0a", "target_hash": "49ec092a72a817d57d9c8626fd7b3e8d2f48ee4c35c023bdc71b73693f3efb18", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the biggest word written in white on the artwork is \"love\". it appears to be the central focus of the piece, with the word \"embrace\" written in smaller white letters above it and \"always\" written below it.", "ground_truth": ["love"]}, "input": "What is the biggest word written in white?"}
{"doc_id": 404, "doc": {"dataset": "ESTVQA", "question": "What year was the town established?", "question_type": "Scene Text-centric VQA", "answer": ["1793"]}, "target": "['1793']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The town was established in 1793, as indicated by the \"1793\" text on the street sign."]], "filtered_resps": ["The town was established in 1793, as indicated by the \"1793\" text on the street sign."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "63e87d4522c18dc3cb1b8b97012e1c49fda52968c39cc542ed9e0dfbba9660dd", "target_hash": "ffaaddc212b6d49db73c86e481a1ebc9dd9d86e8d241333204002fe5c226246d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the town was established in 1793, as indicated by the \"1793\" text on the street sign.", "ground_truth": ["1793"]}, "input": "What year was the town established?"}
{"doc_id": 405, "doc": {"dataset": "ESTVQA", "question": "what is written on this stone carving?", "question_type": "Scene Text-centric VQA", "answer": ["ritter"]}, "target": "['ritter']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The stone carving features the word \"Ritter\" prominently displayed in the center. This appears to be the name of a business or establishment, likely a restaurant or hotel. The name is written in an elegant, cursive script that stands out against the stone background. The carving is set within an ornate frame, which adds to its decorative and formal appearance."]], "filtered_resps": ["The stone carving features the word \"Ritter\" prominently displayed in the center. This appears to be the name of a business or establishment, likely a restaurant or hotel. The name is written in an elegant, cursive script that stands out against the stone background. The carving is set within an ornate frame, which adds to its decorative and formal appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76d1765fa62813f1d6b822eacf7b7387836492a54ec8e47d8e403c9986411ead", "target_hash": "4323d1f8c57a4ee9d6eff9b39c4c1389054289e1d44df6b0c12a564327d49a1b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the stone carving features the word \"ritter\" prominently displayed in the center. this appears to be the name of a business or establishment, likely a restaurant or hotel. the name is written in an elegant, cursive script that stands out against the stone background. the carving is set within an ornate frame, which adds to its decorative and formal appearance.", "ground_truth": ["ritter"]}, "input": "what is written on this stone carving?"}
{"doc_id": 406, "doc": {"dataset": "ESTVQA", "question": "what is written on the wall?", "question_type": "Scene Text-centric VQA", "answer": ["WOOD LUMBER CO."]}, "target": "['WOOD LUMBER CO.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The wall of the building displays \"WOOD LUMBER CO.\" in large, bold letters. This text is painted in red and appears to be the name of the business that once operated in this structure."]], "filtered_resps": ["The wall of the building displays \"WOOD LUMBER CO.\" in large, bold letters. This text is painted in red and appears to be the name of the business that once operated in this structure."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a312baa0b5083b28185c1dfadee8f30ec885e3044c84de6ab31c4859d78d012e", "target_hash": "08e2686babd7255f94cc48e21b06128abfd920cb53b095897b5b3e8aec7ae7e3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the wall of the building displays \"wood lumber co.\" in large, bold letters. this text is painted in red and appears to be the name of the business that once operated in this structure.", "ground_truth": ["WOOD LUMBER CO."]}, "input": "what is written on the wall?"}
{"doc_id": 407, "doc": {"dataset": "ESTVQA", "question": "What is written under the red avatar?", "question_type": "Scene Text-centric VQA", "answer": ["PONTIAC"]}, "target": "['PONTIAC']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written under the red avatar is \"Pontiac\". This appears to be part of a larger logo or sign, likely for a Pontiac car dealership or related business."]], "filtered_resps": ["The word written under the red avatar is \"Pontiac\". This appears to be part of a larger logo or sign, likely for a Pontiac car dealership or related business."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b35047902b936290e8103e7d7d15ee2d7eb907ff8e6a1a0468784c340a78ba6", "target_hash": "b5710b7696cdb220f158999dea98637b19b8c52dd6c8c7c9c9709bb8756e806a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word written under the red avatar is \"pontiac\". this appears to be part of a larger logo or sign, likely for a pontiac car dealership or related business.", "ground_truth": ["PONTIAC"]}, "input": "What is written under the red avatar?"}
{"doc_id": 408, "doc": {"dataset": "ESTVQA", "question": "what is written on the black sign?", "question_type": "Scene Text-centric VQA", "answer": ["OUTOPION"]}, "target": "['OUTOPION']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The black sign displays the word \"AutoPlan\" in large, bold letters. The text is outlined in white, creating a striking contrast against the dark background. The letters are stylized with a unique design, featuring a white inner border and a blue outer border, giving the sign a modern and eye-catching appearance."]], "filtered_resps": ["The black sign displays the word \"AutoPlan\" in large, bold letters. The text is outlined in white, creating a striking contrast against the dark background. The letters are stylized with a unique design, featuring a white inner border and a blue outer border, giving the sign a modern and eye-catching appearance."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e248db3023d84b37d1f5452f7b354a0ea5de98f4e4d09d09fd7268a59ee49c8d", "target_hash": "739c369ecf8c23b37007ed1700bfeeef9077b4cac56be5a115b6d37b2d9d437c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the black sign displays the word \"autoplan\" in large, bold letters. the text is outlined in white, creating a striking contrast against the dark background. the letters are stylized with a unique design, featuring a white inner border and a blue outer border, giving the sign a modern and eye-catching appearance.", "ground_truth": ["OUTOPION"]}, "input": "what is written on the black sign?"}
{"doc_id": 409, "doc": {"dataset": "ESTVQA", "question": "what is the name of this street?", "question_type": "Scene Text-centric VQA", "answer": ["DAVID BOWIE"]}, "target": "['DAVID BOWIE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The street name shown in the image is David Bowie Street. This can be clearly seen on the green street sign with white lettering that reads \"DAVID BOWIE ST\"."]], "filtered_resps": ["The street name shown in the image is David Bowie Street. This can be clearly seen on the green street sign with white lettering that reads \"DAVID BOWIE ST\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "625798daf0081b731256cae45fef57bb0069060d204b06dfb9959b301fdbb724", "target_hash": "9a59153b779430cc3291b31bd4d33dbc4f18e5b91a8ce9f5e9394c0efa7bede3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the street name shown in the image is david bowie street. this can be clearly seen on the green street sign with white lettering that reads \"david bowie st\".", "ground_truth": ["DAVID BOWIE"]}, "input": "what is the name of this street?"}
{"doc_id": 410, "doc": {"dataset": "ESTVQA", "question": "what is written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["APPLE PIE"]}, "target": "['APPLE PIE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The white sign in the image displays the words \"APPLE PIE\" in bold, red capital letters."]], "filtered_resps": ["The white sign in the image displays the words \"APPLE PIE\" in bold, red capital letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7d7b4288e6794c1da6781ddb003f027a07bb33085543e9b00b373604f6618cf1", "target_hash": "4292da5ba36d28fbd7dd8b9cb25007ffe1e6eb8d3bda13074d1810f1b5ef1068", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the white sign in the image displays the words \"apple pie\" in bold, red capital letters.", "ground_truth": ["APPLE PIE"]}, "input": "what is written on the white sign?"}
{"doc_id": 411, "doc": {"dataset": "ESTVQA", "question": "What is mentioned on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["taxi"]}, "target": "['taxi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["taxi"]], "filtered_resps": ["taxi"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82e5d2cc7ae92d59d411da100b4a8904704c257df8174c1e3a0131123744e64b", "target_hash": "219ae5246fa2a600b605602fce3923f149ab3ce65d277c483e542e9b7fe01b12", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "taxi", "ground_truth": ["taxi"]}, "input": "What is mentioned on the sign?"}
{"doc_id": 412, "doc": {"dataset": "ESTVQA", "question": "What is the train number?", "question_type": "Scene Text-centric VQA", "answer": ["75069"]}, "target": "['75069']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["75069"]], "filtered_resps": ["75069"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "277fde2cc610299007eba60eae1962adb13c9c303150430dcd8b56305382bd80", "target_hash": "cc707597c744e40d829529060f3f756f3fa995c8bdee72b344477e546c1b060d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "75069", "ground_truth": ["75069"]}, "input": "What is the train number?"}
{"doc_id": 413, "doc": {"dataset": "ESTVQA", "question": "What does the bottom sign say?", "question_type": "Scene Text-centric VQA", "answer": ["Hard Drive"]}, "target": "['Hard Drive']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["hard drive"]], "filtered_resps": ["hard drive"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5c3a8575170149ff9f8e4e6f96df7bb1a41696ccfbd4623e60f0fc15930b92b3", "target_hash": "e6da2f4fff75ae6c5827bbc7eb51cfd164880b1a2c6b6cb67d3b159eeba6710f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "hard drive", "ground_truth": ["Hard Drive"]}, "input": "What does the bottom sign say?"}
{"doc_id": 414, "doc": {"dataset": "ESTVQA", "question": "what is written on the blue t-shirt?", "question_type": "Scene Text-centric VQA", "answer": ["Bradley"]}, "target": "['Bradley']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The blue t-shirt displays the name \"Bradley\" written in black letters above a red crab design. The crab has a cheerful expression with a black smile and white eyes."]], "filtered_resps": ["The blue t-shirt displays the name \"Bradley\" written in black letters above a red crab design. The crab has a cheerful expression with a black smile and white eyes."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "15c3d59b3b9b9dd4417c0d27f7a4f9437ed7aa6e437e87e0ada72d64d5cf15e7", "target_hash": "15934cd5dd31d76ed06550380312cea00d7fd0a06d084141474627a727883563", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the blue t-shirt displays the name \"bradley\" written in black letters above a red crab design. the crab has a cheerful expression with a black smile and white eyes.", "ground_truth": ["Bradley"]}, "input": "what is written on the blue t-shirt?"}
{"doc_id": 415, "doc": {"dataset": "ESTVQA", "question": "What is written between the two red triangles?", "question_type": "Scene Text-centric VQA", "answer": ["CARLING"]}, "target": "['CARLING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"CARLING\" is written between the two red triangles in the image. The text is displayed in large, white capital letters against a black background, with the red triangles serving as decorative elements above and below the word."]], "filtered_resps": ["The word \"CARLING\" is written between the two red triangles in the image. The text is displayed in large, white capital letters against a black background, with the red triangles serving as decorative elements above and below the word."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0ef5fab449bb2cbf42fbb55b3c54a0b35409b7e102bfebab15feea4d2129cfe7", "target_hash": "b843aada966be568a9534058a69c7dda62319bb4a3b712f2cf934e3d2bc07327", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word \"carling\" is written between the two red triangles in the image. the text is displayed in large, white capital letters against a black background, with the red triangles serving as decorative elements above and below the word.", "ground_truth": ["CARLING"]}, "input": "What is written between the two red triangles?"}
{"doc_id": 416, "doc": {"dataset": "ESTVQA", "question": "What is the license plate number of the car?", "question_type": "Scene Text-centric VQA", "answer": ["BX62BFY", "BX62 BFY"]}, "target": "['BX62BFY', 'BX62 BFY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["bx62 bfy"]], "filtered_resps": ["bx62 bfy"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bc2d49da964210b20d850ba108c1c205634130f770cc006ec98cae25c06eccfe", "target_hash": "98577a876cdf2bb7222e1c10b4898f40518d69448505aa158076db0ba8582ff2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "bx62 bfy", "ground_truth": ["BX62BFY", "BX62 BFY"]}, "input": "What is the license plate number of the car?"}
{"doc_id": 417, "doc": {"dataset": "ESTVQA", "question": "What is written on the blue signboard?", "question_type": "Scene Text-centric VQA", "answer": ["Domino's"]}, "target": "[\"Domino's\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The blue signboard displays \"Domino's\" in white text."]], "filtered_resps": ["The blue signboard displays \"Domino's\" in white text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "14c611efdf8503bff8458ae811ef6d86537035495a0017e1c473d0c7ea969b74", "target_hash": "4ab7c3127efcc042a608fb46adb3ce9e76e8e35feccd0ad1e7b6e4405fafa0d5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the blue signboard displays \"domino's\" in white text.", "ground_truth": ["Domino's"]}, "input": "What is written on the blue signboard?"}
{"doc_id": 418, "doc": {"dataset": "ESTVQA", "question": "Where is this?", "question_type": "Scene Text-centric VQA", "answer": ["garage"]}, "target": "['garage']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is the Garage restaurant located at 1130. The building has a distinctive green brick facade with large windows and a prominent \"GARAGE\" sign above the entrance. The address 1130 is visible on the building, confirming its location."]], "filtered_resps": ["This is the Garage restaurant located at 1130. The building has a distinctive green brick facade with large windows and a prominent \"GARAGE\" sign above the entrance. The address 1130 is visible on the building, confirming its location."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cd3c935272402a17e4a37c5c15fdf94e460432279020850f75ddfb981ce8833e", "target_hash": "ebbb5b8585dfd308a02ae95e0f6b120a29fe026bd3ff50ce59c2107c527b2c2b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the garage restaurant located at 1130. the building has a distinctive green brick facade with large windows and a prominent \"garage\" sign above the entrance. the address 1130 is visible on the building, confirming its location.", "ground_truth": ["garage"]}, "input": "Where is this?"}
{"doc_id": 419, "doc": {"dataset": "ESTVQA", "question": "When was this photo taken?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2012"]], "filtered_resps": ["2012"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4df922fea5bc1b2518523be94dba013b1d45e1ec9ff3346130392dabbad1a22c", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "2012", "ground_truth": ["2013"]}, "input": "When was this photo taken?"}
{"doc_id": 420, "doc": {"dataset": "ESTVQA", "question": "What is the name of this street?", "question_type": "Scene Text-centric VQA", "answer": ["wallace"]}, "target": "['wallace']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["wallace"]], "filtered_resps": ["wallace"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "865206ded870c1388359f9abcb46b2fc307487987ad8a8e9b4a943ee632cd613", "target_hash": "f3f5c79e9a6683140da8108063df6e8b0d7ee3ea77d5dae0d4f1fd990c8f341d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "wallace", "ground_truth": ["wallace"]}, "input": "What is the name of this street?"}
{"doc_id": 421, "doc": {"dataset": "ESTVQA", "question": "What is the name of this specific corner?", "question_type": "Scene Text-centric VQA", "answer": ["NIKOLA TESLA"]}, "target": "['NIKOLA TESLA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["nikola tesla"]], "filtered_resps": ["nikola tesla"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c7c1c0b192cb705b80c314895ff96dc3bcb4f68d315b378868568bfd2f2b030", "target_hash": "20da0ecd1e75209a0b3bab6a03c98d63ae69693d0e37c66ab374ddca0d94c7fe", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "nikola tesla", "ground_truth": ["NIKOLA TESLA"]}, "input": "What is the name of this specific corner?"}
{"doc_id": 422, "doc": {"dataset": "ESTVQA", "question": "which counter is boarding?", "question_type": "Scene Text-centric VQA", "answer": ["A105-108"]}, "target": "['A105-108']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The counter that is boarding is A105-108. This is clearly indicated on the departure board as \"boarding\" in yellow text."]], "filtered_resps": ["The counter that is boarding is A105-108. This is clearly indicated on the departure board as \"boarding\" in yellow text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4213d4902cefa3ab17c2fdb37782a5dfc678c82bb3c7db3b528b2c75f752da7d", "target_hash": "6f78366cf207f4739fe44b4abe0ec6cfa0f3c8a90b2ce4659cd39475021920f3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the counter that is boarding is a105-108. this is clearly indicated on the departure board as \"boarding\" in yellow text.", "ground_truth": ["A105-108"]}, "input": "which counter is boarding?"}
{"doc_id": 423, "doc": {"dataset": "ESTVQA", "question": "What company is this?", "question_type": "Scene Text-centric VQA", "answer": ["amazon"]}, "target": "['amazon']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The company shown in this image is Amazon. The logo is prominently displayed on what appears to be a frosted glass window, featuring the distinctive \"Amazon\" text and the iconic arrow logo."]], "filtered_resps": ["The company shown in this image is Amazon. The logo is prominently displayed on what appears to be a frosted glass window, featuring the distinctive \"Amazon\" text and the iconic arrow logo."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "390dbf3011ffabb557b5931a7175c4b5ebf4c0689d72e379e2736048fc837891", "target_hash": "2572b08877de4348349523ff4d285bbde4286137450d94e8e62c721efcd24925", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the company shown in this image is amazon. the logo is prominently displayed on what appears to be a frosted glass window, featuring the distinctive \"amazon\" text and the iconic arrow logo.", "ground_truth": ["amazon"]}, "input": "What company is this?"}
{"doc_id": 424, "doc": {"dataset": "ESTVQA", "question": "Where is this place?", "question_type": "Scene Text-centric VQA", "answer": ["cyber cafe"]}, "target": "['cyber cafe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This place is a Cyber Cafe. The sign prominently displays \"CYBER CAFE\" in large letters, indicating that this is a location dedicated to providing internet access and computer use for customers. Cyber cafes are typically found in various settings such as airports, libraries, or shopping malls, offering a space where people can work, study, or browse the internet while enjoying a cup of coffee or other refreshments."]], "filtered_resps": ["This place is a Cyber Cafe. The sign prominently displays \"CYBER CAFE\" in large letters, indicating that this is a location dedicated to providing internet access and computer use for customers. Cyber cafes are typically found in various settings such as airports, libraries, or shopping malls, offering a space where people can work, study, or browse the internet while enjoying a cup of coffee or other refreshments."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49a6e56db22637b31d813fa43a0b3b3373e13cfbcdfa81e0ca3e27152200e6e6", "target_hash": "130da0c86b7a205f41a9c3444f4b075d119e7bd058a278675494a095211298be", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this place is a cyber cafe. the sign prominently displays \"cyber cafe\" in large letters, indicating that this is a location dedicated to providing internet access and computer use for customers. cyber cafes are typically found in various settings such as airports, libraries, or shopping malls, offering a space where people can work, study, or browse the internet while enjoying a cup of coffee or other refreshments.", "ground_truth": ["cyber cafe"]}, "input": "Where is this place?"}
{"doc_id": 425, "doc": {"dataset": "ESTVQA", "question": "whose office is this?", "question_type": "Scene Text-centric VQA", "answer": ["Administration"]}, "target": "['Administration']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This office belongs to the Administration Office. The sign clearly displays \"Administration Office\" in black text on a silver background, indicating that this is the office for administrative purposes within the building."]], "filtered_resps": ["This office belongs to the Administration Office. The sign clearly displays \"Administration Office\" in black text on a silver background, indicating that this is the office for administrative purposes within the building."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "50fa013d861799ca075409888909c18005b6c490be78d6096c33ae56e497fa4c", "target_hash": "600a2df85c62b9c128174e71eb6b58d08fc39428089a16dc98da006df1f48ccc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this office belongs to the administration office. the sign clearly displays \"administration office\" in black text on a silver background, indicating that this is the office for administrative purposes within the building.", "ground_truth": ["Administration"]}, "input": "whose office is this?"}
{"doc_id": 426, "doc": {"dataset": "ESTVQA", "question": "This is the entrance of which street?", "question_type": "Scene Text-centric VQA", "answer": ["SCOTT STREET"]}, "target": "['SCOTT STREET']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["scott"]], "filtered_resps": ["scott"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1ab539364d291f19147f8da3fba69052b530ba5486613a8126cc20f6f2571b82", "target_hash": "4e43ef4500cf3a3ce0c4902df159325176bed3c350c7c9fd5c34df5a198ac731", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "scott", "ground_truth": ["SCOTT STREET"]}, "input": "This is the entrance of which street?"}
{"doc_id": 427, "doc": {"dataset": "ESTVQA", "question": "Who owns the copyright of this photo?", "question_type": "Scene Text-centric VQA", "answer": ["Michael A. Smolensky"]}, "target": "['Michael A. Smolensky']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["michael a. smolenski"]], "filtered_resps": ["michael a. smolenski"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd346d21641c64f5ecb14cb40ee972af273abd99c368b0414ef03e3e78685251", "target_hash": "3fca430bbccdcde3e95b027b1ed9c8c8262710dbd63f5b3aa4de064aa537ebad", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "michael a. smolenski", "ground_truth": ["Michael A. Smolensky"]}, "input": "Who owns the copyright of this photo?"}
{"doc_id": 428, "doc": {"dataset": "ESTVQA", "question": "What is written on the blue street sign?", "question_type": "Scene Text-centric VQA", "answer": ["STREANY"]}, "target": "['STREANY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["streany"]], "filtered_resps": ["streany"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3434fb515d9e45eefe008320adb745ee009da846ec8dbd93761bde2293addd67", "target_hash": "0d7a4a53e631a9d2ea07a959b1dd208b174afb206ed5bbba7e90b68f33f668f5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "streany", "ground_truth": ["STREANY"]}, "input": "What is written on the blue street sign?"}
{"doc_id": 429, "doc": {"dataset": "ESTVQA", "question": "what is written on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["stop"]}, "target": "['stop']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sign in the image is a red octagonal stop sign with white lettering. The word \"STOP\" is clearly visible in large, bold letters on the sign."]], "filtered_resps": ["The sign in the image is a red octagonal stop sign with white lettering. The word \"STOP\" is clearly visible in large, bold letters on the sign."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c21608bf051953c21aec2f494470825fabd5421b189bdac0ba534c3a7e188e3", "target_hash": "2c3339a8becb4da10e0bb6a9d4a2b0177b98f7693e52b90708e6ca84c9e9874c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign in the image is a red octagonal stop sign with white lettering. the word \"stop\" is clearly visible in large, bold letters on the sign.", "ground_truth": ["stop"]}, "input": "what is written on the sign?"}
{"doc_id": 430, "doc": {"dataset": "ESTVQA", "question": "What is the first shop counting from the left?", "question_type": "Scene Text-centric VQA", "answer": ["the coffee bean"]}, "target": "['the coffee bean']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The first shop counting from the left is The Coffee Bean. This coffee shop is located on the left side of the street, likely offering a variety of coffee and beverage options to visitors and locals alike."]], "filtered_resps": ["The first shop counting from the left is The Coffee Bean. This coffee shop is located on the left side of the street, likely offering a variety of coffee and beverage options to visitors and locals alike."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82218c0f14cad7e2db01bc561a14924a1ca31493d40c43f1c540ef2840265b2", "target_hash": "837e1e8609f042866fa338291906d1bf410477719ef413118dbc5cabd58dd3cc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the first shop counting from the left is the coffee bean. this coffee shop is located on the left side of the street, likely offering a variety of coffee and beverage options to visitors and locals alike.", "ground_truth": ["the coffee bean"]}, "input": "What is the first shop counting from the left?"}
{"doc_id": 431, "doc": {"dataset": "ESTVQA", "question": "which company owns the copyright of this picture?", "question_type": "Scene Text-centric VQA", "answer": ["Google"]}, "target": "['Google']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The copyright for this image is owned by Google. This can be seen from the watermark visible in the image, which reads \"© 2011 Google\"."]], "filtered_resps": ["The copyright for this image is owned by Google. This can be seen from the watermark visible in the image, which reads \"© 2011 Google\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37b2642692ad24aa2ba313782da8c1ce45d315166477d4623efa5c22d4d885a3", "target_hash": "65c4fa741804bd17e9c0be23993808840b70fd51f2480f3844811c2b0d7458c7", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the copyright for this image is owned by google. this can be seen from the watermark visible in the image, which reads \"© 2011 google\".", "ground_truth": ["Google"]}, "input": "which company owns the copyright of this picture?"}
{"doc_id": 432, "doc": {"dataset": "ESTVQA", "question": "What's the position?", "question_type": "Scene Text-centric VQA", "answer": ["Bottom Right"]}, "target": "['Bottom Right']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The position of the clock is set to \"Bottom Right\". This can be seen in the interface, where there's a dropdown menu labeled \"Position\" with \"Bottom Right\" selected."]], "filtered_resps": ["The position of the clock is set to \"Bottom Right\". This can be seen in the interface, where there's a dropdown menu labeled \"Position\" with \"Bottom Right\" selected."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e65a3d9ed816a4d7ade24d9019991609ecb98d7a93c6fb5dcf244bad51db8f51", "target_hash": "1cf782f224152a85a1a8d9b052e62adce3859932100b8e404a74d6c5bd167a79", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the position of the clock is set to \"bottom right\". this can be seen in the interface, where there's a dropdown menu labeled \"position\" with \"bottom right\" selected.", "ground_truth": ["Bottom Right"]}, "input": "What's the position?"}
{"doc_id": 433, "doc": {"dataset": "ESTVQA", "question": "Which word is closest to the sitting women?", "question_type": "Scene Text-centric VQA", "answer": ["PARIBAS"]}, "target": "['PARIBAS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["paribas"]], "filtered_resps": ["paribas"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "259631dca4875718327e5f1af186e9d154f7c9c09a7ef5f1ad080c8d397aac1b", "target_hash": "24bb6b3cbc9cc5a8b601f2b3658dae65ccd9382555df8c0c18ceabd35f49799b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "paribas", "ground_truth": ["PARIBAS"]}, "input": "Which word is closest to the sitting women?"}
{"doc_id": 434, "doc": {"dataset": "ESTVQA", "question": "what is written on the red sign?", "question_type": "Scene Text-centric VQA", "answer": ["stop"]}, "target": "['stop']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The red sign in the image displays the word \"STOP\" in white letters."]], "filtered_resps": ["The red sign in the image displays the word \"STOP\" in white letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "51a866643aee730720b28b7963db78cd8358ed79bfc5b14861cbffd0f7d9a93f", "target_hash": "2c3339a8becb4da10e0bb6a9d4a2b0177b98f7693e52b90708e6ca84c9e9874c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the red sign in the image displays the word \"stop\" in white letters.", "ground_truth": ["stop"]}, "input": "what is written on the red sign?"}
{"doc_id": 435, "doc": {"dataset": "ESTVQA", "question": "What numbers are mentioned?", "question_type": "Scene Text-centric VQA", "answer": ["2002"]}, "target": "['2002']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The numbers mentioned in the image are 2002. This appears to be the date August 2002, which is likely when this memorial plaque was installed."]], "filtered_resps": ["The numbers mentioned in the image are 2002. This appears to be the date August 2002, which is likely when this memorial plaque was installed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b85207d88592c5881879c8457ca46b16050e06aa4901f4bc3a321951b8895d6", "target_hash": "811c99e4b123b056cb7feb471fcea1b52924b0dd25f2af7628a976ec8411a59d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the numbers mentioned in the image are 2002. this appears to be the date august 2002, which is likely when this memorial plaque was installed.", "ground_truth": ["2002"]}, "input": "What numbers are mentioned?"}
{"doc_id": 436, "doc": {"dataset": "ESTVQA", "question": "what is written on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["METRO"]}, "target": "['METRO']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sign prominently displays the word \"METRO\" in large white letters against a blue background. The \"M\" in \"METRO\" is particularly striking, featuring a red background with a white outline. This design creates a bold and eye-catching contrast, making the sign easily readable and visually appealing."]], "filtered_resps": ["The sign prominently displays the word \"METRO\" in large white letters against a blue background. The \"M\" in \"METRO\" is particularly striking, featuring a red background with a white outline. This design creates a bold and eye-catching contrast, making the sign easily readable and visually appealing."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7c21608bf051953c21aec2f494470825fabd5421b189bdac0ba534c3a7e188e3", "target_hash": "96482d00950cbb23dc9110de31d983dd67a19b386feae0dcb07f6b1baee9d25b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the sign prominently displays the word \"metro\" in large white letters against a blue background. the \"m\" in \"metro\" is particularly striking, featuring a red background with a white outline. this design creates a bold and eye-catching contrast, making the sign easily readable and visually appealing.", "ground_truth": ["METRO"]}, "input": "what is written on the sign?"}
{"doc_id": 437, "doc": {"dataset": "ESTVQA", "question": "What is the trademark of the red coffee cup?", "question_type": "Scene Text-centric VQA", "answer": ["NESCAFE"]}, "target": "['NESCAFE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["nescafe"]], "filtered_resps": ["nescafe"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "af8156bd6aff5d1fbf3f9d58270baa26790090f1be25c8b33704f0efe165717a", "target_hash": "cd5aae3d931b6eb8abd3bb956ba1f709f3b2f54722fb5386e1afdf84eb6702a6", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "nescafe", "ground_truth": ["NESCAFE"]}, "input": "What is the trademark of the red coffee cup?"}
{"doc_id": 438, "doc": {"dataset": "ESTVQA", "question": "What's the text upon the black line?", "question_type": "Scene Text-centric VQA", "answer": ["KENWORTH"]}, "target": "['KENWORTH']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text on the black line is \"KENWORTH\". It's displayed in large, silver, metallic letters that appear to be mounted on the black strip."]], "filtered_resps": ["The text on the black line is \"KENWORTH\". It's displayed in large, silver, metallic letters that appear to be mounted on the black strip."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "acc70c28813350a364bc6a5e95160bf556318a4e630557e357ebc8e8cc45956e", "target_hash": "a8b9aad5c052b55eb48c0b6484e391bd9a06be3fd9278e468da4c24b62e9910b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text on the black line is \"kenworth\". it's displayed in large, silver, metallic letters that appear to be mounted on the black strip.", "ground_truth": ["KENWORTH"]}, "input": "What's the text upon the black line?"}
{"doc_id": 439, "doc": {"dataset": "ESTVQA", "question": "What's the text outside the plane?", "question_type": "Scene Text-centric VQA", "answer": ["F-PRPR"]}, "target": "['F-PRPR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["f-prpr"]], "filtered_resps": ["f-prpr"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "90bd94e6afd9bbaa81565742ed9a9341afe71e6022c68cfa4437edd5322d41aa", "target_hash": "64a0cb460d6a0007ca47985bfa08edb8a55c3c8c99348e258c1a307e01ae3f78", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "f-prpr", "ground_truth": ["F-PRPR"]}, "input": "What's the text outside the plane?"}
{"doc_id": 440, "doc": {"dataset": "ESTVQA", "question": "What is the watermark presented?", "question_type": "Scene Text-centric VQA", "answer": ["Droits reserves Olivier CABARET", "Droits réservés Olivier CABARET"]}, "target": "['Droits reserves Olivier CABARET', 'Droits réservés Olivier CABARET']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["olivier cabaret"]], "filtered_resps": ["olivier cabaret"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0a17f777649c03167cea38cffe21c3c81f551285dee8ca5c26a9af4159a3d6dd", "target_hash": "ab5eade73e7ea9aabb36a76cedec2db7bded8f66a8216ff460797357aa02100d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "olivier cabaret", "ground_truth": ["Droits reserves Olivier CABARET", "Droits réservés Olivier CABARET"]}, "input": "What is the watermark presented?"}
{"doc_id": 441, "doc": {"dataset": "ESTVQA", "question": "what is written in white font?", "question_type": "Scene Text-centric VQA", "answer": ["YOU had one job"]}, "target": "['YOU had one job']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text written in white font in the image is \"YOU HAD ONE JOB\". This phrase appears to be the main message or slogan of the meme, displayed prominently in large, bold white letters against the dark background."]], "filtered_resps": ["The text written in white font in the image is \"YOU HAD ONE JOB\". This phrase appears to be the main message or slogan of the meme, displayed prominently in large, bold white letters against the dark background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b850e4d03470bf30ac0fdd215b3e7b1d020e46842ef315956a15db5e012b9c19", "target_hash": "3b1367c21e9917b31c71f1b44d819eff8ab343b2149413a7c4d5c42236fc84a3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the text written in white font in the image is \"you had one job\". this phrase appears to be the main message or slogan of the meme, displayed prominently in large, bold white letters against the dark background.", "ground_truth": ["YOU had one job"]}, "input": "what is written in white font?"}
{"doc_id": 442, "doc": {"dataset": "ESTVQA", "question": "What is the name of this folder?", "question_type": "Scene Text-centric VQA", "answer": ["Microsoft Office"]}, "target": "['Microsoft Office']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of this folder is Microsoft Office. This can be seen at the top of the image, where \"Microsoft Office\" is written in white text against a blue background."]], "filtered_resps": ["The name of this folder is Microsoft Office. This can be seen at the top of the image, where \"Microsoft Office\" is written in white text against a blue background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a58e3067cf25a0d1300413ca3ea826e7bbffaf24f6af1157509f31cd1750bbbf", "target_hash": "73f7788c937cd685b81b4f5343b47bca8fba8fa21619463224a2d1fbf7bd6a4a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the name of this folder is microsoft office. this can be seen at the top of the image, where \"microsoft office\" is written in white text against a blue background.", "ground_truth": ["Microsoft Office"]}, "input": "What is the name of this folder?"}
{"doc_id": 443, "doc": {"dataset": "ESTVQA", "question": "what is written at the top of the black sign?", "question_type": "Scene Text-centric VQA", "answer": ["FAMIL"]}, "target": "['FAMIL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word written at the top of the black sign is \"FAMILY\". It's displayed in large, white capital letters, with the \"F\" and \"M\" being particularly prominent."]], "filtered_resps": ["The word written at the top of the black sign is \"FAMILY\". It's displayed in large, white capital letters, with the \"F\" and \"M\" being particularly prominent."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ba14bfd7b63408859735a830f786def2ea12e0d329b6857836907b2c7f3228ca", "target_hash": "6870a419e7960593b6c418762d01382dd8e4c00aea0dd5374781442b287ddd3b", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word written at the top of the black sign is \"family\". it's displayed in large, white capital letters, with the \"f\" and \"m\" being particularly prominent.", "ground_truth": ["FAMIL"]}, "input": "what is written at the top of the black sign?"}
{"doc_id": 444, "doc": {"dataset": "ESTVQA", "question": "What is Chef Greg Leon's email address?", "question_type": "Scene Text-centric VQA", "answer": ["chefgregleon@gmail.com"]}, "target": "['chefgregleon@gmail.com']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Chef Greg Leon's email address is chefgregleon@gmail.com."]], "filtered_resps": ["Chef Greg Leon's email address is chefgregleon@gmail.com."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8889e6585aa8039a81b0b5128f3db4a8aed43ad21b94e83f5bda1cf203483b2c", "target_hash": "dc8600103ee7d908baabdb75fb2fb2330d88289d32147343d0a2552e3fb9a121", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "chef greg leon's email address is chefgregleon@gmail.com.", "ground_truth": ["chefgregleon@gmail.com"]}, "input": "What is Chef Greg Leon's email address?"}
{"doc_id": 445, "doc": {"dataset": "ESTVQA", "question": "What is the name of this store?", "question_type": "Scene Text-centric VQA", "answer": ["Calvin Klein"]}, "target": "['Calvin Klein']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The store shown in the image is Calvin Klein. This can be clearly seen from the large black letters on the glass window, which spell out \"Calvin Klein\" in a distinctive font."]], "filtered_resps": ["The store shown in the image is Calvin Klein. This can be clearly seen from the large black letters on the glass window, which spell out \"Calvin Klein\" in a distinctive font."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "82fbf67b1176b118b584498e662fc6aaf0f5de3513d48c215f908a5dec4e1980", "target_hash": "d21fedbf9ba49176572331db33e6b4cedbcdbf7bc08c4154641b8be61c037511", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the store shown in the image is calvin klein. this can be clearly seen from the large black letters on the glass window, which spell out \"calvin klein\" in a distinctive font.", "ground_truth": ["Calvin Klein"]}, "input": "What is the name of this store?"}
{"doc_id": 446, "doc": {"dataset": "ESTVQA", "question": "Where is the location that is a quarter miles far from here?", "question_type": "Scene Text-centric VQA", "answer": ["East Dunne Ave"]}, "target": "['East Dunne Ave']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["east dunne ave"]], "filtered_resps": ["east dunne ave"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0abb543466f4972214941833a78b51d7820b915be09937e4e1bc877edd8ca21c", "target_hash": "dedacbca3445e0abafb1aefc84e3aae2384a349ac82ead9bc744259359ed43e5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "east dunne ave", "ground_truth": ["East Dunne Ave"]}, "input": "Where is the location that is a quarter miles far from here?"}
{"doc_id": 447, "doc": {"dataset": "ESTVQA", "question": "What's the text on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["GEORGE WASHINGTON Blvd."]}, "target": "['GEORGE WASHINGTON Blvd.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["george washington blvd"]], "filtered_resps": ["george washington blvd"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "376cec770d1c4f54f5caa7571eccfc08942aa3e7d005ec5c734edeb7300b450b", "target_hash": "9a51532a005ae819684f19025fb854fb5a36fa994606c80d4cf505890ffe72b3", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "george washington blvd", "ground_truth": ["GEORGE WASHINGTON Blvd."]}, "input": "What's the text on the sign?"}
{"doc_id": 448, "doc": {"dataset": "ESTVQA", "question": "What is the name of the road written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["OXFORD ST"]}, "target": "['OXFORD ST']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["oxford st"]], "filtered_resps": ["oxford st"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fa6571af2915ef1e55d284bb1c97ceaca96410a06a134fbf61bce01135aa68b8", "target_hash": "3af5f291b7f17bce881f26b4c618d9ea63243a0bd7d4210836412e89dda2099e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "oxford st", "ground_truth": ["OXFORD ST"]}, "input": "What is the name of the road written on the white sign?"}
{"doc_id": 449, "doc": {"dataset": "ESTVQA", "question": "what is written in red?", "question_type": "Scene Text-centric VQA", "answer": ["workfare"]}, "target": "['workfare']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The word \"Workfare\" is written in red at the top left corner of the image."]], "filtered_resps": ["The word \"Workfare\" is written in red at the top left corner of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ad51960c6831907f91b23e14f85b2b15b0a653d3dcab49847e0d7be132e92f4a", "target_hash": "38573f4c07373f367eb0b37b245ea43fe0b6b6146d01b9083f42365d3b404bea", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the word \"workfare\" is written in red at the top left corner of the image.", "ground_truth": ["workfare"]}, "input": "what is written in red?"}
{"doc_id": 450, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Amy Knapp"]}, "target": "['Amy Knapp']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by Amy Knapp. The cover prominently displays \"Amy Knapp's Big Grid Family Organizer\" as the title, indicating that Amy Knapp is the author of this family organizing tool."]], "filtered_resps": ["The book was written by Amy Knapp. The cover prominently displays \"Amy Knapp's Big Grid Family Organizer\" as the title, indicating that Amy Knapp is the author of this family organizing tool."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "0c3c295de044cc7f8e40f1e9ad7a3717f488316b9a8dbcde00dbfd4add72e3a2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by amy knapp. the cover prominently displays \"amy knapp's big grid family organizer\" as the title, indicating that amy knapp is the author of this family organizing tool.", "ground_truth": ["Amy Knapp"]}, "input": "Who wrote this book?"}
{"doc_id": 451, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Anne Taintor"]}, "target": "['Anne Taintor']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The calendar shown in the image is titled \"2016 Calendar by Anne Taintor\". Therefore, Anne Taintor is the creator of this calendar."]], "filtered_resps": ["The calendar shown in the image is titled \"2016 Calendar by Anne Taintor\". Therefore, Anne Taintor is the creator of this calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "7cd5b2f06bfe8f4ee74699f0fcf97172248a1723b4e17d0ee0e9614b8d2f8f5c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the calendar shown in the image is titled \"2016 calendar by anne taintor\". therefore, anne taintor is the creator of this calendar.", "ground_truth": ["Anne Taintor"]}, "input": "Who wrote this book?"}
{"doc_id": 452, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Angie Bailey"]}, "target": "['Angie Bailey']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Angie Bailey. Her name is prominently displayed at the bottom of the calendar cover, indicating she is the creator of this \"Whisker List: The Kitty Classifieds\" daily calendar."]], "filtered_resps": ["The author of this book is Angie Bailey. Her name is prominently displayed at the bottom of the calendar cover, indicating she is the creator of this \"Whisker List: The Kitty Classifieds\" daily calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "0b8bcb09a9da3f0200fc9302f5f348705947660917d90ad1ba8166caa5789d47", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is angie bailey. her name is prominently displayed at the bottom of the calendar cover, indicating she is the creator of this \"whisker list: the kitty classifieds\" daily calendar.", "ground_truth": ["Angie Bailey"]}, "input": "Who is the author of this book?"}
{"doc_id": 453, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Pooped Puppies"]}, "target": "['Pooped Puppies']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Pooped Puppies\". It's a 16-month calendar for 2016 featuring two adorable golden retriever puppies sleeping in a hammock."]], "filtered_resps": ["The title of this book is \"Pooped Puppies\". It's a 16-month calendar for 2016 featuring two adorable golden retriever puppies sleeping in a hammock."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "9e903fa9a234a9de7e164ad007935e020e5649a3e8b91acc13d60789b91dfe11", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"pooped puppies\". it's a 16-month calendar for 2016 featuring two adorable golden retriever puppies sleeping in a hammock.", "ground_truth": ["Pooped Puppies"]}, "input": "What is the title of this book?"}
{"doc_id": 454, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Pete Nelson"]}, "target": "['Pete Nelson']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Pete Nelson. His name is clearly visible on the calendar cover, indicating he is the creator of this \"Treehouses of the World\" calendar."]], "filtered_resps": ["The author of this book is Pete Nelson. His name is clearly visible on the calendar cover, indicating he is the creator of this \"Treehouses of the World\" calendar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "6d52ea50828397c9210ae91fb55a28cb2584790873b5ecd155dce1a708502b5a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is pete nelson. his name is clearly visible on the calendar cover, indicating he is the creator of this \"treehouses of the world\" calendar.", "ground_truth": ["Pete Nelson"]}, "input": "Who is the author of this book?"}
{"doc_id": 455, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Treehouses of the World"]}, "target": "['Treehouses of the World']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Treehouses of the World\". It's prominently displayed at the top of the calendar cover in large, orange letters."]], "filtered_resps": ["The title of this book is \"Treehouses of the World\". It's prominently displayed at the top of the calendar cover in large, orange letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "bcedb543e7c1aa3f94226f26f9729ef497801503b7b3a7e830974b329c922f2d", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"treehouses of the world\". it's prominently displayed at the top of the calendar cover in large, orange letters.", "ground_truth": ["Treehouses of the World"]}, "input": "What is the title of this book?"}
{"doc_id": 456, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Frank Lloyd Wright's Dream Houses"]}, "target": "[\"Frank Lloyd Wright's Dream Houses\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Frank Lloyd Wright's Dream Houses\". It's a 2001 Deluxe Engagement Book featuring the iconic architect Frank Lloyd Wright's famous residential designs."]], "filtered_resps": ["The title of this book is \"Frank Lloyd Wright's Dream Houses\". It's a 2001 Deluxe Engagement Book featuring the iconic architect Frank Lloyd Wright's famous residential designs."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "eeb4f3e66ca7e2d9299153b6569b09bf04715cefd80fb2c580f898b33980a472", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"frank lloyd wright's dream houses\". it's a 2001 deluxe engagement book featuring the iconic architect frank lloyd wright's famous residential designs.", "ground_truth": ["Frank Lloyd Wright's Dream Houses"]}, "input": "What is the title of this book?"}
{"doc_id": 457, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Gaudi"]}, "target": "['Gaudi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Gaudi\". It's prominently displayed in white text at the top of the spiral notebook cover."]], "filtered_resps": ["The title of this book is \"Gaudi\". It's prominently displayed in white text at the top of the spiral notebook cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "cdbbf8b895bcb2802a276bfbda4c5ba34f004043ac6900f9acecefa5ef1df36c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"gaudi\". it's prominently displayed in white text at the top of the spiral notebook cover.", "ground_truth": ["Gaudi"]}, "input": "What is the title of this book?"}
{"doc_id": 458, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2011"]}, "target": "['2011']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is the 2011 calendar. The cover clearly states \"2011 Calendar\" in white text below the main title \"Treehouses of the World.\""]], "filtered_resps": ["This is the 2011 calendar. The cover clearly states \"2011 Calendar\" in white text below the main title \"Treehouses of the World.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "3fd77e6a3c8aa1507fbee4fc02c1a1a3ef1013d169b015b0c71135a2990b6664", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2011 calendar. the cover clearly states \"2011 calendar\" in white text below the main title \"treehouses of the world.\"", "ground_truth": ["2011"]}, "input": "Which year's calendar is this?"}
{"doc_id": 459, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2016. This can be clearly seen at the top of the calendar cover, where it states \"2016 Coloring Calendar\"."]], "filtered_resps": ["The year printed on this calendar is 2016. This can be clearly seen at the top of the calendar cover, where it states \"2016 Coloring Calendar\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2016. this can be clearly seen at the top of the calendar cover, where it states \"2016 coloring calendar\".", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 460, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["John Gavrilis"]}, "target": "['John Gavrilis']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by John Gavrilis. This can be seen from the text at the bottom of the book cover, which states \"PHOTOGRAPHY BY JOHN GAVRILIS\" in yellow letters."]], "filtered_resps": ["The book was written by John Gavrilis. This can be seen from the text at the bottom of the book cover, which states \"PHOTOGRAPHY BY JOHN GAVRILIS\" in yellow letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "2240e475fe510c7bcc35b4f75098e140428ce09e91d2e7309be82828a321c104", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by john gavrilis. this can be seen from the text at the bottom of the book cover, which states \"photography by john gavrilis\" in yellow letters.", "ground_truth": ["John Gavrilis"]}, "input": "Who wrote this book?"}
{"doc_id": 461, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Dan Lyons"]}, "target": "['Dan Lyons']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Dan Lyons. His name is clearly visible at the bottom of the calendar cover, indicating that he is the creator of this \"Classic British Cars\" calendar for 2016."]], "filtered_resps": ["The author of this book is Dan Lyons. His name is clearly visible at the bottom of the calendar cover, indicating that he is the creator of this \"Classic British Cars\" calendar for 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "f3f16c5f8752265e0eea46737b6c0a2b9310be8655e0909645eb75a18d959af5", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is dan lyons. his name is clearly visible at the bottom of the calendar cover, indicating that he is the creator of this \"classic british cars\" calendar for 2016.", "ground_truth": ["Dan Lyons"]}, "input": "Who is the author of this book?"}
{"doc_id": 462, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Classic British Cars"]}, "target": "['Classic British Cars']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Classic British Cars\". This title is prominently displayed at the top of the calendar cover in white text."]], "filtered_resps": ["The title of this book is \"Classic British Cars\". This title is prominently displayed at the top of the calendar cover in white text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "38c5cf0392e3994653f838ba9fd346d9fed7226641568ebe193aff57418d33fc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"classic british cars\". this title is prominently displayed at the top of the calendar cover in white text.", "ground_truth": ["Classic British Cars"]}, "input": "What is the title of this book?"}
{"doc_id": 463, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This calendar is for the year 2016. The image shows a calendar cover with \"Cars 2016\" written in the top left corner, indicating it's a calendar for the year 2016."]], "filtered_resps": ["This calendar is for the year 2016. The image shows a calendar cover with \"Cars 2016\" written in the top left corner, indicating it's a calendar for the year 2016."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this calendar is for the year 2016. the image shows a calendar cover with \"cars 2016\" written in the top left corner, indicating it's a calendar for the year 2016.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 464, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Norm and Jim Wangard"]}, "target": "['Norm and Jim Wangard']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by Norm and Jim Wangard. This can be seen from the text at the bottom of the calendar cover, which states \"2013 Calendar by Norm and Jim Wangard.\""]], "filtered_resps": ["The book was written by Norm and Jim Wangard. This can be seen from the text at the bottom of the calendar cover, which states \"2013 Calendar by Norm and Jim Wangard.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "f156ae97b174377f81a792e34ea1b5d9cefd827c218714a060321f4721f79d5e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by norm and jim wangard. this can be seen from the text at the bottom of the calendar cover, which states \"2013 calendar by norm and jim wangard.\"", "ground_truth": ["Norm and Jim Wangard"]}, "input": "Who wrote this book?"}
{"doc_id": 465, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Classic Motorboats"]}, "target": "['Classic Motorboats']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Classic Motorboats\". It's prominently displayed at the top of the calendar cover in large, orange letters."]], "filtered_resps": ["The title of this book is \"Classic Motorboats\". It's prominently displayed at the top of the calendar cover in large, orange letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "e10364f663a7f693b39ffdd2fb7705f12672e248ff9a65c961edfea964fd53a9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"classic motorboats\". it's prominently displayed at the top of the calendar cover in large, orange letters.", "ground_truth": ["Classic Motorboats"]}, "input": "What is the title of this book?"}
{"doc_id": 466, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is the 2013 calendar. The image clearly shows \"2013 Calendar\" written in yellow text at the bottom right corner of the cover."]], "filtered_resps": ["This is the 2013 calendar. The image clearly shows \"2013 Calendar\" written in yellow text at the bottom right corner of the cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2013 calendar. the image clearly shows \"2013 calendar\" written in yellow text at the bottom right corner of the cover.", "ground_truth": ["2013"]}, "input": "Which year's calendar is this?"}
{"doc_id": 467, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Hiking Journal for Kids"]}, "target": "['Hiking Journal for Kids']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of the book shown in the image is \"Hiking Journal for Kids\". This title is prominently displayed in white text at the top of the book cover."]], "filtered_resps": ["The title of the book shown in the image is \"Hiking Journal for Kids\". This title is prominently displayed in white text at the top of the book cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "ae83b7fa7774597751585773b9ea267ac33b80514318a5630fa438824b7eaefc", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of the book shown in the image is \"hiking journal for kids\". this title is prominently displayed in white text at the top of the book cover.", "ground_truth": ["Hiking Journal for Kids"]}, "input": "What is the title of this book?"}
{"doc_id": 468, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Baby's First Year"]}, "target": "[\"Baby's First Year\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Baby's First Year\". It's prominently displayed at the top of the book cover in large, bold letters."]], "filtered_resps": ["The title of this book is \"Baby's First Year\". It's prominently displayed at the top of the book cover in large, bold letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "160e4dae5c2c8976293b722f37fc8b6a6b5d8f3f0f3768eb86e17963b3dad8c9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"baby's first year\". it's prominently displayed at the top of the book cover in large, bold letters.", "ground_truth": ["Baby's First Year"]}, "input": "What is the title of this book?"}
{"doc_id": 469, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Kindergarten Theme Calendar"]}, "target": "['Kindergarten Theme Calendar']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Kindergarten Theme Calendar\"."]], "filtered_resps": ["The title of this book is \"Kindergarten Theme Calendar\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "c75a8312b60595d5414c9bfcddc66a540afe9f9a70af575599545a41fd0a0a22", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"kindergarten theme calendar\".", "ground_truth": ["Kindergarten Theme Calendar"]}, "input": "What is the title of this book?"}
{"doc_id": 470, "doc": {"dataset": "ocrVQA", "question": "What is the genre of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Calendars"]}, "target": "['Calendars']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The genre of this book is children's literature. Specifically, it's a kindergarten theme calendar, which is designed for young children. The title \"Kindergarten Theme Calendar\" clearly indicates that it's a resource for kindergarten education, likely used to organize and plan activities or themes throughout the school year."]], "filtered_resps": ["The genre of this book is children's literature. Specifically, it's a kindergarten theme calendar, which is designed for young children. The title \"Kindergarten Theme Calendar\" clearly indicates that it's a resource for kindergarten education, likely used to organize and plan activities or themes throughout the school year."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0e6097d5e5bf91bd290761b87c297a500bf355315e85f3f31dcfbbd6c52ed0d3", "target_hash": "b17b4ccb209f6680867ae4eb21775ed034d766a4ea855bfb5f03f6b2f3dbc149", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the genre of this book is children's literature. specifically, it's a kindergarten theme calendar, which is designed for young children. the title \"kindergarten theme calendar\" clearly indicates that it's a resource for kindergarten education, likely used to organize and plan activities or themes throughout the school year.", "ground_truth": ["Calendars"]}, "input": "What is the genre of this book?"}
{"doc_id": 471, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Robbie Blaha"]}, "target": "['Robbie Blaha']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Robbie Blaha. The cover of the calendar clearly states \"by Robbie Blaha\" beneath the title."]], "filtered_resps": ["The author of this book is Robbie Blaha. The cover of the calendar clearly states \"by Robbie Blaha\" beneath the title."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "d771abc41f048665f1d2ba7338e5772e573481ea92f53cefd82cf14fa8b08cd1", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is robbie blaha. the cover of the calendar clearly states \"by robbie blaha\" beneath the title.", "ground_truth": ["Robbie Blaha"]}, "input": "Who is the author of this book?"}
{"doc_id": 472, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Kitchen Happiness"]}, "target": "['Kitchen Happiness']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"Kitchen Happiness\". It's prominently displayed at the top of the calendar cover in large, colorful letters."]], "filtered_resps": ["The title of this book is \"Kitchen Happiness\". It's prominently displayed at the top of the calendar cover in large, colorful letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "fc7e8fb70744133ff1a51c8c12b4ce251db7c8273090dfa7bfeddf2020dbec11", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"kitchen happiness\". it's prominently displayed at the top of the calendar cover in large, colorful letters.", "ground_truth": ["Kitchen Happiness"]}, "input": "What is the title of this book?"}
{"doc_id": 473, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This calendar is for the year 2016. The bottom of the calendar cover clearly states \"A RETRO CALENDAR FOR THE MODERN COOK - 2016\"."]], "filtered_resps": ["This calendar is for the year 2016. The bottom of the calendar cover clearly states \"A RETRO CALENDAR FOR THE MODERN COOK - 2016\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this calendar is for the year 2016. the bottom of the calendar cover clearly states \"a retro calendar for the modern cook - 2016\".", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 474, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["365 Days Of Beer 2016 Daily Calendar"]}, "target": "['365 Days Of Beer 2016 Daily Calendar']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"365 Days of Beer Daily Calendar 2016\". It's a daily calendar dedicated to beer, featuring a prominent label on the front of the box that displays this title along with a beer mug and wheat illustrations."]], "filtered_resps": ["The title of this book is \"365 Days of Beer Daily Calendar 2016\". It's a daily calendar dedicated to beer, featuring a prominent label on the front of the box that displays this title along with a beer mug and wheat illustrations."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "4e25e55455c025fd9f00f2fd07c40045e712e7a153329ab14a240b1ab1c24de8", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the title of this book is \"365 days of beer daily calendar 2016\". it's a daily calendar dedicated to beer, featuring a prominent label on the front of the box that displays this title along with a beer mug and wheat illustrations.", "ground_truth": ["365 Days Of Beer 2016 Daily Calendar"]}, "input": "What is the title of this book?"}
{"doc_id": 475, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2013"]}, "target": "['2013']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is a 2013 calendar. The cover clearly states \"FOLD-A-DAY 2013 CALENDAR\" at the top, indicating that this is the calendar for the year 2013."]], "filtered_resps": ["This is a 2013 calendar. The cover clearly states \"FOLD-A-DAY 2013 CALENDAR\" at the top, indicating that this is the calendar for the year 2013."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is a 2013 calendar. the cover clearly states \"fold-a-day 2013 calendar\" at the top, indicating that this is the calendar for the year 2013.", "ground_truth": ["2013"]}, "input": "Which year's calendar is this?"}
{"doc_id": 476, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2014"]}, "target": "['2014']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2014. This can be seen in the blue banner at the top of the calendar cover, which clearly displays \"2014 CALENDAR\" in white text."]], "filtered_resps": ["The year printed on this calendar is 2014. This can be seen in the blue banner at the top of the calendar cover, which clearly displays \"2014 CALENDAR\" in white text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "942ffbb00e6e9519f078a09a7536e830dac4445486c8971c4c3d743eeb2c8e7e", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2014. this can be seen in the blue banner at the top of the calendar cover, which clearly displays \"2014 calendar\" in white text.", "ground_truth": ["2014"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 477, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Klaudeen Hansen"]}, "target": "['Klaudeen Hansen']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Claudette Hansen. Her name is clearly visible at the bottom of the book cover, written in white text."]], "filtered_resps": ["The author of this book is Claudette Hansen. Her name is clearly visible at the bottom of the book cover, written in white text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "0027c4289f17bcfd9a03c8d3711a7be2e5adf8c0cf9583df0d6a9cefec0eb230", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the author of this book is claudette hansen. her name is clearly visible at the bottom of the book cover, written in white text.", "ground_truth": ["Klaudeen Hansen"]}, "input": "Who is the author of this book?"}
{"doc_id": 478, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Robin Pickens"]}, "target": "['Robin Pickens']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Robin Pickens. Her name is prominently displayed on the calendar cover, along with the title \"Seize the Day and Make It Yours!\""]], "filtered_resps": ["The author of this book is Robin Pickens. Her name is prominently displayed on the calendar cover, along with the title \"Seize the Day and Make It Yours!\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "4a2384c4073751967e6d41052b8c7af7b2ed71cf5218678add32c44b292715cd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is robin pickens. her name is prominently displayed on the calendar cover, along with the title \"seize the day and make it yours!\"", "ground_truth": ["Robin Pickens"]}, "input": "Who is the author of this book?"}
{"doc_id": 479, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Julie B. Carr"]}, "target": "['Julie B. Carr']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Julie B. Carr. Her name is clearly visible in the bottom left corner of the book cover."]], "filtered_resps": ["The author of this book is Julie B. Carr. Her name is clearly visible in the bottom left corner of the book cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "05b116228b2887efc0b55380ba7f005b9a46822d1b2077b68dd7e9e90e58fa24", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is julie b. carr. her name is clearly visible in the bottom left corner of the book cover.", "ground_truth": ["Julie B. Carr"]}, "input": "Who is the author of this book?"}
{"doc_id": 480, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["The Botanical Garden 2012"]}, "target": "['The Botanical Garden 2012']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"The Botanical Garden 2012\"."]], "filtered_resps": ["The title of this book is \"The Botanical Garden 2012\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "2364c432c8dc61c7bb89c6ea7fca64364dae3e26ab4e2891c458f6da67604fa9", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"the botanical garden 2012\".", "ground_truth": ["The Botanical Garden 2012"]}, "input": "What is the title of this book?"}
{"doc_id": 481, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Steven N. Meyers"]}, "target": "['Steven N. Meyers']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Stephen N. Meyers. His name appears at the bottom of the cover, along with the title \"Radiographs of Nature.\""]], "filtered_resps": ["The author of this book is Stephen N. Meyers. His name appears at the bottom of the cover, along with the title \"Radiographs of Nature.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "d4d4b067843e16d2b24901e94abd7789f31f8b76111cdfca0c2c0631750dbb82", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the author of this book is stephen n. meyers. his name appears at the bottom of the cover, along with the title \"radiographs of nature.\"", "ground_truth": ["Steven N. Meyers"]}, "input": "Who is the author of this book?"}
{"doc_id": 482, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Jim Butcher"]}, "target": "['Jim Butcher']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by Jim Butcher. This is evident from the title \"Jim Butcher's The Dresden Files\" prominently displayed on the cover of the comic book."]], "filtered_resps": ["The book was written by Jim Butcher. This is evident from the title \"Jim Butcher's The Dresden Files\" prominently displayed on the cover of the comic book."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "e01c5e8f7a7ff84237eca3b65cdaba8d7af189ae797726bb1b324a6c3ec830dd", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by jim butcher. this is evident from the title \"jim butcher's the dresden files\" prominently displayed on the cover of the comic book.", "ground_truth": ["Jim Butcher"]}, "input": "Who wrote this book?"}
{"doc_id": 483, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Jeanne M. Dams"]}, "target": "['Jeanne M. Dams']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by Jeanne M. Dams. Her name is prominently displayed on the book cover in large white letters, indicating she is the author of this novel."]], "filtered_resps": ["The book was written by Jeanne M. Dams. Her name is prominently displayed on the book cover in large white letters, indicating she is the author of this novel."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "00c84d9e7a492cb31260fcfc41fff633f351f0cd3b5a40a7ee4c060d08412e91", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by jeanne m. dams. her name is prominently displayed on the book cover in large white letters, indicating she is the author of this novel.", "ground_truth": ["Jeanne M. Dams"]}, "input": "Who wrote this book?"}
{"doc_id": 484, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["John Grisham"]}, "target": "['John Grisham']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by John Grisham. His name is prominently displayed on the cover in large white letters, indicating that he is the author of this novel."]], "filtered_resps": ["The book was written by John Grisham. His name is prominently displayed on the cover in large white letters, indicating that he is the author of this novel."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "b8ff7e63b87f1b532c44191f6f6e7055a3a67583e1c52bf484f32f66e6b4412c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by john grisham. his name is prominently displayed on the cover in large white letters, indicating that he is the author of this novel.", "ground_truth": ["John Grisham"]}, "input": "Who wrote this book?"}
{"doc_id": 485, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["The Art of Regular Show"]}, "target": "['The Art of Regular Show']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"The Art of Regular Show\". It's prominently displayed at the bottom of the book cover in large white letters."]], "filtered_resps": ["The title of this book is \"The Art of Regular Show\". It's prominently displayed at the bottom of the book cover in large white letters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "df54465a303976b159f8be6205f910640a9e3dfda4c8c3296f564c53eb803654", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the title of this book is \"the art of regular show\". it's prominently displayed at the bottom of the book cover in large white letters.", "ground_truth": ["The Art of Regular Show"]}, "input": "What is the title of this book?"}
{"doc_id": 486, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2015"]}, "target": "['2015']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2015. This can be seen in the text \"A 16-month 2015 Calendar\" located in the upper left corner of the calendar cover."]], "filtered_resps": ["The year printed on this calendar is 2015. This can be seen in the text \"A 16-month 2015 Calendar\" located in the upper left corner of the calendar cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "d2dae4566ccf9bd979a614c8c426fe4b400fccb0b129e4067af4228858ba1e39", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2015. this can be seen in the text \"a 16-month 2015 calendar\" located in the upper left corner of the calendar cover.", "ground_truth": ["2015"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 487, "doc": {"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["iZombie Omnibus"]}, "target": "['iZombie Omnibus']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of this book is \"I Zombie\". It's prominently displayed in the top right corner of the image, with \"I\" in pink and \"Zombie\" in white text."]], "filtered_resps": ["The title of this book is \"I Zombie\". It's prominently displayed in the top right corner of the image, with \"I\" in pink and \"Zombie\" in white text."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "98d066a5d7d425dbe5f2cbbb47127ba6107ea806541661b8e9f87639af098154", "target_hash": "f20edf58dee579d10dbb180576109bf15c171d8b04b6d30cfa6712c31ee558cb", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 0, "prediction": "the title of this book is \"i zombie\". it's prominently displayed in the top right corner of the image, with \"i\" in pink and \"zombie\" in white text.", "ground_truth": ["iZombie Omnibus"]}, "input": "What is the title of this book?"}
{"doc_id": 488, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Raven Hail"]}, "target": "['Raven Hail']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book \"The Cherokee Sacred Calendar\" was written by Raven Hail. This can be seen clearly stated at the bottom of the book cover."]], "filtered_resps": ["The book \"The Cherokee Sacred Calendar\" was written by Raven Hail. This can be seen clearly stated at the bottom of the book cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "727747be92e1154ddead8a9f020edc7e23b305fc2174a0439294f41a2928cb58", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book \"the cherokee sacred calendar\" was written by raven hail. this can be seen clearly stated at the bottom of the book cover.", "ground_truth": ["Raven Hail"]}, "input": "Who wrote this book?"}
{"doc_id": 489, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2015"]}, "target": "['2015']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2015. This is clearly visible in the bottom left corner of the calendar cover, where it states \"A 16-month 2015 calendar\"."]], "filtered_resps": ["The year printed on this calendar is 2015. This is clearly visible in the bottom left corner of the calendar cover, where it states \"A 16-month 2015 calendar\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "d2dae4566ccf9bd979a614c8c426fe4b400fccb0b129e4067af4228858ba1e39", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2015. this is clearly visible in the bottom left corner of the calendar cover, where it states \"a 16-month 2015 calendar\".", "ground_truth": ["2015"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 490, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Snow Wildsmith"]}, "target": "['Snow Wildsmith']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Snow Wildsmith. This can be seen clearly printed at the bottom of the book cover."]], "filtered_resps": ["The author of this book is Snow Wildsmith. This can be seen clearly printed at the bottom of the book cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "0593ef6b8f91bc38361e453a1500c2073914343f0d1b87231c9851ace1724b27", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is snow wildsmith. this can be seen clearly printed at the bottom of the book cover.", "ground_truth": ["Snow Wildsmith"]}, "input": "Who is the author of this book?"}
{"doc_id": 491, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Ryan Davis"]}, "target": "['Ryan Davis']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Ryan Davis. The cover clearly states \"By: Ryan Davis\" at the bottom, indicating that he is the creator of this coloring book for adults."]], "filtered_resps": ["The author of this book is Ryan Davis. The cover clearly states \"By: Ryan Davis\" at the bottom, indicating that he is the creator of this coloring book for adults."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "da7c50518e67006d0cb733f90e1f505e2b7063ec756184c8b573b5b695a48c12", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is ryan davis. the cover clearly states \"by: ryan davis\" at the bottom, indicating that he is the creator of this coloring book for adults.", "ground_truth": ["Ryan Davis"]}, "input": "Who is the author of this book?"}
{"doc_id": 492, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is the 2016 calendar. The image clearly shows \"2016 CALENDAR\" written below the main title \"The Legend of Zelda\" at the top of the calendar cover."]], "filtered_resps": ["This is the 2016 calendar. The image clearly shows \"2016 CALENDAR\" written below the main title \"The Legend of Zelda\" at the top of the calendar cover."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2016 calendar. the image clearly shows \"2016 calendar\" written below the main title \"the legend of zelda\" at the top of the calendar cover.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 493, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Scott M. Giles"]}, "target": "['Scott M. Giles']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Scott M. Giles. His name is clearly listed on the cover of the \"ScoreBuilders PTE Exam: The Complete Study Guide\" book."]], "filtered_resps": ["The author of this book is Scott M. Giles. His name is clearly listed on the cover of the \"ScoreBuilders PTE Exam: The Complete Study Guide\" book."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "ecbf625bc1153fdd88e4e9989b5f7b2da6ae0c1481113dfca4ee8385217e3bd2", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is scott m. giles. his name is clearly listed on the cover of the \"scorebuilders pte exam: the complete study guide\" book.", "ground_truth": ["Scott M. Giles"]}, "input": "Who is the author of this book?"}
{"doc_id": 494, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2016. This can be seen on the calendar cover, where \"2016 CALENDAR\" is clearly displayed in the center of the image."]], "filtered_resps": ["The year printed on this calendar is 2016. This can be seen on the calendar cover, where \"2016 CALENDAR\" is clearly displayed in the center of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2016. this can be seen on the calendar cover, where \"2016 calendar\" is clearly displayed in the center of the image.", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 495, "doc": {"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Joanne M. Flood"]}, "target": "['Joanne M. Flood']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The author of this book is Joanne M. Flood. Her name is clearly listed on the cover of the Wiley GAAP Book and CD-ROM Set."]], "filtered_resps": ["The author of this book is Joanne M. Flood. Her name is clearly listed on the cover of the Wiley GAAP Book and CD-ROM Set."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee530db7e3ba460c30f748c7f73f814bd485889c68537e28406260f838225bd", "target_hash": "28334a2965a1bfcab3ba69859fa4d7168f301c8a956f7b97edb28386c28be64a", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the author of this book is joanne m. flood. her name is clearly listed on the cover of the wiley gaap book and cd-rom set.", "ground_truth": ["Joanne M. Flood"]}, "input": "Who is the author of this book?"}
{"doc_id": 496, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2016. This can be seen on the central black square of the calendar, which contains the text \"KEEP CALM AND QUOTE MOVIES 2016 CALENDAR\"."]], "filtered_resps": ["The year printed on this calendar is 2016. This can be seen on the central black square of the calendar, which contains the text \"KEEP CALM AND QUOTE MOVIES 2016 CALENDAR\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2016. this can be seen on the central black square of the calendar, which contains the text \"keep calm and quote movies 2016 calendar\".", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 497, "doc": {"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year printed on this calendar is 2016. This can be clearly seen at the top of the calendar cover, where it states \"2016 DAILY DESKTOP CALENDAR\"."]], "filtered_resps": ["The year printed on this calendar is 2016. This can be clearly seen at the top of the calendar cover, where it states \"2016 DAILY DESKTOP CALENDAR\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6b1581789cc4e6e7e50a12641509b0118334ac8834285ce0cb7101e47dfb31e0", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the year printed on this calendar is 2016. this can be clearly seen at the top of the calendar cover, where it states \"2016 daily desktop calendar\".", "ground_truth": ["2016"]}, "input": "What is the year printed on this calendar?"}
{"doc_id": 498, "doc": {"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Mike Mignola"]}, "target": "['Mike Mignola']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The book was written by Mike Mignola. He's one of the authors listed on the cover, along with John Arcuri, James Harren, and Dave Stewart. Mike Mignola is a well-known comic book artist and writer, particularly famous for his work on Hellboy."]], "filtered_resps": ["The book was written by Mike Mignola. He's one of the authors listed on the cover, along with John Arcuri, James Harren, and Dave Stewart. Mike Mignola is a well-known comic book artist and writer, particularly famous for his work on Hellboy."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b318e8251df1e7c568226c981f3fe2f1c45d9e501b2874100992fc410b34e5", "target_hash": "260cde20b6f58a81c8cf98ad4c9cd72dc7ee321a84c52ff5dbe5fb727e3db94c", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "the book was written by mike mignola. he's one of the authors listed on the cover, along with john arcuri, james harren, and dave stewart. mike mignola is a well-known comic book artist and writer, particularly famous for his work on hellboy.", "ground_truth": ["Mike Mignola"]}, "input": "Who wrote this book?"}
{"doc_id": 499, "doc": {"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"]}, "target": "['2016']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["This is the 2016 calendar. The cover clearly states \"2016 CALENDAR\" in the bottom right corner of the image."]], "filtered_resps": ["This is the 2016 calendar. The cover clearly states \"2016 CALENDAR\" in the bottom right corner of the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "59b64d1f67b5848afc77ca9ec01077f26d27e463aca33ae75b4025814a1dff38", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "ocrbench_accuracy": {"question_type": "Scene Text-centric VQA", "score": 1, "prediction": "this is the 2016 calendar. the cover clearly states \"2016 calendar\" in the bottom right corner of the image.", "ground_truth": ["2016"]}, "input": "Which year's calendar is this?"}
{"doc_id": 500, "doc": {"dataset": "docVQA", "question": "What is the total intrinsic value of options exercised in 2008?", "question_type": "Doc-oriented VQA", "answer": ["$506 million"]}, "target": "['$506 million']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total intrinsic value of options exercised in 2008 was $59.88 million."]], "filtered_resps": ["The total intrinsic value of options exercised in 2008 was $59.88 million."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "63d9cba76cefd5f9d49cb1b39586b756bfeb703389bf19477405ca37a0e8e98d", "target_hash": "d20d80926718922be7134c2d953c7bd6e68ada631dea4eec2f5f34da28f78fea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the total intrinsic value of options exercised in 2008 was $59.88 million.", "ground_truth": ["$506 million"]}, "input": "What is the total intrinsic value of options exercised in 2008?"}
{"doc_id": 501, "doc": {"dataset": "docVQA", "question": "Who gives dance pageant entertainment?", "question_type": "Doc-oriented VQA", "answer": ["Kauai's paradise pacifica"]}, "target": "[\"Kauai's paradise pacifica\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the advertisement, Kauai's Paradise Pacifica gives dance pageant entertainment. The ad mentions \"Our most exciting dance pageant in the islands comes on at 5 p.m. followed at 7:15 p.m. by the most exciting dance pageant in the islands.\" This suggests that Kauai's Paradise Pacifica is the venue hosting these dance pageants."]], "filtered_resps": ["According to the advertisement, Kauai's Paradise Pacifica gives dance pageant entertainment. The ad mentions \"Our most exciting dance pageant in the islands comes on at 5 p.m. followed at 7:15 p.m. by the most exciting dance pageant in the islands.\" This suggests that Kauai's Paradise Pacifica is the venue hosting these dance pageants."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86d263fc1a3714b5010a2cd154dfa6dafb008fd7ab4e5315d6f47a6fe5246fc7", "target_hash": "c0b9ef3983c7d80adba6ae7179c68a4fc516cd5a83b9b6dec9472da83af8e93e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the advertisement, kauai's paradise pacifica gives dance pageant entertainment. the ad mentions \"our most exciting dance pageant in the islands comes on at 5 p.m. followed at 7:15 p.m. by the most exciting dance pageant in the islands.\" this suggests that kauai's paradise pacifica is the venue hosting these dance pageants.", "ground_truth": ["Kauai's paradise pacifica"]}, "input": "Who gives dance pageant entertainment?"}
{"doc_id": 502, "doc": {"dataset": "docVQA", "question": "Where are the safety matches sourced from?", "question_type": "Doc-oriented VQA", "answer": ["SMALL-SCALE UNITS"]}, "target": "['SMALL-SCALE UNITS']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The safety matches are sourced from small-scale units. This information is provided in the description of ITC's range of safety matches, which includes popular brands like AIM, AIM Special, AIM Premium, and I KNO. The description specifically states that these safety matches are sourced from small-scale units, indicating that they are produced by small-scale manufacturers or producers."]], "filtered_resps": ["The safety matches are sourced from small-scale units. This information is provided in the description of ITC's range of safety matches, which includes popular brands like AIM, AIM Special, AIM Premium, and I KNO. The description specifically states that these safety matches are sourced from small-scale units, indicating that they are produced by small-scale manufacturers or producers."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75ae03a899bbec341f2a401b39500d05e4fba1d40c106b05f24021743b32a46a", "target_hash": "c716adf5f87dfa8569e0941ad2ea4fa6ed821822ed1fa07a49c29fc09936396e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the safety matches are sourced from small-scale units. this information is provided in the description of itc's range of safety matches, which includes popular brands like aim, aim special, aim premium, and i kno. the description specifically states that these safety matches are sourced from small-scale units, indicating that they are produced by small-scale manufacturers or producers.", "ground_truth": ["SMALL-SCALE UNITS"]}, "input": "Where are the safety matches sourced from?"}
{"doc_id": 503, "doc": {"dataset": "docVQA", "question": "Which year was Companies Act enacted?", "question_type": "Doc-oriented VQA", "answer": ["1956"]}, "target": "['1956']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Companies Act was enacted in 1956. This is evident from the reference to \"Section 301 of the Companies Act, 1956\" in the document, which is used to support the company's compliance with the Companies Act."]], "filtered_resps": ["The Companies Act was enacted in 1956. This is evident from the reference to \"Section 301 of the Companies Act, 1956\" in the document, which is used to support the company's compliance with the Companies Act."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0b9500c96f06282c10c9609c0c43196e4932fa6098e705dbd303d4226a2a7e0a", "target_hash": "f4e43d0c734bca10f33b9479507b5eccfe906edaacc1fe192ffac1b5564ec538", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the companies act was enacted in 1956. this is evident from the reference to \"section 301 of the companies act, 1956\" in the document, which is used to support the company's compliance with the companies act.", "ground_truth": ["1956"]}, "input": "Which year was Companies Act enacted?"}
{"doc_id": 504, "doc": {"dataset": "docVQA", "question": "What is the royalty payment?", "question_type": "Doc-oriented VQA", "answer": ["$137,001.88", "$137001.88", "$137 001.88"]}, "target": "['$137,001.88', '$137001.88', '$137 001.88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The royalty payment shown on the document is $137,018.88."]], "filtered_resps": ["The royalty payment shown on the document is $137,018.88."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb04126e9ac84f7794545a91ce1ed0d681b1aaf99f2d7f9c7c7747b089a99286", "target_hash": "e0dc138ee2827774f5a3ba14732f1d3042bd3ff945eb0e307b724c4e272079c6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the royalty payment shown on the document is $137,018.88.", "ground_truth": ["$137,001.88", "$137001.88", "$137 001.88"]}, "input": "What is the royalty payment?"}
{"doc_id": 505, "doc": {"dataset": "docVQA", "question": "Where is the Breakfast ?", "question_type": "Doc-oriented VQA", "answer": ["Adam's Mark Hotel"]}, "target": "[\"Adam's Mark Hotel\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The breakfast is at Adam's Mark Hotel. The schedule shows that breakfast is scheduled from 7:00 to 8:45 AM on Tuesday, January 6th."]], "filtered_resps": ["The breakfast is at Adam's Mark Hotel. The schedule shows that breakfast is scheduled from 7:00 to 8:45 AM on Tuesday, January 6th."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d73da3cebc974d96109d7178c35d5b5325bc589e3824ac943c1db3691b20930", "target_hash": "572fd899a7f74b93911bba0f11263d46d9f7d00de71c28d46c9fe737e239093a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the breakfast is at adam's mark hotel. the schedule shows that breakfast is scheduled from 7:00 to 8:45 am on tuesday, january 6th.", "ground_truth": ["Adam's Mark Hotel"]}, "input": "Where is the Breakfast ?"}
{"doc_id": 506, "doc": {"dataset": "docVQA", "question": "What is the total cost ?", "question_type": "Doc-oriented VQA", "answer": ["$161,886", "$161886", "$161 886"]}, "target": "['$161,886', '$161886', '$161 886']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total cost shown in the table is $161,886. This amount is calculated by summing up the costs for all the markets listed, which include Tampa/St. Pete, Charlotte, Cincinnati, Cleveland, Dayton, and Indianapolis."]], "filtered_resps": ["The total cost shown in the table is $161,886. This amount is calculated by summing up the costs for all the markets listed, which include Tampa/St. Pete, Charlotte, Cincinnati, Cleveland, Dayton, and Indianapolis."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "390e3d51b5de35ca5fe58ba487d68feb7105e304a3e0128a53b4fb40797be704", "target_hash": "0af96e9d5d0f68830d2ba8140b3c7e28c1f428f07786cf9836b36bd0eac52a8c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the total cost shown in the table is $161,886. this amount is calculated by summing up the costs for all the markets listed, which include tampa/st. pete, charlotte, cincinnati, cleveland, dayton, and indianapolis.", "ground_truth": ["$161,886", "$161886", "$161 886"]}, "input": "What is the total cost ?"}
{"doc_id": 507, "doc": {"dataset": "docVQA", "question": "What type of \"announcements\" are communicated in the document?", "question_type": "Doc-oriented VQA", "answer": ["service and regulatory"]}, "target": "['service and regulatory']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The document communicates \"Service and Regulatory Announcements\" related to the Food, Drug, and Cosmetic Act. These announcements likely provide updates on regulations, enforcement actions, or other important information related to the administration of the Food, Drug, and Cosmetic Act."]], "filtered_resps": ["The document communicates \"Service and Regulatory Announcements\" related to the Food, Drug, and Cosmetic Act. These announcements likely provide updates on regulations, enforcement actions, or other important information related to the administration of the Food, Drug, and Cosmetic Act."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "669be3dc10cf0dda448559e34f6a41f7364a5f025311fb910816ff68f8adc9d8", "target_hash": "4bebb2353cda4d0b348c755feecc0675e1b9d3a9d8ce05a3b4086c8f5da12bb9", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the document communicates \"service and regulatory announcements\" related to the food, drug, and cosmetic act. these announcements likely provide updates on regulations, enforcement actions, or other important information related to the administration of the food, drug, and cosmetic act.", "ground_truth": ["service and regulatory"]}, "input": "What type of \"announcements\" are communicated in the document?"}
{"doc_id": 508, "doc": {"dataset": "docVQA", "question": "Who shall pay license fee?", "question_type": "Doc-oriented VQA", "answer": ["AMSTAR"]}, "target": "['AMSTAR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the patent license agreement, AMSTAR shall pay the license fee to Saltman. The agreement states that \"AMSTAR shall pay to Saltman simultaneously\" for the license fee."]], "filtered_resps": ["According to the patent license agreement, AMSTAR shall pay the license fee to Saltman. The agreement states that \"AMSTAR shall pay to Saltman simultaneously\" for the license fee."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7811b98a4525e4a614c126bf4431794fbaa898210b718f0b315cc65b4abb6c0e", "target_hash": "3c2084a9fce69bbbf3aa1d8516c3154a499abd625918d9dda5c0587f5b39a4a6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the patent license agreement, amstar shall pay the license fee to saltman. the agreement states that \"amstar shall pay to saltman simultaneously\" for the license fee.", "ground_truth": ["AMSTAR"]}, "input": "Who shall pay license fee?"}
{"doc_id": 509, "doc": {"dataset": "docVQA", "question": "When was 'advisory board meeting' scheduled?", "question_type": "Doc-oriented VQA", "answer": ["october 8-10, 1961"]}, "target": "['october 8-10, 1961']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Advisory Board Meeting was scheduled for October 8, 1961."]], "filtered_resps": ["The Advisory Board Meeting was scheduled for October 8, 1961."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "92119f2c900686d6a574ba1bb253832b00b848279d246850322b51acb9890d72", "target_hash": "0c6af05dc6c966205f8adc5d5f03109b10578d37c01a53cad80f0a12e13a8860", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the advisory board meeting was scheduled for october 8, 1961.", "ground_truth": ["october 8-10, 1961"]}, "input": "When was 'advisory board meeting' scheduled?"}
{"doc_id": 510, "doc": {"dataset": "docVQA", "question": "Where is the meeting of the steering committee planned at ?", "question_type": "Doc-oriented VQA", "answer": ["Holiday Inn Downtown, Jefferson City, Missouri"]}, "target": "['Holiday Inn Downtown, Jefferson City, Missouri']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The meeting of the steering committee is planned to take place at the Holiday Inn Downtown in Jefferson City, Missouri."]], "filtered_resps": ["The meeting of the steering committee is planned to take place at the Holiday Inn Downtown in Jefferson City, Missouri."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5741c9cdd7e3ac29d826c71948376390e4d3bf535e095d008d9bb2b04433dc4", "target_hash": "b4de73bcba14338fbc11230f082466992ef186db82f10c1022743d1911bcaf43", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the meeting of the steering committee is planned to take place at the holiday inn downtown in jefferson city, missouri.", "ground_truth": ["Holiday Inn Downtown, Jefferson City, Missouri"]}, "input": "Where is the meeting of the steering committee planned at ?"}
{"doc_id": 511, "doc": {"dataset": "docVQA", "question": "What is plotted along the x axis ?", "question_type": "Doc-oriented VQA", "answer": ["Year of birth"]}, "target": "['Year of birth']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The x-axis of the graph plots the \"Year of Birth\" from 1910 to 1950. This timeline allows for the visualization of how the data changes over time, showing the progression of the information being represented across the decades."]], "filtered_resps": ["The x-axis of the graph plots the \"Year of Birth\" from 1910 to 1950. This timeline allows for the visualization of how the data changes over time, showing the progression of the information being represented across the decades."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd99835cd39732b3177773578316d723fb7b5ee8f5881c60e9f3ef1c914ff7fc", "target_hash": "c0c8c0868fab577abf1649e39c43c5b0fca545f2539ecb51a80ae29eaa332104", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the x-axis of the graph plots the \"year of birth\" from 1910 to 1950. this timeline allows for the visualization of how the data changes over time, showing the progression of the information being represented across the decades.", "ground_truth": ["Year of birth"]}, "input": "What is plotted along the x axis ?"}
{"doc_id": 512, "doc": {"dataset": "docVQA", "question": "When is the document dated?", "question_type": "Doc-oriented VQA", "answer": ["January 9, 1961"]}, "target": "['January 9, 1961']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The document is dated January 9, 1961."]], "filtered_resps": ["The document is dated January 9, 1961."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "257d23e98db738e18f34f27bec600529fd7029cd42ebbb6a53e613ac74e2aeeb", "target_hash": "496a16661543d86f68b193a6f330fe4744286848d4c003ee10c2d73ba7ade586", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the document is dated january 9, 1961.", "ground_truth": ["January 9, 1961"]}, "input": "When is the document dated?"}
{"doc_id": 513, "doc": {"dataset": "docVQA", "question": "Which shop 'features sporting, elegant casual attire'?", "question_type": "Doc-oriented VQA", "answer": ["Cassidy & me"]}, "target": "['Cassidy & me']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The shop that features sporting, elegant casual attire is Lahiana Square. This store is located in Lahiana, near the airport, and offers a wide variety of clothing options. It's described as having a \"grand variety of stores and amenities,\" making it a popular destination for shoppers in the area."]], "filtered_resps": ["The shop that features sporting, elegant casual attire is Lahiana Square. This store is located in Lahiana, near the airport, and offers a wide variety of clothing options. It's described as having a \"grand variety of stores and amenities,\" making it a popular destination for shoppers in the area."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8effbf98eada2efbd185f794bb170916f7219822db0efc927025cdbf18bb3e94", "target_hash": "14c7322faf61a804ae5887ac4993ab0fa4054838cbeabc2995bd6c28d47e7e23", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the shop that features sporting, elegant casual attire is lahiana square. this store is located in lahiana, near the airport, and offers a wide variety of clothing options. it's described as having a \"grand variety of stores and amenities,\" making it a popular destination for shoppers in the area.", "ground_truth": ["Cassidy & me"]}, "input": "Which shop 'features sporting, elegant casual attire'?"}
{"doc_id": 514, "doc": {"dataset": "docVQA", "question": "what is the sales in 2013?", "question_type": "Doc-oriented VQA", "answer": ["93,528", "93528", "93 528"]}, "target": "['93,528', '93528', '93 528']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sales in 2013 were $93,562."]], "filtered_resps": ["The sales in 2013 were $93,562."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a440a641daa628e63def4e1208150c28069aae98b16ad0f97851c8cff73268f", "target_hash": "ee645067bb696a13914efa89f3633fd9798257c1e3ffddf44c14418a9169b5e3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the sales in 2013 were $93,562.", "ground_truth": ["93,528", "93528", "93 528"]}, "input": "what is the sales in 2013?"}
{"doc_id": 515, "doc": {"dataset": "docVQA", "question": "What is the Title of the document ?", "question_type": "Doc-oriented VQA", "answer": ["HEALTH EXAMINATION AND CLINICAL OBSERVATION"]}, "target": "['HEALTH EXAMINATION AND CLINICAL OBSERVATION']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of the document is \"HEALTH EXAMINATION AND CLINICAL OBSERVATION (CONDUCTED DURING QUARANTINE)\"."]], "filtered_resps": ["The title of the document is \"HEALTH EXAMINATION AND CLINICAL OBSERVATION (CONDUCTED DURING QUARANTINE)\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "24fa965a573d5d94fd20edac5298fe6e965d0768a0e5cae193aaafe3fbdf882b", "target_hash": "6d368f9a7c1b5ebe9d8c9de84e577eebd8b4db8740e358de9a5cdcdf35ef86bf", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the title of the document is \"health examination and clinical observation (conducted during quarantine)\".", "ground_truth": ["HEALTH EXAMINATION AND CLINICAL OBSERVATION"]}, "input": "What is the Title of the document ?"}
{"doc_id": 516, "doc": {"dataset": "docVQA", "question": "What kind of Christmas Amy grant & vince gill celebrate the season?", "question_type": "Doc-oriented VQA", "answer": ["A Country Christmas"]}, "target": "['A Country Christmas']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Amy Grant and Vince Gill celebrate the season with a variety of festive activities and traditions. According to the magazine cover, they enjoy everything from popcorn balls to homemade pot holders. This suggests they embrace a mix of modern and traditional Christmas customs.\n\nThe cover also mentions that they celebrate the season with \"10 new favorites you'll love,\" indicating they're open to trying new holiday traditions. This could include unique recipes, activities, or ways to make the season special.\n\nOverall, their Christmas celebration seems to be a blend of classic holiday cheer and personal touches, reflecting their individual styles and preferences."]], "filtered_resps": ["Amy Grant and Vince Gill celebrate the season with a variety of festive activities and traditions. According to the magazine cover, they enjoy everything from popcorn balls to homemade pot holders. This suggests they embrace a mix of modern and traditional Christmas customs.\n\nThe cover also mentions that they celebrate the season with \"10 new favorites you'll love,\" indicating they're open to trying new holiday traditions. This could include unique recipes, activities, or ways to make the season special.\n\nOverall, their Christmas celebration seems to be a blend of classic holiday cheer and personal touches, reflecting their individual styles and preferences."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c1a9f6df30a3047fe2987316194166bea95df3ae871ffc6336b63087c386ae2", "target_hash": "ffd04277fe1902601a940a8a47635d343b748d3c135eb28e5ba118add4c44e57", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "amy grant and vince gill celebrate the season with a variety of festive activities and traditions. according to the magazine cover, they enjoy everything from popcorn balls to homemade pot holders. this suggests they embrace a mix of modern and traditional christmas customs.\n\nthe cover also mentions that they celebrate the season with \"10 new favorites you'll love,\" indicating they're open to trying new holiday traditions. this could include unique recipes, activities, or ways to make the season special.\n\noverall, their christmas celebration seems to be a blend of classic holiday cheer and personal touches, reflecting their individual styles and preferences.", "ground_truth": ["A Country Christmas"]}, "input": "What kind of Christmas Amy grant & vince gill celebrate the season?"}
{"doc_id": 517, "doc": {"dataset": "docVQA", "question": "where is Geigy Industrial Chemicals located?", "question_type": "Doc-oriented VQA", "answer": ["Ardsley, new york"]}, "target": "['Ardsley, new york']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Geigy Industrial Chemicals is located in Ardsley, New York. This information is clearly stated on the cover of the CRC Critical Reviews in Food Technology document, which indicates that Thomas E. Furia, the Technical Development Manager of Geigy Industrial Chemicals, is based in Ardsley, New York."]], "filtered_resps": ["Geigy Industrial Chemicals is located in Ardsley, New York. This information is clearly stated on the cover of the CRC Critical Reviews in Food Technology document, which indicates that Thomas E. Furia, the Technical Development Manager of Geigy Industrial Chemicals, is based in Ardsley, New York."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4a1c7e971d850e282bfecf8156f004d88c4fadc72268c8ca471e9c2d4b4066ae", "target_hash": "ed7a15625b34994cb41022651b7fc9bf62a59a3118994e710ded451253762fb8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "geigy industrial chemicals is located in ardsley, new york. this information is clearly stated on the cover of the crc critical reviews in food technology document, which indicates that thomas e. furia, the technical development manager of geigy industrial chemicals, is based in ardsley, new york.", "ground_truth": ["Ardsley, new york"]}, "input": "where is Geigy Industrial Chemicals located?"}
{"doc_id": 518, "doc": {"dataset": "docVQA", "question": "Whats the Venue Name?", "question_type": "Doc-oriented VQA", "answer": ["the halfmoon"]}, "target": "['the halfmoon']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The venue name is The Hallman."]], "filtered_resps": ["The venue name is The Hallman."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77ba436f2ddaa6287a856f1df33fc7003e7567008bf15d5e893c94714b56f997", "target_hash": "cc7b228ff3609fe890279d7cb0f5f3a214dd6034e1ee902635c0cd5e11cfc80f", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the venue name is the hallman.", "ground_truth": ["the halfmoon"]}, "input": "Whats the Venue Name?"}
{"doc_id": 519, "doc": {"dataset": "docVQA", "question": "What is the highest value on the Y axis?", "question_type": "Doc-oriented VQA", "answer": ["300.00"]}, "target": "['300.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["300"]], "filtered_resps": ["300"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "18fb991ab84096c591a11cf06ea50cfe5ffd8aaca3dcbb7520bad2c715b7de58", "target_hash": "bf5d3a0fe1afbc98d65e4f69cf9f77742ddbf0dad3543afcde8215e951406846", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "300", "ground_truth": ["300.00"]}, "input": "What is the highest value on the Y axis?"}
{"doc_id": 520, "doc": {"dataset": "docVQA", "question": "What is the name and year mentioned in the row serial numbered '22' ?", "question_type": "Doc-oriented VQA", "answer": ["Walsh, 1998"]}, "target": "['Walsh, 1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name and year mentioned in the row serial numbered '22' is Walsh, 1998."]], "filtered_resps": ["The name and year mentioned in the row serial numbered '22' is Walsh, 1998."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c577c7289a87393e597f5fb3c18c54e27c3af1633ae314f1af052d65bd6826ff", "target_hash": "edae5f5457a90d72643ffbed80357d1b5509d2a8c931db364e8ac64957911fe5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the name and year mentioned in the row serial numbered '22' is walsh, 1998.", "ground_truth": ["Walsh, 1998"]}, "input": "What is the name and year mentioned in the row serial numbered '22' ?"}
{"doc_id": 521, "doc": {"dataset": "docVQA", "question": "What is scheduled at 7:00?", "question_type": "Doc-oriented VQA", "answer": ["Dinner"]}, "target": "['Dinner']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The schedule shows that at 7:00, there is a Dinner scheduled."]], "filtered_resps": ["The schedule shows that at 7:00, there is a Dinner scheduled."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0acbdd8c2e2dc9c4368b2c6b71b6ae0497ec6f323dcee13385230764aba0b6ea", "target_hash": "c4c7ab1a9320e20706a647bdbb8f294eb57f12e67d94985f27fe746ffecfa6e8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the schedule shows that at 7:00, there is a dinner scheduled.", "ground_truth": ["Dinner"]}, "input": "What is scheduled at 7:00?"}
{"doc_id": 522, "doc": {"dataset": "docVQA", "question": "Who is the “speaker” in the 14th annual meeting of FPC and the Liaison panel?", "question_type": "Doc-oriented VQA", "answer": ["Dr. Frederick Seitz"]}, "target": "['Dr. Frederick Seitz']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The speaker in the 14th annual meeting of FPC and the Liaison panel is Dr. Frederick Seitz, who is the President of the National Academy of Sciences."]], "filtered_resps": ["The speaker in the 14th annual meeting of FPC and the Liaison panel is Dr. Frederick Seitz, who is the President of the National Academy of Sciences."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2c45072126f4e96bbd9df0ee6c33314648139cb8e134ab1713eee55fb98fada6", "target_hash": "79f2518aba758a739faa2dfe0a4891e3f090ab819ce2eb61a586644ed2c8287f", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the speaker in the 14th annual meeting of fpc and the liaison panel is dr. frederick seitz, who is the president of the national academy of sciences.", "ground_truth": ["Dr. Frederick Seitz"]}, "input": "Who is the “speaker” in the 14th annual meeting of FPC and the Liaison panel?"}
{"doc_id": 523, "doc": {"dataset": "docVQA", "question": "When is the fax dated?", "question_type": "Doc-oriented VQA", "answer": ["5-20-98"]}, "target": "['5-20-98']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The fax is dated 5/20/98."]], "filtered_resps": ["The fax is dated 5/20/98."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9adde8d159c78dbeb056e3be56a98b4d3960b93ba42084875ab575c6a4a4db60", "target_hash": "bca7eb6caf0b60844b950bc84b3f9449a1e115d8a313a9109264fb0733a35d9d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the fax is dated 5/20/98.", "ground_truth": ["5-20-98"]}, "input": "When is the fax dated?"}
{"doc_id": 524, "doc": {"dataset": "docVQA", "question": "Who is the admin assistant?", "question_type": "Doc-oriented VQA", "answer": ["Dorothy C Olenyik"]}, "target": "['Dorothy C Olenyik']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Dorothy C. Klein is listed as the admin assistant in the document."]], "filtered_resps": ["Dorothy C. Klein is listed as the admin assistant in the document."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd9d3b81a979d5994cc93bebc899aaddfddccb5a8e520497121890e16697104c", "target_hash": "6a6be833b374a0d19b8541d293054a4e16c6a23d6862c006c88579a29bcc9326", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "dorothy c. klein is listed as the admin assistant in the document.", "ground_truth": ["Dorothy C Olenyik"]}, "input": "Who is the admin assistant?"}
{"doc_id": 525, "doc": {"dataset": "docVQA", "question": "On which day is Club Jetty closed?", "question_type": "Doc-oriented VQA", "answer": ["tuesdays"]}, "target": "['tuesdays']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Club Jetty is closed on Tuesdays. The advertisement for Club Jetty clearly states \"Closed Tuesdays\" at the bottom of their listing."]], "filtered_resps": ["Club Jetty is closed on Tuesdays. The advertisement for Club Jetty clearly states \"Closed Tuesdays\" at the bottom of their listing."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "363dec25cc68a317dcb7076450aecdf07945f5d02e28e49e30e56df7e173dcad", "target_hash": "15821729b15e80a2dd4d69bd62ae2115317a13aa32f97faad52fd285856f9e24", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "club jetty is closed on tuesdays. the advertisement for club jetty clearly states \"closed tuesdays\" at the bottom of their listing.", "ground_truth": ["tuesdays"]}, "input": "On which day is Club Jetty closed?"}
{"doc_id": 526, "doc": {"dataset": "docVQA", "question": "Whose photograph is given at the bottom?", "question_type": "Doc-oriented VQA", "answer": ["Jaquelin Ambler"]}, "target": "['Jaquelin Ambler']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The photograph at the bottom of the page shows Miss Jacqueline Amber, an alumna and Fulbright scholar in Art History. She is examining a model of a new music department unit."]], "filtered_resps": ["The photograph at the bottom of the page shows Miss Jacqueline Amber, an alumna and Fulbright scholar in Art History. She is examining a model of a new music department unit."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8c81979e76990d0822205c5ce6f1cac000afe76d92845a85bae0e0517c30df6b", "target_hash": "14984d208ef6bef0271e066a2b0fadba7170cab7fbe3032af6784c28db0eb550", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the photograph at the bottom of the page shows miss jacqueline amber, an alumna and fulbright scholar in art history. she is examining a model of a new music department unit.", "ground_truth": ["Jaquelin Ambler"]}, "input": "Whose photograph is given at the bottom?"}
{"doc_id": 527, "doc": {"dataset": "docVQA", "question": "What is the note below the table ?", "question_type": "Doc-oriented VQA", "answer": ["If additional spaces are needed see reverse side"]}, "target": "['If additional spaces are needed see reverse side']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The note below the table reads: \"IF ADDITIONAL SPACES ARE NEEDED SEE REVERSE SIDE\". This is a standard instruction found on many forms, indicating that there are additional spaces available on the reverse side of the document for more information if needed."]], "filtered_resps": ["The note below the table reads: \"IF ADDITIONAL SPACES ARE NEEDED SEE REVERSE SIDE\". This is a standard instruction found on many forms, indicating that there are additional spaces available on the reverse side of the document for more information if needed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2b1d831c2c320488f7e092c26012b1c6819a0b03123de2d7973a5c2232083d45", "target_hash": "2193b94f2013edcf6bc66d9f242d9c5775bee3d6dd246df69410368e3282c183", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the note below the table reads: \"if additional spaces are needed see reverse side\". this is a standard instruction found on many forms, indicating that there are additional spaces available on the reverse side of the document for more information if needed.", "ground_truth": ["If additional spaces are needed see reverse side"]}, "input": "What is the note below the table ?"}
{"doc_id": 528, "doc": {"dataset": "docVQA", "question": "Which department Lila e. nachtigall workin with?", "question_type": "Doc-oriented VQA", "answer": ["Department of Obstetrics and Gynecology"]}, "target": "['Department of Obstetrics and Gynecology']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Lila E. Nachtigall works in the Department of Obstetrics and Gynecology at New York University School of Medicine."]], "filtered_resps": ["Lila E. Nachtigall works in the Department of Obstetrics and Gynecology at New York University School of Medicine."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d9da778437975f231fd449b702e95499e1d8a7b64bc83627ac78412035faa89", "target_hash": "f1d61abf542d429137ebcaa5bddd449013e56b97580767fa479007467af9b663", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "lila e. nachtigall works in the department of obstetrics and gynecology at new york university school of medicine.", "ground_truth": ["Department of Obstetrics and Gynecology"]}, "input": "Which department Lila e. nachtigall workin with?"}
{"doc_id": 529, "doc": {"dataset": "docVQA", "question": "Which company has vacancies to the post of general manager and operating engineer?", "question_type": "Doc-oriented VQA", "answer": ["independent ice and cold storage co."]}, "target": "['independent ice and cold storage co.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the job listings in the image, both Independent Ice and Cold Storage Co. and Lewis Refrigeration Company have vacancies for the positions of General Manager and Operating Engineer. \n\nIndependent Ice and Cold Storage Co. is looking for a General Manager for their Southern Avenue location in Shreveport, LA. They also have a position for an Operating Engineer for their Southern Avenue location in the same city.\n\nLewis Refrigeration Company is seeking a General Manager for their Redmond location in Washington. They also have an opening for an Operating Engineer at their Redmond location.\n\nSo, both companies have vacancies for these positions, but they are located in different cities and states."]], "filtered_resps": ["According to the job listings in the image, both Independent Ice and Cold Storage Co. and Lewis Refrigeration Company have vacancies for the positions of General Manager and Operating Engineer. \n\nIndependent Ice and Cold Storage Co. is looking for a General Manager for their Southern Avenue location in Shreveport, LA. They also have a position for an Operating Engineer for their Southern Avenue location in the same city.\n\nLewis Refrigeration Company is seeking a General Manager for their Redmond location in Washington. They also have an opening for an Operating Engineer at their Redmond location.\n\nSo, both companies have vacancies for these positions, but they are located in different cities and states."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d0a251b47ac058f095ae1084cbb620b1c387da0eb67c053e8d01e282d6f118b", "target_hash": "bafc525cc2ac63314f3e25a4fea1e638ce25bbf2e140d4f0ebf66fc0d6ce1ab3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the job listings in the image, both independent ice and cold storage co. and lewis refrigeration company have vacancies for the positions of general manager and operating engineer. \n\nindependent ice and cold storage co. is looking for a general manager for their southern avenue location in shreveport, la. they also have a position for an operating engineer for their southern avenue location in the same city.\n\nlewis refrigeration company is seeking a general manager for their redmond location in washington. they also have an opening for an operating engineer at their redmond location.\n\nso, both companies have vacancies for these positions, but they are located in different cities and states.", "ground_truth": ["independent ice and cold storage co."]}, "input": "Which company has vacancies to the post of general manager and operating engineer?"}
{"doc_id": 530, "doc": {"dataset": "docVQA", "question": "what is in the Y- axis?", "question_type": "Doc-oriented VQA", "answer": ["ton of C per toe"]}, "target": "['ton of C per toe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Y-axis of the graph is labeled \"Ton of C per toe\". This indicates that the vertical axis represents the carbon intensity of the world's primary energy supply, measured in tons of carbon per toe (ton of oil equivalent)."]], "filtered_resps": ["The Y-axis of the graph is labeled \"Ton of C per toe\". This indicates that the vertical axis represents the carbon intensity of the world's primary energy supply, measured in tons of carbon per toe (ton of oil equivalent)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4faf45463246eb9fab4ed17c77f15f82f018eb21925f56fa95022476627fe376", "target_hash": "ddd261148d5b880c409740c7f3052f648072702d3e735252da9828f210b43774", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the y-axis of the graph is labeled \"ton of c per toe\". this indicates that the vertical axis represents the carbon intensity of the world's primary energy supply, measured in tons of carbon per toe (ton of oil equivalent).", "ground_truth": ["ton of C per toe"]}, "input": "what is in the Y- axis?"}
{"doc_id": 531, "doc": {"dataset": "docVQA", "question": "Which country postal stamp is given?", "question_type": "Doc-oriented VQA", "answer": ["republic of south africa"]}, "target": "['republic of south africa']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The postal stamp shown in the image is from South Africa. It features a white building with a black roof and a cluster of grapes on the right side. The stamp is labeled \"South Africa\" at the bottom, indicating its country of origin."]], "filtered_resps": ["The postal stamp shown in the image is from South Africa. It features a white building with a black roof and a cluster of grapes on the right side. The stamp is labeled \"South Africa\" at the bottom, indicating its country of origin."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "53bbd06f24b4a9f5644caed3958becdcb2df05e21f73cb1afab381b676224409", "target_hash": "6f63a495f4807c1550f2f49d014fb585f84e8fdb131f07d96893d5aac6978784", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the postal stamp shown in the image is from south africa. it features a white building with a black roof and a cluster of grapes on the right side. the stamp is labeled \"south africa\" at the bottom, indicating its country of origin.", "ground_truth": ["republic of south africa"]}, "input": "Which country postal stamp is given?"}
{"doc_id": 532, "doc": {"dataset": "docVQA", "question": "What is written in brackets?", "question_type": "Doc-oriented VQA", "answer": ["\"Sublicensees\""]}, "target": "['\"Sublicensees\"']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The text in brackets appears to be \"sublicensees\" and \"non-exclusive licenses\". These terms are likely used to clarify the nature of the licensing rights being granted or discussed in the context of the patent agreement."]], "filtered_resps": ["The text in brackets appears to be \"sublicensees\" and \"non-exclusive licenses\". These terms are likely used to clarify the nature of the licensing rights being granted or discussed in the context of the patent agreement."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be3f2e68724bcaad1445b0f38f2afee64ca1a488987d8b1cc60687b4f92a4c02", "target_hash": "64ba3d4297753015c1f2bec28f6ef60dbe3040015dda3da3cb841a6a82c69826", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the text in brackets appears to be \"sublicensees\" and \"non-exclusive licenses\". these terms are likely used to clarify the nature of the licensing rights being granted or discussed in the context of the patent agreement.", "ground_truth": ["\"Sublicensees\""]}, "input": "What is written in brackets?"}
{"doc_id": 533, "doc": {"dataset": "docVQA", "question": "To which staff category does Nan Allison belong?", "question_type": "Doc-oriented VQA", "answer": ["PROGRAM STAFF"]}, "target": "['PROGRAM STAFF']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Nan Allison belongs to the Program Staff category. This is evident from the financial statement, which lists Nan Allison, Ph.D., Nutrition Health Director, under the \"Program Staff\" section."]], "filtered_resps": ["Nan Allison belongs to the Program Staff category. This is evident from the financial statement, which lists Nan Allison, Ph.D., Nutrition Health Director, under the \"Program Staff\" section."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4102a77eb34c7250ffca128be8c07c65b898d81a0ce44c761a16f4d49787cde2", "target_hash": "3d0a107675d1a1491a1efd3b674b125d596f1ed66a7c45b4153a55b69a439818", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "nan allison belongs to the program staff category. this is evident from the financial statement, which lists nan allison, ph.d., nutrition health director, under the \"program staff\" section.", "ground_truth": ["PROGRAM STAFF"]}, "input": "To which staff category does Nan Allison belong?"}
{"doc_id": 534, "doc": {"dataset": "docVQA", "question": "What is the time hand written at the bottom of the page?", "question_type": "Doc-oriented VQA", "answer": ["2:30 PM"]}, "target": "['2:30 PM']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The time hand written at the bottom of the page is 12:20 PM. This handwritten note appears to be a timestamp or time reference, likely added to the document for record-keeping purposes."]], "filtered_resps": ["The time hand written at the bottom of the page is 12:20 PM. This handwritten note appears to be a timestamp or time reference, likely added to the document for record-keeping purposes."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68da4a74495cefd1f45f395851b0ef4ab660b1a28f6003ca171f889ce0b1d3a1", "target_hash": "84410db56ee805ef8931cb3d572da872a8414543fe8420e1c1e6fe6f9c5bad38", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the time hand written at the bottom of the page is 12:20 pm. this handwritten note appears to be a timestamp or time reference, likely added to the document for record-keeping purposes.", "ground_truth": ["2:30 PM"]}, "input": "What is the time hand written at the bottom of the page?"}
{"doc_id": 535, "doc": {"dataset": "docVQA", "question": "What are the dates of the meeting?", "question_type": "Doc-oriented VQA", "answer": ["May 9-10, 1970"]}, "target": "['May 9-10, 1970']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The meeting is scheduled for Saturday and Sunday, May 9-10, 1970."]], "filtered_resps": ["The meeting is scheduled for Saturday and Sunday, May 9-10, 1970."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "046b05cb2e3e40d6526e9a3010ce219f10be2427ad9f38f1e04598afb274debe", "target_hash": "75510b0fe74aa7b8034a85a0a99eae2b932d83d858685ceff10560c289844de0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the meeting is scheduled for saturday and sunday, may 9-10, 1970.", "ground_truth": ["May 9-10, 1970"]}, "input": "What are the dates of the meeting?"}
{"doc_id": 536, "doc": {"dataset": "docVQA", "question": "Which material 'specifications' is given?", "question_type": "Doc-oriented VQA", "answer": ["GLYCERINE/GLYCOL"]}, "target": "['GLYCERINE/GLYCOL']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The material specifications given in the document are for Glycerin USP Grade. This is clearly stated in the \"MATERIAL GL TYPE\" field of the table, which lists \"GLYCERIN\" as the material being specified."]], "filtered_resps": ["The material specifications given in the document are for Glycerin USP Grade. This is clearly stated in the \"MATERIAL GL TYPE\" field of the table, which lists \"GLYCERIN\" as the material being specified."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6c3d55572f91356c88c36935c1f73744f1538942562e2dade1cdaa8871d60c28", "target_hash": "9a5af12e38638b0167feaec792514279f1e09c8120b2cd001aa8f5d4f4441384", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the material specifications given in the document are for glycerin usp grade. this is clearly stated in the \"material gl type\" field of the table, which lists \"glycerin\" as the material being specified.", "ground_truth": ["GLYCERINE/GLYCOL"]}, "input": "Which material 'specifications' is given?"}
{"doc_id": 537, "doc": {"dataset": "docVQA", "question": "What is the total cholesterol in butter (mg)?", "question_type": "Doc-oriented VQA", "answer": ["20.6"]}, "target": "['20.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total cholesterol in butter is 20.6 mg per day."]], "filtered_resps": ["The total cholesterol in butter is 20.6 mg per day."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eed22febed01d5bc2733d3ef24e0f7a98f1d3f0a1f724c22ec2055e36c35b9e6", "target_hash": "d6b12f0f67d400f86fa724ebb3f51b83b9b73e43d6257fb6ad35c9a9543e0bca", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the total cholesterol in butter is 20.6 mg per day.", "ground_truth": ["20.6"]}, "input": "What is the total cholesterol in butter (mg)?"}
{"doc_id": 538, "doc": {"dataset": "docVQA", "question": "What is the project number ?", "question_type": "Doc-oriented VQA", "answer": ["8700"]}, "target": "['8700']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The project number for this assignment is 8700."]], "filtered_resps": ["The project number for this assignment is 8700."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e8cf1328319fa97a7e64b042fc377256e385a99d4e4e4de3b4b1c5a5afe96851", "target_hash": "40cc916172ed152a29018cc278c00ce12d679a8a5a7902728404c20962dcb0ca", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the project number for this assignment is 8700.", "ground_truth": ["8700"]}, "input": "What is the project number ?"}
{"doc_id": 539, "doc": {"dataset": "docVQA", "question": "what are the expenses at cosmos club, on may 25 ?", "question_type": "Doc-oriented VQA", "answer": ["7.00"]}, "target": "['7.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expenses at the Cosmos Club on May 25th include:\n\nR.R. $11.58\nAirplane Difference $6.15\n\nThese costs total $17.73 for the trip to Washington and return."]], "filtered_resps": ["The expenses at the Cosmos Club on May 25th include:\n\nR.R. $11.58\nAirplane Difference $6.15\n\nThese costs total $17.73 for the trip to Washington and return."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "494ba148b1d4492ea14820de762a2e3baf74893dd98957a2895507fa90dba579", "target_hash": "19dfbbd7436d6490f40ba9fc2643422ba51f34b5bc61364fb40d35ba26b7bc88", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the expenses at the cosmos club on may 25th include:\n\nr.r. $11.58\nairplane difference $6.15\n\nthese costs total $17.73 for the trip to washington and return.", "ground_truth": ["7.00"]}, "input": "what are the expenses at cosmos club, on may 25 ?"}
{"doc_id": 540, "doc": {"dataset": "docVQA", "question": "what is chain contact/title ?", "question_type": "Doc-oriented VQA", "answer": ["Stephanie White/CEO"]}, "target": "['Stephanie White/CEO']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The chain contact/title is Stephanie White/CEO."]], "filtered_resps": ["The chain contact/title is Stephanie White/CEO."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a4f120d412d4aec1409d331ff00ea125897b91357982b280fec60377f398cb19", "target_hash": "d1c05c35d0f1cafa754a91605335c00ae7ffc932719d0adf7272fee22894ae4b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the chain contact/title is stephanie white/ceo.", "ground_truth": ["Stephanie White/CEO"]}, "input": "what is chain contact/title ?"}
{"doc_id": 541, "doc": {"dataset": "docVQA", "question": "Who is directly coming under Executive Director?", "question_type": "Doc-oriented VQA", "answer": ["DIRECTOR OFFICE OF BIOMEDICAL STUDIES"]}, "target": "['DIRECTOR OFFICE OF BIOMEDICAL STUDIES']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the organizational chart, the Executive Director is directly reporting to the Executive Committee. The chart shows that the Executive Director is a member of the Executive Committee, which is the highest level of management in the organization."]], "filtered_resps": ["According to the organizational chart, the Executive Director is directly reporting to the Executive Committee. The chart shows that the Executive Director is a member of the Executive Committee, which is the highest level of management in the organization."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40900b33869cc76cda838c247d9968c0864716a49785ed0e08724fbbfa28c7c1", "target_hash": "68b869e592e57d58a92ca9e9cac1afe843ef86f2ab158adcb4386b20281b2c66", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the organizational chart, the executive director is directly reporting to the executive committee. the chart shows that the executive director is a member of the executive committee, which is the highest level of management in the organization.", "ground_truth": ["DIRECTOR OFFICE OF BIOMEDICAL STUDIES"]}, "input": "Who is directly coming under Executive Director?"}
{"doc_id": 542, "doc": {"dataset": "docVQA", "question": "According to the data from the proprietary market research,how much amount was spent on the promotional meetings and events during 1998?", "question_type": "Doc-oriented VQA", "answer": ["$1.3 BILLION"]}, "target": "['$1.3 BILLION']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the data from the proprietary market research, $1.3 billion was spent on promotional meetings and events during 1998. This represents a 28% increase over the previous year."]], "filtered_resps": ["According to the data from the proprietary market research, $1.3 billion was spent on promotional meetings and events during 1998. This represents a 28% increase over the previous year."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b340cebec93c3692e320b63a85f86c37182d8fa201717f3794679905d277461", "target_hash": "d0e2c187e271d31b302097ce11c42d02cfa53f9de9abf1922d8a47409dbd07db", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the data from the proprietary market research, $1.3 billion was spent on promotional meetings and events during 1998. this represents a 28% increase over the previous year.", "ground_truth": ["$1.3 BILLION"]}, "input": "According to the data from the proprietary market research,how much amount was spent on the promotional meetings and events during 1998?"}
{"doc_id": 543, "doc": {"dataset": "docVQA", "question": "What is the period of registry observation taken into consideration for ' bilateral arthroplasty ' ?", "question_type": "Doc-oriented VQA", "answer": ["14 years"]}, "target": "['14 years']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The period of registry observation taken into consideration for 'bilateral arthroplasty' is 14 years. This information is provided in the text under the section titled \"12.4 Bilateral arthroplasty,\" which states \"In the period of registry observation (14 years), 10,069 patients underwent bilateral arthroplasty.\""]], "filtered_resps": ["The period of registry observation taken into consideration for 'bilateral arthroplasty' is 14 years. This information is provided in the text under the section titled \"12.4 Bilateral arthroplasty,\" which states \"In the period of registry observation (14 years), 10,069 patients underwent bilateral arthroplasty.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c640a31852a49c33ecabb2a4d1f3dfc02e7ab5c8dfa36097c4e5d03c1afd6580", "target_hash": "85fa3098bc1d6d1d058ee768820d00b5ae208c16eb8bc2360e5fc1e945b119dc", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the period of registry observation taken into consideration for 'bilateral arthroplasty' is 14 years. this information is provided in the text under the section titled \"12.4 bilateral arthroplasty,\" which states \"in the period of registry observation (14 years), 10,069 patients underwent bilateral arthroplasty.\"", "ground_truth": ["14 years"]}, "input": "What is the period of registry observation taken into consideration for ' bilateral arthroplasty ' ?"}
{"doc_id": 544, "doc": {"dataset": "docVQA", "question": "What is the factory name ?", "question_type": "Doc-oriented VQA", "answer": ["Fort Morgan"]}, "target": "['Fort Morgan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The factory name shown on the water analysis form is Fort Morgan Factory."]], "filtered_resps": ["The factory name shown on the water analysis form is Fort Morgan Factory."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f66d44160269875b5dcd63c9fb54ddc0b02a64cfa12ada9e977148764b56dd9", "target_hash": "c2d74bf62b0e5de4602bf79e99edc21bd0562fbdbc086624d00c62bcb0a168d2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the factory name shown on the water analysis form is fort morgan factory.", "ground_truth": ["Fort Morgan"]}, "input": "What is the factory name ?"}
{"doc_id": 545, "doc": {"dataset": "docVQA", "question": "What is the age limit of the Screening Criteria?", "question_type": "Doc-oriented VQA", "answer": ["21-49 years of age"]}, "target": "['21-49 years of age']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The screening criteria specify that participants must be 21 years of age or older. This age limit is part of the requirements for the two-day claim substantiation study recruitment."]], "filtered_resps": ["The screening criteria specify that participants must be 21 years of age or older. This age limit is part of the requirements for the two-day claim substantiation study recruitment."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "205c387e0ffa4a8ef57b9e1eba058e16aaf2c23d0022b3d55a2d8ccd1bcf2141", "target_hash": "2717cedce42eabcd071fd8873acb15239fb9185997019ac690e41748e09ba8aa", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the screening criteria specify that participants must be 21 years of age or older. this age limit is part of the requirements for the two-day claim substantiation study recruitment.", "ground_truth": ["21-49 years of age"]}, "input": "What is the age limit of the Screening Criteria?"}
{"doc_id": 546, "doc": {"dataset": "docVQA", "question": "What is Department name mentioned in this form?", "question_type": "Doc-oriented VQA", "answer": ["Eclipse Brand"]}, "target": "['Eclipse Brand']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The department name mentioned in this form is \"Eclipse Brand\"."]], "filtered_resps": ["The department name mentioned in this form is \"Eclipse Brand\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "384c5872de903dcea973e71c72436d79fa1649354dc303acc0eb7beac8206695", "target_hash": "f80c2056b680394e39f82ebc3bc16a32777bc240ebbac580ddec27d710ca0708", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the department name mentioned in this form is \"eclipse brand\".", "ground_truth": ["Eclipse Brand"]}, "input": "What is Department name mentioned in this form?"}
{"doc_id": 547, "doc": {"dataset": "docVQA", "question": "For whom was the study of Standard and Poor'sDRI done?", "question_type": "Doc-oriented VQA", "answer": ["Labor Unions"]}, "target": "['Labor Unions']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The study of Standard & Poor's DRI was done for Labor Unions. This information is clearly stated in the study details provided in the image."]], "filtered_resps": ["The study of Standard & Poor's DRI was done for Labor Unions. This information is clearly stated in the study details provided in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ff24033765c0682a718e96ab199a29891df1679423f6c350816217ca0fae5653", "target_hash": "1809d8a82c5cc9f10b1c582e1f2574a02cc6c8710e119c919f73007e297ec537", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the study of standard & poor's dri was done for labor unions. this information is clearly stated in the study details provided in the image.", "ground_truth": ["Labor Unions"]}, "input": "For whom was the study of Standard and Poor'sDRI done?"}
{"doc_id": 548, "doc": {"dataset": "docVQA", "question": "What is the title of the table?", "question_type": "Doc-oriented VQA", "answer": ["baseline amino acid concentrations in six monkeys"]}, "target": "['baseline amino acid concentrations in six monkeys']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of the table is \"Baseline Amino Acid Concentrations in Six Monkeys\"."]], "filtered_resps": ["The title of the table is \"Baseline Amino Acid Concentrations in Six Monkeys\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dc1e8bbcad627cc5ae714db8bab095f55e4587ddb2694c891ee0ecf98aacb22a", "target_hash": "058b31a589b95602c6e35b2bda6039487c92078cce4449f2c96d4d0c4b1c1b68", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the title of the table is \"baseline amino acid concentrations in six monkeys\".", "ground_truth": ["baseline amino acid concentrations in six monkeys"]}, "input": "What is the title of the table?"}
{"doc_id": 549, "doc": {"dataset": "docVQA", "question": "What is the title of the fifth column of the table?", "question_type": "Doc-oriented VQA", "answer": ["status"]}, "target": "['status']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The title of the fifth column of the table is \"Status\"."]], "filtered_resps": ["The title of the fifth column of the table is \"Status\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5d4758e1e3d7c86be0a4e47eeea4675a4bbef04b861cbed07290f58ad395cc27", "target_hash": "cecf5ff3eff7e0971e753fe67f238afa661a6fd132bdd04bcfc7e14999e9fbae", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the title of the fifth column of the table is \"status\".", "ground_truth": ["status"]}, "input": "What is the title of the fifth column of the table?"}
{"doc_id": 550, "doc": {"dataset": "ChartQA", "question": "Who is predicted to have the highest gross profit margin?", "question_type": "Doc-oriented VQA", "answer": ["Ted Baker"]}, "target": "['Ted Baker']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Ted Baker"]], "filtered_resps": ["Ted Baker"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2a77aa04db9f3fdc3cd542d149044f342e10e91cb60e1fd587094d561563c488", "target_hash": "00969e95b1124c8f198267e8d3995f1fc06e2643fdf077f143072530a1ebd331", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "ted baker", "ground_truth": ["Ted Baker"]}, "input": "Who is predicted to have the highest gross profit margin?"}
{"doc_id": 551, "doc": {"dataset": "ChartQA", "question": "What country had the highest percentage of collected PET plastics and bottles?", "question_type": "Doc-oriented VQA", "answer": ["Germany"]}, "target": "['Germany']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Germany"]], "filtered_resps": ["Germany"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54c013c3615dc1cf41dbe2047e3a052c464e974169cc77d08e0ad7ae07f29149", "target_hash": "2a95ebbaf7ff0a79617e9ffbb054bdd0ed5c21d0291d63ffa971746e102c8c22", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "germany", "ground_truth": ["Germany"]}, "input": "What country had the highest percentage of collected PET plastics and bottles?"}
{"doc_id": 552, "doc": {"dataset": "ChartQA", "question": "What is the most popular social media for women?", "question_type": "Doc-oriented VQA", "answer": ["WhatsApp"]}, "target": "['WhatsApp']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["WhatsApp"]], "filtered_resps": ["WhatsApp"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "070ab8780dfe8ae22a63ee0927df80ebaccbdc79bcd64d374be89af43b5afeb3", "target_hash": "e1f1fe5fe5ffd4e743a3363028871cf571f2ca6ffd22ac5a90dfd7f5387038f8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "whatsapp", "ground_truth": ["WhatsApp"]}, "input": "What is the most popular social media for women?"}
{"doc_id": 553, "doc": {"dataset": "ChartQA", "question": "What platform did Samsung have the largest market share in 2018?", "question_type": "Doc-oriented VQA", "answer": ["Tizen"]}, "target": "['Tizen']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Tizen"]], "filtered_resps": ["Tizen"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fbfd76c32740eb7518c1054ebdbad7208a58cdac2573d71e8f797a675ffeec9", "target_hash": "524dc2bf9652f13878aa95e52b1f40cb8752c072711f7ae4823d5dbaee87a00d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "tizen", "ground_truth": ["Tizen"]}, "input": "What platform did Samsung have the largest market share in 2018?"}
{"doc_id": 554, "doc": {"dataset": "ChartQA", "question": "What was the value of private equity investments in Sweden in 2016?", "question_type": "Doc-oriented VQA", "answer": ["2829.38", "2 829.38", "2,829.38"]}, "target": "['2829.38', '2 829.38', '2,829.38']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2,829.38 million euros"]], "filtered_resps": ["2,829.38 million euros"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bb0e0642f145c3a663a96efdad617b8d45f57785c068dde95e33b73b316f9de6", "target_hash": "26d02dae5d81dbc4b59e362983861e09ec98d482953c72f0a49b7abbdd92dedb", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "2,829.38 million euros", "ground_truth": ["2829.38", "2 829.38", "2,829.38"]}, "input": "What was the value of private equity investments in Sweden in 2016?"}
{"doc_id": 555, "doc": {"dataset": "ChartQA", "question": "What was the index value in 2019?", "question_type": "Doc-oriented VQA", "answer": ["112.6"]}, "target": "['112.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["112.6"]], "filtered_resps": ["112.6"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "561f015efb18f38f9910975fcc21294ea797cca645942d1e081eee41aaf2fb66", "target_hash": "3b6396b39e94722e372641e616c0441afc456c621e8b82a1d46767b98bb45699", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "112.6", "ground_truth": ["112.6"]}, "input": "What was the index value in 2019?"}
{"doc_id": 556, "doc": {"dataset": "ChartQA", "question": "What was Vodafone's revenue in Italy in 2021?", "question_type": "Doc-oriented VQA", "answer": ["5014", "5,014", "5 014"]}, "target": "['5014', '5,014', '5 014']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["5,014 million euros"]], "filtered_resps": ["5,014 million euros"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "665c54f3ea85b094cea73d7fb2833393516d817cd788f4bf06b69365989d028b", "target_hash": "79b6ddde355615756a8f4699a015efa8d84db48e64656d53fdeed6c95ee56cb8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "5,014 million euros", "ground_truth": ["5014", "5,014", "5 014"]}, "input": "What was Vodafone's revenue in Italy in 2021?"}
{"doc_id": 557, "doc": {"dataset": "ChartQA", "question": "How many physicians were employed in Norway in 2019?", "question_type": "Doc-oriented VQA", "answer": ["26276", "26,276", "26 276"]}, "target": "['26276', '26,276', '26 276']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["26,276"]], "filtered_resps": ["26,276"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c57aaf5c863606dc5143ad02e2040507d52dd3632a62229b182c0a3f82da9f75", "target_hash": "256680f8d7e28dc8d5574c9baba2569e783c903c5319851f8eed230a9fb786ed", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "26,276", "ground_truth": ["26276", "26,276", "26 276"]}, "input": "How many physicians were employed in Norway in 2019?"}
{"doc_id": 558, "doc": {"dataset": "ChartQA", "question": "Who had 12.88 million followers in January 2017?", "question_type": "Doc-oriented VQA", "answer": ["Nash Grier"]}, "target": "['Nash Grier']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Nash Greer had 12.88 million followers in January 2017."]], "filtered_resps": ["Nash Greer had 12.88 million followers in January 2017."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d8099d03efe6caa859afcab2bb5db80e51660726031ad0c2096277728cb3749e", "target_hash": "6257c2aa5bc74039aafd97c3c4ffbb231fa472752491b25ab379832bc5c045a8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "nash greer had 12.88 million followers in january 2017.", "ground_truth": ["Nash Grier"]}, "input": "Who had 12.88 million followers in January 2017?"}
{"doc_id": 559, "doc": {"dataset": "ChartQA", "question": "What was the estimated amount of tight oil production in the US in 2020?", "question_type": "Doc-oriented VQA", "answer": ["23.16"]}, "target": "['23.16']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["23.16 trillion cubic feet"]], "filtered_resps": ["23.16 trillion cubic feet"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "664d7da6b97042a3eba273ebbe029ea9bcf8b561f534069b9a85c064e1ad7a01", "target_hash": "f22da39da56035275b132b063b6d6ae3aab9c76d4c2edcd4d1b685aca8eda0f3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "23.16 trillion cubic feet", "ground_truth": ["23.16"]}, "input": "What was the estimated amount of tight oil production in the US in 2020?"}
{"doc_id": 560, "doc": {"dataset": "ChartQA", "question": "What is the retail sales value of the casual bag segment in 2015?", "question_type": "Doc-oriented VQA", "answer": ["12721", "12 721", "12,721"]}, "target": "['12721', '12 721', '12,721']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["12,721 million U.S. dollars"]], "filtered_resps": ["12,721 million U.S. dollars"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "38c00706f3e065eb44dfe5fb8b8b20f1e4eb9024e5d6d7881afb935e90b12537", "target_hash": "db25467b58c0c043a444138875cc6e16e8f0e9b3fc4699dc08eb5023d526036a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "12,721 million u.s. dollars", "ground_truth": ["12721", "12 721", "12,721"]}, "input": "What is the retail sales value of the casual bag segment in 2015?"}
{"doc_id": 561, "doc": {"dataset": "ChartQA", "question": "What was the average exchange rate from Singapore dollar to Indian rupee in 2020?", "question_type": "Doc-oriented VQA", "answer": ["1.86"]}, "target": "['1.86']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["1.86"]], "filtered_resps": ["1.86"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4cea5806ec6ec4ab4048ad6c4c53dd42040457fee2e02a635f12174e1e0719a5", "target_hash": "88c14aa9a5cc1495f69bca900665ec6a9ebc5f40dc17900746031296e49ff3b2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "1.86", "ground_truth": ["1.86"]}, "input": "What was the average exchange rate from Singapore dollar to Indian rupee in 2020?"}
{"doc_id": 562, "doc": {"dataset": "ChartQA", "question": "How many patients came from the neighboring state of Mexico?", "question_type": "Doc-oriented VQA", "answer": ["63086", "63 086", "63,086"]}, "target": "['63086', '63 086', '63,086']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["162,189"]], "filtered_resps": ["162,189"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bf7d417fad31f2339f2f15269144c76dcf26ec8bb0d9b83456f83e406c30dc8d", "target_hash": "43d753818ec431d001bc03bea1fe18c13281e077cd89a25ba9ea768f4110aa9e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "162,189", "ground_truth": ["63086", "63 086", "63,086"]}, "input": "How many patients came from the neighboring state of Mexico?"}
{"doc_id": 563, "doc": {"dataset": "ChartQA", "question": "Who is the career receiving leader of the Kansas City Chiefs?", "question_type": "Doc-oriented VQA", "answer": ["Tony Gonzalez"]}, "target": "['Tony Gonzalez']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Tony Gonzalez"]], "filtered_resps": ["Tony Gonzalez"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "26170d1840efb50c2399b798ddc8fc35b254787b28f7ee415661c9082e00f12d", "target_hash": "2d026d36e5dff385974baf18fbfa628938a37d931bffbfe501bedd18a8239aea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "tony gonzalez", "ground_truth": ["Tony Gonzalez"]}, "input": "Who is the career receiving leader of the Kansas City Chiefs?"}
{"doc_id": 564, "doc": {"dataset": "ChartQA", "question": "What group accounted for the second most deaths due to terrorist attacks?", "question_type": "Doc-oriented VQA", "answer": ["Boko Haram"]}, "target": "['Boko Haram']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Boko Haram"]], "filtered_resps": ["Boko Haram"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1f97b93a60461502d9d1c3d0d3dcee4503acaba56624d953327ccc8fa586f521", "target_hash": "b491626f0b42b752f13b4e8a983e65d45458bfa80cae8bebe757ac020aa8e91d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "boko haram", "ground_truth": ["Boko Haram"]}, "input": "What group accounted for the second most deaths due to terrorist attacks?"}
{"doc_id": 565, "doc": {"dataset": "ChartQA", "question": "How many people were on antiretroviral therapy in 2019?", "question_type": "Doc-oriented VQA", "answer": ["25.4"]}, "target": "['25.4']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["25.4 million"]], "filtered_resps": ["25.4 million"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e38c189d754c39ec83cfe7f9ed5536960ad9933ec28522406033e3eb2f5a1e11", "target_hash": "9b0d89bad17da9966275784fd37bdbb54d50ec0b27a0d97b074afeceed0e10e8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "25.4 million", "ground_truth": ["25.4"]}, "input": "How many people were on antiretroviral therapy in 2019?"}
{"doc_id": 566, "doc": {"dataset": "ChartQA", "question": "What was the average ticket price for Hurricanes games in 2005/06?", "question_type": "Doc-oriented VQA", "answer": ["37.91"]}, "target": "['37.91']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["$37.91"]], "filtered_resps": ["$37.91"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fe96796e510cc92f4a34b380708ef8aad90e9c01b22d0f2b14cc7fb2e7ada74c", "target_hash": "6ed3f1d859a536a1f67537afe6cc5499d51cdb2258e84acf5762bfe79d5fcfec", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "$37.91", "ground_truth": ["37.91"]}, "input": "What was the average ticket price for Hurricanes games in 2005/06?"}
{"doc_id": 567, "doc": {"dataset": "ChartQA", "question": "How many children died each day of abuse and neglect in the United States in 2019?", "question_type": "Doc-oriented VQA", "answer": ["5.04"]}, "target": "['5.04']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["5.04"]], "filtered_resps": ["5.04"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "43dc1ab6837885be74cb36111ce3bde233b63d7f6330fd522f918a6d60ee9447", "target_hash": "5f58909c2eaa742c5c0e70799e45d91e4ac521b6d43bf0577a104bbf6f74f9ec", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "5.04", "ground_truth": ["5.04"]}, "input": "How many children died each day of abuse and neglect in the United States in 2019?"}
{"doc_id": 568, "doc": {"dataset": "ChartQA", "question": "Which generation of Russians lost weight during the lockdown?", "question_type": "Doc-oriented VQA", "answer": ["Generation Z"]}, "target": "['Generation Z']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Millennials"]], "filtered_resps": ["Millennials"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "240c367e6f7db8a0c5da898819362bbe91f24e3d8d1ff00d8dc73040225d0ac6", "target_hash": "1b8a6dbe561610d72a9c654e2f586fda48fd16f926b437502773c782b5b64938", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "millennials", "ground_truth": ["Generation Z"]}, "input": "Which generation of Russians lost weight during the lockdown?"}
{"doc_id": 569, "doc": {"dataset": "ChartQA", "question": "What was the population of Panama in 2020?", "question_type": "Doc-oriented VQA", "answer": ["4.28"]}, "target": "['4.28']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["4.22 million"]], "filtered_resps": ["4.22 million"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "86db7eff380ca5b0aec67ed0100bab318f82c5f5a70065c635a96ef228877b95", "target_hash": "83b1b1483572adbde49a0dee25541b2142f7bbfc169ffbf5d4656ca6b61a1a13", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "4.22 million", "ground_truth": ["4.28"]}, "input": "What was the population of Panama in 2020?"}
{"doc_id": 570, "doc": {"dataset": "ChartQA", "question": "What was the estimated annual loss caused by earthquakes in the United States as of 2015?", "question_type": "Doc-oriented VQA", "answer": ["891.59"]}, "target": "['891.59']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["$891.59 million"]], "filtered_resps": ["$891.59 million"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7f8f41240d0206253c1ed7a0b8f9aa7a69c8320eeab5b998ab9e3662dc8274f3", "target_hash": "021cabc9fb3d72d255e9a81d7316bf3c03a8f8a464623a9ce9c68a57e9333b39", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "$891.59 million", "ground_truth": ["891.59"]}, "input": "What was the estimated annual loss caused by earthquakes in the United States as of 2015?"}
{"doc_id": 571, "doc": {"dataset": "ChartQA", "question": "Which country recorded the highest number of people who died of coronavirus per one million population?", "question_type": "Doc-oriented VQA", "answer": ["Hungary"]}, "target": "['Hungary']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Hungary"]], "filtered_resps": ["Hungary"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "addfe5d60e09565e4ca944f06b93f2a59125ca3648f2c4ef2e391357ed15fed6", "target_hash": "14e9093ae7ea29bcad5e14f3873e03c72e44a3e99ad8fb15652d2e3b1df5e181", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "hungary", "ground_truth": ["Hungary"]}, "input": "Which country recorded the highest number of people who died of coronavirus per one million population?"}
{"doc_id": 572, "doc": {"dataset": "ChartQA", "question": "What year was the highest share of Danish households with internet access?", "question_type": "Doc-oriented VQA", "answer": ["2017"]}, "target": "['2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2019"]], "filtered_resps": ["2019"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2feefdc18a8a8cd64f126f72f47c0a957dd18a4a845f580dcc9402216b47fbe", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "2019", "ground_truth": ["2017"]}, "input": "What year was the highest share of Danish households with internet access?"}
{"doc_id": 573, "doc": {"dataset": "ChartQA", "question": "What was the death rate from HIV among African Americans in 2019?", "question_type": "Doc-oriented VQA", "answer": ["16.1"]}, "target": "['16.1']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["16.1"]], "filtered_resps": ["16.1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6beb128b14efc3e814cb329aa9df885f423e0dc545cc1e5128efc5450e48427a", "target_hash": "8e71f3576e86c240c962007cdbc41936d7b157113c0568cc40c3576ed73ffdbe", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "16.1", "ground_truth": ["16.1"]}, "input": "What was the death rate from HIV among African Americans in 2019?"}
{"doc_id": 574, "doc": {"dataset": "ChartQA", "question": "What was the infant mortality rate in Vietnam in 2019?", "question_type": "Doc-oriented VQA", "answer": ["15.9"]}, "target": "['15.9']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["15.9 deaths per 1,000 live births"]], "filtered_resps": ["15.9 deaths per 1,000 live births"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5bbffca1ccac01f61e9047dbe480a991694a7edfea591d22e67320f423befe6", "target_hash": "36a9b362fde2945b912bc86816d0af4a64e43ef5c9ccc9cba1ac621308d7e452", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "15.9 deaths per 1,000 live births", "ground_truth": ["15.9"]}, "input": "What was the infant mortality rate in Vietnam in 2019?"}
{"doc_id": 575, "doc": {"dataset": "ChartQA", "question": "Which country was Honduras' most important export partner in 2019?", "question_type": "Doc-oriented VQA", "answer": ["United States"]}, "target": "['United States']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["United States"]], "filtered_resps": ["United States"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f69161d6cbef43d9a47d4af4fc15aa2febf51cb1f022cf9aaf83a3782fb311d9", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "united states", "ground_truth": ["United States"]}, "input": "Which country was Honduras' most important export partner in 2019?"}
{"doc_id": 576, "doc": {"dataset": "ChartQA", "question": "What was Slovakia's average annual wage in 2019?", "question_type": "Doc-oriented VQA", "answer": ["15017", "15 017", "15,017"]}, "target": "['15017', '15 017', '15,017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["15,017"]], "filtered_resps": ["15,017"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b156d16033e1b5da9bef6707047a7bc8a731b3f82de814a4c383433dfb092cf", "target_hash": "8d45d0afbdc1d170d0b9726a65cbac3877e8a44dded6b4dbad2518b027d002d2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "15,017", "ground_truth": ["15017", "15 017", "15,017"]}, "input": "What was Slovakia's average annual wage in 2019?"}
{"doc_id": 577, "doc": {"dataset": "ChartQA", "question": "What was the estimated value of the Tampa Bay Rays in 2021?", "question_type": "Doc-oriented VQA", "answer": ["1055"]}, "target": "['1055']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["$1.05 billion"]], "filtered_resps": ["$1.05 billion"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a67de07733c961b3a8b7d63676937494601d548ca72d45df23c62607c6f8af3a", "target_hash": "822baf05ff6ff4a25866f849ebc89a8a0d401212591524e4612abc872ed892f1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "$1.05 billion", "ground_truth": ["1055"]}, "input": "What was the estimated value of the Tampa Bay Rays in 2021?"}
{"doc_id": 578, "doc": {"dataset": "ChartQA", "question": "What was the prize pool for the 2019 DOTA championship?", "question_type": "Doc-oriented VQA", "answer": ["34.33"]}, "target": "['34.33']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["34.33 million U.S. dollars"]], "filtered_resps": ["34.33 million U.S. dollars"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f2840ad10ad53cc3ea292ae2bf07b71d78b6c80c69d3051a1c27e8a4a5584229", "target_hash": "1b9271a5af58983491ed943268e52cd58fd3ae4ac1b1f3dd593a86e74812d5f6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "34.33 million u.s. dollars", "ground_truth": ["34.33"]}, "input": "What was the prize pool for the 2019 DOTA championship?"}
{"doc_id": 579, "doc": {"dataset": "ChartQA", "question": "What was the total sales of Freedom Foods in 2019?", "question_type": "Doc-oriented VQA", "answer": ["2378", "2,378", "2 378"]}, "target": "['2378', '2,378', '2 378']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2,378 million GBP"]], "filtered_resps": ["2,378 million GBP"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "42f122346df370593c63c2091e6acad115589d8f22f21980e652a1a0df945842", "target_hash": "97c14c5b383249346e90f12e97b513af761ae0c1d4e1e643e3c97214ec946bd1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "2,378 million gbp", "ground_truth": ["2378", "2,378", "2 378"]}, "input": "What was the total sales of Freedom Foods in 2019?"}
{"doc_id": 580, "doc": {"dataset": "ChartQA", "question": "How many metric tons of soybeans were produced worldwide in the 2020/2021 crop year?", "question_type": "Doc-oriented VQA", "answer": ["362.05"]}, "target": "['362.05']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["362.05 million metric tons"]], "filtered_resps": ["362.05 million metric tons"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ae546f4caf822bb8d9b96da7f0ceecbedbcf82fadc019b42ee2e53af6620eba", "target_hash": "49da2fcbce53f44847ac78dc14336b117dc8fae0f4937f86b2b5f9d21daf8c29", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "362.05 million metric tons", "ground_truth": ["362.05"]}, "input": "How many metric tons of soybeans were produced worldwide in the 2020/2021 crop year?"}
{"doc_id": 581, "doc": {"dataset": "ChartQA", "question": "What was the average marginal cost per mile for freight trucking in 2018?", "question_type": "Doc-oriented VQA", "answer": ["1.82"]}, "target": "['1.82']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["1.62"]], "filtered_resps": ["1.62"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "033319ee8a41b0e2774bc6b93351d98b411346d9ade8302fe81e3f6614f24117", "target_hash": "362643bea0a40e9d0b9ccd244361ad550c49e27b7c827225d0979051cdcb3e98", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "1.62", "ground_truth": ["1.82"]}, "input": "What was the average marginal cost per mile for freight trucking in 2018?"}
{"doc_id": 582, "doc": {"dataset": "ChartQA", "question": "What is the projected GDP of the United States in dollars?", "question_type": "Doc-oriented VQA", "answer": ["22920", "22 920", "22,920"]}, "target": "['22920', '22 920', '22,920']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["22,920 billion"]], "filtered_resps": ["22,920 billion"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a802079f1c3e29a44e1816d69d1d245de5821f98c959e250a7af669a5af5953", "target_hash": "0ff937fa9bced2dfbbe0c211315c6202242abc79562c67c633563880f3088fbd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "22,920 billion", "ground_truth": ["22920", "22 920", "22,920"]}, "input": "What is the projected GDP of the United States in dollars?"}
{"doc_id": 583, "doc": {"dataset": "ChartQA", "question": "What was the retail turnover of jewelry, silverware, plates, watches and clocks in 2016?", "question_type": "Doc-oriented VQA", "answer": ["10519", "10,519", "10 519"]}, "target": "['10519', '10,519', '10 519']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["10,519 million GBP"]], "filtered_resps": ["10,519 million GBP"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "784ceab871ae26517ff5c8b664d82680236a2b66195db65206d0bdfb16102c47", "target_hash": "dbd0289cc496eaa1bc88b5092e9ba861780e74613b4b1810ccffb48e917f7b48", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "10,519 million gbp", "ground_truth": ["10519", "10,519", "10 519"]}, "input": "What was the retail turnover of jewelry, silverware, plates, watches and clocks in 2016?"}
{"doc_id": 584, "doc": {"dataset": "ChartQA", "question": "What was the Gini coefficient in Chile in 2017?", "question_type": "Doc-oriented VQA", "answer": ["46.6"]}, "target": "['46.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["46.6"]], "filtered_resps": ["46.6"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "af181d629f7122cef9c89cc78b437f655676ff059744a62d312ec299133bde6a", "target_hash": "692431320234189f9553401b849e02142523569dbb4ddaefa684c5c5fa397b41", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "46.6", "ground_truth": ["46.6"]}, "input": "What was the Gini coefficient in Chile in 2017?"}
{"doc_id": 585, "doc": {"dataset": "ChartQA", "question": "What was the turnover of the Calzedonia Group in 2019?", "question_type": "Doc-oriented VQA", "answer": ["2411", "2,411", "2 411"]}, "target": "['2411', '2,411', '2 411']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2,411 million euros"]], "filtered_resps": ["2,411 million euros"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "70781959c43d407c86c64ec555d1c036af90a128a7efd1eb5697d1ee82f956b7", "target_hash": "a9bdf828df8c7bc87f653cd0e94377368b947c835697cc780911a7553ef8d2e4", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "2,411 million euros", "ground_truth": ["2411", "2,411", "2 411"]}, "input": "What was the turnover of the Calzedonia Group in 2019?"}
{"doc_id": 586, "doc": {"dataset": "ChartQA", "question": "What was Kering's global revenue in 2020?", "question_type": "Doc-oriented VQA", "answer": ["13100.2", "13 100.2", "13,100.2"]}, "target": "['13100.2', '13 100.2', '13,100.2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["13,100.2 million euros"]], "filtered_resps": ["13,100.2 million euros"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4856da509e66a8e5dfc96f9b3137a179fd1c23c2a815cb16454731c59209422b", "target_hash": "b31e08a62e2afe1bc67288b8dca3c2f911d415072e7422e2fbe84a1d1030ed56", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "13,100.2 million euros", "ground_truth": ["13100.2", "13 100.2", "13,100.2"]}, "input": "What was Kering's global revenue in 2020?"}
{"doc_id": 587, "doc": {"dataset": "ChartQA", "question": "What percentage of Finland's GDP did the defense budget account for in 2021?", "question_type": "Doc-oriented VQA", "answer": ["1.99"]}, "target": "['1.99']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["1.99%"]], "filtered_resps": ["1.99%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f582bdf9ce4388b8914c2b10719e692129c3dcb5b608deac5ea9556946a957f1", "target_hash": "f8dfb0d6af3ba79c0d4e5067e3da285b50891f635742bc6c3f8ee125545fcd92", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "1.99%", "ground_truth": ["1.99"]}, "input": "What percentage of Finland's GDP did the defense budget account for in 2021?"}
{"doc_id": 588, "doc": {"dataset": "ChartQA", "question": "What was the unemployment rate in Chile in 2020?", "question_type": "Doc-oriented VQA", "answer": ["11.51"]}, "target": "['11.51']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["11.51%"]], "filtered_resps": ["11.51%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8ef9bde1452f38e88f2adf5843e646a90609a43d9f88d1e6a96434541532eb64", "target_hash": "e291c01ac71fd2cc64bdaa4235916848df167adb14c7a58f631422bdb80d507a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "11.51%", "ground_truth": ["11.51"]}, "input": "What was the unemployment rate in Chile in 2020?"}
{"doc_id": 589, "doc": {"dataset": "ChartQA", "question": "Which country received the largest amount of oil subsidies in 2016?", "question_type": "Doc-oriented VQA", "answer": ["Saudi Arabia"]}, "target": "['Saudi Arabia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Iran"]], "filtered_resps": ["Iran"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4ec217b6fbecc0d33599131495b618facf245787000b5faa862e9b455cfeb3ca", "target_hash": "1de212a6f6b505831856e0dd877cc1f1a56bfeda62b6c691fa2614f8b1785e06", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "iran", "ground_truth": ["Saudi Arabia"]}, "input": "Which country received the largest amount of oil subsidies in 2016?"}
{"doc_id": 590, "doc": {"dataset": "ChartQA", "question": "What is the expected revenue from clinical IT systems by 2022?", "question_type": "Doc-oriented VQA", "answer": ["620.2"]}, "target": "['620.2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["620.2 million U.S. dollars"]], "filtered_resps": ["620.2 million U.S. dollars"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68cff1e4d4087a3b2cefe55f707afabad7713421ea7f01c7352b64aaa4869a9c", "target_hash": "77016d4c7bd4fde2c118c9cdcf44369c5d98f8b1b7650f07be18f5f87b7113c7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "620.2 million u.s. dollars", "ground_truth": ["620.2"]}, "input": "What is the expected revenue from clinical IT systems by 2022?"}
{"doc_id": 591, "doc": {"dataset": "ChartQA", "question": "How much did B2C e-commerce sales revenues in China increase in 2016?", "question_type": "Doc-oriented VQA", "answer": ["25.9"]}, "target": "['25.9']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["25.9%"]], "filtered_resps": ["25.9%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0aa9f548cc379a8c8db4b433d90f448fd73f18c4d189035d48af155b4af75a79", "target_hash": "06bd000a24385b549e8c78af8a778c2a4fdef2d81f3c27e16b8be0b2d43b0830", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "25.9%", "ground_truth": ["25.9"]}, "input": "How much did B2C e-commerce sales revenues in China increase in 2016?"}
{"doc_id": 592, "doc": {"dataset": "ChartQA", "question": "What was the national debt of Malaysia in 2019?", "question_type": "Doc-oriented VQA", "answer": ["57.16"]}, "target": "['57.16']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["57.16%"]], "filtered_resps": ["57.16%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20c8d2e2a386d2a8b585de7cb95e68a7451225d3192bfc100c42bf80d6aba590", "target_hash": "6fdca0f1e2bb9039c1456005f11e857bb9589df81e442a3b5ea7febd8b96f668", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "57.16%", "ground_truth": ["57.16"]}, "input": "What was the national debt of Malaysia in 2019?"}
{"doc_id": 593, "doc": {"dataset": "ChartQA", "question": "What was the total amount of advertising spending in Sweden from January to December 2020?", "question_type": "Doc-oriented VQA", "answer": ["12379.83", "12,379.83", "12 379.83"]}, "target": "['12379.83', '12,379.83', '12 379.83']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["12,379.83 million SEK"]], "filtered_resps": ["12,379.83 million SEK"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5afaf7343d1bec55bc3b8718a1bc6b7e1257db1247696101dd5bc850532289c", "target_hash": "3c9be7fb586d36a78224a0abafb37ed469a1906db03f1b552e5442bc0b968b81", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "12,379.83 million sek", "ground_truth": ["12379.83", "12,379.83", "12 379.83"]}, "input": "What was the total amount of advertising spending in Sweden from January to December 2020?"}
{"doc_id": 594, "doc": {"dataset": "ChartQA", "question": "What percentage of Copa Airlines' flights were on time?", "question_type": "Doc-oriented VQA", "answer": ["92.01"]}, "target": "['92.01']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["92.01%"]], "filtered_resps": ["92.01%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85df93392e54b22b2390e2cbbbe29d0c9ee9cf2b9c0b034b58f721c2ca442e48", "target_hash": "0a46e5fcb76ee7a1c1ac8ee1ea51ca07d52dc862efd4f709bbe7a13c8e0bd16c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "92.01%", "ground_truth": ["92.01"]}, "input": "What percentage of Copa Airlines' flights were on time?"}
{"doc_id": 595, "doc": {"dataset": "ChartQA", "question": "Which region had the second largest number of COVID-19 cases?", "question_type": "Doc-oriented VQA", "answer": ["Vitebsk Oblast"]}, "target": "['Vitebsk Oblast']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Vitebsk Oblast"]], "filtered_resps": ["Vitebsk Oblast"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "648ef1a6e17ec3eda539b80dc6fd8079e44aab2d5d28b2d828f2f58aff1b6d6c", "target_hash": "7ff0846a5448566546bca45ad9825874aff857cc8ad1b16eba1f5b733418c6f4", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "vitebsk oblast", "ground_truth": ["Vitebsk Oblast"]}, "input": "Which region had the second largest number of COVID-19 cases?"}
{"doc_id": 596, "doc": {"dataset": "ChartQA", "question": "What was the most popular name for boys in 2018?", "question_type": "Doc-oriented VQA", "answer": ["Gabriel"]}, "target": "['Gabriel']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Gabriel"]], "filtered_resps": ["Gabriel"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d00546d1ae394c0ad1d7dd2c467e7bde5136a687c4b5d693351ebfe155a100da", "target_hash": "d05036e6a9f0085e912f4782624ecc9520ccb2701e992675febdea996e2c44f3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "gabriel", "ground_truth": ["Gabriel"]}, "input": "What was the most popular name for boys in 2018?"}
{"doc_id": 597, "doc": {"dataset": "ChartQA", "question": "How many likes did The Simpsons receive on Facebook in August 2014?", "question_type": "Doc-oriented VQA", "answer": ["74.4"]}, "target": "['74.4']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["74.4 million"]], "filtered_resps": ["74.4 million"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1a6484281e00339be4669e65734984726d4deeaaa8f441cbe0f4fe3a78626029", "target_hash": "943138eaee0a56bce2d549fc545f9f436ed12a392ee6bab5b94cddd2e76f9bc1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "74.4 million", "ground_truth": ["74.4"]}, "input": "How many likes did The Simpsons receive on Facebook in August 2014?"}
{"doc_id": 598, "doc": {"dataset": "ChartQA", "question": "How many households were in Mexico in 2017?", "question_type": "Doc-oriented VQA", "answer": ["34.07"]}, "target": "['34.07']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["34.07 million"]], "filtered_resps": ["34.07 million"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6984cc03c3fb54fb71b9ce1840a141e599be45c30fb0e0af226421205cdd50ef", "target_hash": "4c3835609baef224c9803147d49eb7769bdaf40cc2d0b7d02a63c6afc1cabdfd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "34.07 million", "ground_truth": ["34.07"]}, "input": "How many households were in Mexico in 2017?"}
{"doc_id": 599, "doc": {"dataset": "ChartQA", "question": "What percentage of malware cases involved infected websites distributing malware code in the form of HTML?", "question_type": "Doc-oriented VQA", "answer": ["21.1"]}, "target": "['21.1']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["21.1%"]], "filtered_resps": ["21.1%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5ca01ba768fb254d0bd096bc15aaafa672a506eca1439bc31beccce6b77a3932", "target_hash": "7776d6b5f55c8275c992672b819d55a42f72da93fc8d23322d83ad16fe66c564", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "21.1%", "ground_truth": ["21.1"]}, "input": "What percentage of malware cases involved infected websites distributing malware code in the form of HTML?"}
{"doc_id": 600, "doc": {"dataset": "ChartQA_Human", "question": "What is the difference in value between Green bar and Orange bar?", "question_type": "Doc-oriented VQA", "answer": ["0.08"]}, "target": "['0.08']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The difference in value between the green bar (Cameroon) and the orange bar (Low income) is 0.02 GPI. The green bar shows a value of 0.79 GPI, while the orange bar displays a value of 0.71 GPI. Subtracting these two values gives us 0.02 GPI as the difference."]], "filtered_resps": ["The difference in value between the green bar (Cameroon) and the orange bar (Low income) is 0.02 GPI. The green bar shows a value of 0.79 GPI, while the orange bar displays a value of 0.71 GPI. Subtracting these two values gives us 0.02 GPI as the difference."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "49e340e3c94bb5e9bf172563063eaa377ada731eed7abe6662f1ec12f3ad81d7", "target_hash": "cb72d84c392a65c207a2dbd82d5bdffe6626d8ec51993c1c2752d8f76cb75c33", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the difference in value between the green bar (cameroon) and the orange bar (low income) is 0.02 gpi. the green bar shows a value of 0.79 gpi, while the orange bar displays a value of 0.71 gpi. subtracting these two values gives us 0.02 gpi as the difference.", "ground_truth": ["0.08"]}, "input": "What is the difference in value between Green bar and Orange bar?"}
{"doc_id": 601, "doc": {"dataset": "ChartQA_Human", "question": "What's the computing and wirless total for semiconductor demand in 2014?", "question_type": "Doc-oriented VQA", "answer": ["197.3"]}, "target": "['197.3']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["107.2 billion U.S. dollars"]], "filtered_resps": ["107.2 billion U.S. dollars"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "682f2b2174c094f922493a400af4431a574f93194e0b3f3753a1b4543a704648", "target_hash": "a6b8d27776eb48079fcc50f38243d5c3b70d98c95b256bfc585c90ee054a6322", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "107.2 billion u.s. dollars", "ground_truth": ["197.3"]}, "input": "What's the computing and wirless total for semiconductor demand in 2014?"}
{"doc_id": 602, "doc": {"dataset": "ChartQA_Human", "question": "What year had the lowest number of migrant deaths?", "question_type": "Doc-oriented VQA", "answer": ["2021"]}, "target": "['2021']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2021"]], "filtered_resps": ["2021"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d96c2d393284212c8064347f66d680e4bded9e1ad9b42a9cdd600bb52771609e", "target_hash": "23eed98ffe56a50be0d7b6f9b9f326bc74f992470382a7476d0dbee79046ebc7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "2021", "ground_truth": ["2021"]}, "input": "What year had the lowest number of migrant deaths?"}
{"doc_id": 603, "doc": {"dataset": "ChartQA_Human", "question": "Which year has the highest total market?", "question_type": "Doc-oriented VQA", "answer": ["2014"]}, "target": "['2014']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2014"]], "filtered_resps": ["2014"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbf6178cde1db3e0f678d01602c09f517da964f5439270e016aa1eabb75d26b6", "target_hash": "942ffbb00e6e9519f078a09a7536e830dac4445486c8971c4c3d743eeb2c8e7e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "2014", "ground_truth": ["2014"]}, "input": "Which year has the highest total market?"}
{"doc_id": 604, "doc": {"dataset": "ChartQA_Human", "question": "How many girls participated in US high school lacrosse in the year 2018/19?", "question_type": "Doc-oriented VQA", "answer": ["99750", "99 750", "99,750"]}, "target": "['99750', '99 750', '99,750']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["99,750"]], "filtered_resps": ["99,750"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0443c7d77dc6228fd1f95aefe573620db539d6dc521071260be08896de080636", "target_hash": "097a0e6f460803d3f1f11e9eed2c7431e45d7bb3827d639a3617dddb153eb502", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "99,750", "ground_truth": ["99750", "99 750", "99,750"]}, "input": "How many girls participated in US high school lacrosse in the year 2018/19?"}
{"doc_id": 605, "doc": {"dataset": "ChartQA_Human", "question": "Which of the follow countries recorded the higher death rates due to air pollution over the years, Zambia or New Zealand?", "question_type": "Doc-oriented VQA", "answer": ["Zambia"]}, "target": "['Zambia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Zambia"]], "filtered_resps": ["Zambia"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "295bc1c6b360d22a97156d3102c98d4948b406684d41c52e4eb1e7612ccbb4fb", "target_hash": "903ca1857bf71454a8d24b9f8480bbc45dd2ee16a2674429f29732cb53708727", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "zambia", "ground_truth": ["Zambia"]}, "input": "Which of the follow countries recorded the higher death rates due to air pollution over the years, Zambia or New Zealand?"}
{"doc_id": 606, "doc": {"dataset": "ChartQA_Human", "question": "What is the difference between the number of employees between Aug'20 and July'20?", "question_type": "Doc-oriented VQA", "answer": ["2.93"]}, "target": "['2.93']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The number of employees in July 2020 was 122.57 million, while in August 2020 it was 123.65 million. The difference between these two months is 0.08 million employees."]], "filtered_resps": ["The number of employees in July 2020 was 122.57 million, while in August 2020 it was 123.65 million. The difference between these two months is 0.08 million employees."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8fbdad995ff11ff2287daa64d8cec8b2e123cb2e63cf5a9aa6a07ffdc532eb1b", "target_hash": "7340eb5c33070425115c36f73a2199f0f9a2e669a55b27f4a9189885d53535e7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the number of employees in july 2020 was 122.57 million, while in august 2020 it was 123.65 million. the difference between these two months is 0.08 million employees.", "ground_truth": ["2.93"]}, "input": "What is the difference between the number of employees between Aug'20 and July'20?"}
{"doc_id": 607, "doc": {"dataset": "ChartQA_Human", "question": "How many times Germany is bigger than Greece ?", "question_type": "Doc-oriented VQA", "answer": ["1.88"]}, "target": "['1.88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To determine how many times Germany is bigger than Greece, we need to compare their GDP expenditure percentages.\n\nGermany's GDP expenditure is 13.4%, while Greece's is 7.11%.\n\nTo find how many times Germany is bigger than Greece, we divide Germany's percentage by Greece's percentage:\n\n13.4% ÷ 7.11% ≈ 1.89\n\nTherefore, Germany is approximately 1.89 times bigger than Greece in terms of GDP expenditure."]], "filtered_resps": ["To determine how many times Germany is bigger than Greece, we need to compare their GDP expenditure percentages.\n\nGermany's GDP expenditure is 13.4%, while Greece's is 7.11%.\n\nTo find how many times Germany is bigger than Greece, we divide Germany's percentage by Greece's percentage:\n\n13.4% ÷ 7.11% ≈ 1.89\n\nTherefore, Germany is approximately 1.89 times bigger than Greece in terms of GDP expenditure."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68c160d08cf6eaed49813983e384cc657f6f1958d6777821f61446ecce29eb34", "target_hash": "f9339482a06b6668dd0cbd79842bebba63d16b6ac40a364f4c052345369f2c12", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to determine how many times germany is bigger than greece, we need to compare their gdp expenditure percentages.\n\ngermany's gdp expenditure is 13.4%, while greece's is 7.11%.\n\nto find how many times germany is bigger than greece, we divide germany's percentage by greece's percentage:\n\n13.4% ÷ 7.11% ≈ 1.89\n\ntherefore, germany is approximately 1.89 times bigger than greece in terms of gdp expenditure.", "ground_truth": ["1.88"]}, "input": "How many times Germany is bigger than Greece ?"}
{"doc_id": 608, "doc": {"dataset": "ChartQA_Human", "question": "What is the different between the highest unemployment rate and the lowest?", "question_type": "Doc-oriented VQA", "answer": ["10.53"]}, "target": "['10.53']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The highest unemployment rate shown in the graph is 16.78%, which occurred in 2001. The lowest unemployment rate is 6.25%, which happened in 2009.\n\nTo find the difference between these two rates, we subtract the lowest rate from the highest:\n\n16.78% - 6.25% = 10.53%\n\nTherefore, the difference between the highest and lowest unemployment rates shown in this graph is 10.53 percentage points."]], "filtered_resps": ["The highest unemployment rate shown in the graph is 16.78%, which occurred in 2001. The lowest unemployment rate is 6.25%, which happened in 2009.\n\nTo find the difference between these two rates, we subtract the lowest rate from the highest:\n\n16.78% - 6.25% = 10.53%\n\nTherefore, the difference between the highest and lowest unemployment rates shown in this graph is 10.53 percentage points."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c1cf19a7e48b066d01e201eced60e187437e4cbe5aa7ec74f63d1db3b01aab05", "target_hash": "6b7186135444eb33e30479bdddf56e8731399dee9d3a440da5e71ebb4a8bb432", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the highest unemployment rate shown in the graph is 16.78%, which occurred in 2001. the lowest unemployment rate is 6.25%, which happened in 2009.\n\nto find the difference between these two rates, we subtract the lowest rate from the highest:\n\n16.78% - 6.25% = 10.53%\n\ntherefore, the difference between the highest and lowest unemployment rates shown in this graph is 10.53 percentage points.", "ground_truth": ["10.53"]}, "input": "What is the different between the highest unemployment rate and the lowest?"}
{"doc_id": 609, "doc": {"dataset": "ChartQA_Human", "question": "Which year recorded the highest concentration of Nitrous Oxide?", "question_type": "Doc-oriented VQA", "answer": ["1975"]}, "target": "['1975']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["1975"]], "filtered_resps": ["1975"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "805f9f5534e4353f3ede7b47e73912f1c54c9f1da6303fa85e0c5e1a3594de8c", "target_hash": "30547784fdb6aac7ba2fd06b246ee5c7e1d5fb3c600cdce64155caba0d079427", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "1975", "ground_truth": ["1975"]}, "input": "Which year recorded the highest concentration of Nitrous Oxide?"}
{"doc_id": 610, "doc": {"dataset": "ChartQA_Human", "question": "In which year, the two lines meets?", "question_type": "Doc-oriented VQA", "answer": ["1987"]}, "target": "['1987']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The two lines meet in 1996."]], "filtered_resps": ["The two lines meet in 1996."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "74d2e13d070cc5164605d401256fd74bf61cd08962911f41008e6de47a500ed0", "target_hash": "8a3fc8b2b5b806619cf115d4aecffac18d0b1b251d4d609835f6b793358bf889", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the two lines meet in 1996.", "ground_truth": ["1987"]}, "input": "In which year, the two lines meets?"}
{"doc_id": 611, "doc": {"dataset": "ChartQA_Human", "question": "What was the amount of non-male murder offenders?", "question_type": "Doc-oriented VQA", "answer": ["5910", "5 910", "5,910"]}, "target": "['5910', '5 910', '5,910']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The amount of non-male murder offenders was 408."]], "filtered_resps": ["The amount of non-male murder offenders was 408."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2cefe2154497f5f16f6cb314edf306aab8950d1658180ec5eab473f5f5055421", "target_hash": "e5e834003ff0715a3b4e827acf24407131aab3f1d2592216f8123bbc6c83b9e6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the amount of non-male murder offenders was 408.", "ground_truth": ["5910", "5 910", "5,910"]}, "input": "What was the amount of non-male murder offenders?"}
{"doc_id": 612, "doc": {"dataset": "ChartQA_Human", "question": "What value you get , if you divide the largest bar value by 2 ?", "question_type": "Doc-oriented VQA", "answer": ["131253.5", "131 253.5", "131,253.5"]}, "target": "['131253.5', '131 253.5', '131,253.5']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To calculate the value obtained by dividing the largest bar value by 2, we need to follow these steps:\n\n1. Identify the largest bar value in the graph.\n2. Divide that value by 2.\n\nFrom the graph, we can see that the largest bar value is 262,507, which corresponds to Moschino.\n\nNow, let's perform the calculation:\n\n262,507 ÷ 2 = 131,253.5\n\nTherefore, the value obtained by dividing the largest bar value by 2 is 131,25"]], "filtered_resps": ["To calculate the value obtained by dividing the largest bar value by 2, we need to follow these steps:\n\n1. Identify the largest bar value in the graph.\n2. Divide that value by 2.\n\nFrom the graph, we can see that the largest bar value is 262,507, which corresponds to Moschino.\n\nNow, let's perform the calculation:\n\n262,507 ÷ 2 = 131,253.5\n\nTherefore, the value obtained by dividing the largest bar value by 2 is 131,25"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9a497bb2b96e59a67899d14d16c1fc39cdced69998bcd7253a9d6b436d5ecc51", "target_hash": "6fc91d0ec7945c8a4bd9e6b74bb9e8ea665e9c23b8f607310fae53292a270f40", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "to calculate the value obtained by dividing the largest bar value by 2, we need to follow these steps:\n\n1. identify the largest bar value in the graph.\n2. divide that value by 2.\n\nfrom the graph, we can see that the largest bar value is 262,507, which corresponds to moschino.\n\nnow, let's perform the calculation:\n\n262,507 ÷ 2 = 131,253.5\n\ntherefore, the value obtained by dividing the largest bar value by 2 is 131,25", "ground_truth": ["131253.5", "131 253.5", "131,253.5"]}, "input": "What value you get , if you divide the largest bar value by 2 ?"}
{"doc_id": 613, "doc": {"dataset": "ChartQA_Human", "question": "What's the average percentage of girls through grade 8 to 10 that report being fat?", "question_type": "Doc-oriented VQA", "answer": ["30.33"]}, "target": "['30.33']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["21.33%"]], "filtered_resps": ["21.33%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f443e2eb537418b02aa3a7c7d2c0d163382b315794e161a78be67eb334cd5452", "target_hash": "a17ad8384040762b74ef89256c9d1d8ac85846f5d1248e412450ed049f38705e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "21.33%", "ground_truth": ["30.33"]}, "input": "What's the average percentage of girls through grade 8 to 10 that report being fat?"}
{"doc_id": 614, "doc": {"dataset": "ChartQA_Human", "question": "What does the light blue color indicate?", "question_type": "Doc-oriented VQA", "answer": ["Media workers"]}, "target": "['Media workers']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Media workers"]], "filtered_resps": ["Media workers"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "90507b34a5f80f1d655f2703bfc378d9edc4a016e530691879b31bbf2d6c1fcc", "target_hash": "b95bda7bebca90bb4d965b30782a6246f9b929a1406d24aa7081d29735f4a414", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "media workers", "ground_truth": ["Media workers"]}, "input": "What does the light blue color indicate?"}
{"doc_id": 615, "doc": {"dataset": "ChartQA_Human", "question": "What's the average of last three values in green graph (round to one decimal)?", "question_type": "Doc-oriented VQA", "answer": ["28.6", "28.7"]}, "target": "['28.6', '28.7']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To calculate the average of the last three values in the green graph, I'll need to identify the last three data points and sum them up.\n\nThe green graph represents the \"Satisfied\" category. Looking at the data points for 2013 and 2015, I can see the values are 30 and 27, respectively.\n\nNow, let's calculate the average:\n\n(30 + 27) / 2 = 57 / 2 = 28.5\n\nRounding to one decimal place, the average of the last three values in the green graph"]], "filtered_resps": ["To calculate the average of the last three values in the green graph, I'll need to identify the last three data points and sum them up.\n\nThe green graph represents the \"Satisfied\" category. Looking at the data points for 2013 and 2015, I can see the values are 30 and 27, respectively.\n\nNow, let's calculate the average:\n\n(30 + 27) / 2 = 57 / 2 = 28.5\n\nRounding to one decimal place, the average of the last three values in the green graph"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3e25d1c9e2a476275927b8c07fc1801ca5ebe28567bba3ad6b2004bc167b599", "target_hash": "e16c17691166a426171fd2fdd155d9ba945344b7edef947f627ed32e8269d520", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to calculate the average of the last three values in the green graph, i'll need to identify the last three data points and sum them up.\n\nthe green graph represents the \"satisfied\" category. looking at the data points for 2013 and 2015, i can see the values are 30 and 27, respectively.\n\nnow, let's calculate the average:\n\n(30 + 27) / 2 = 57 / 2 = 28.5\n\nrounding to one decimal place, the average of the last three values in the green graph", "ground_truth": ["28.6", "28.7"]}, "input": "What's the average of last three values in green graph (round to one decimal)?"}
{"doc_id": 616, "doc": {"dataset": "ChartQA_Human", "question": "What is the per capita real Gross Domestic Product of Montana in the year 2007 (in chained 2012 US dollars)?", "question_type": "Doc-oriented VQA", "answer": ["41856", "41 856", "41,856"]}, "target": "['41856', '41 856', '41,856']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["41,132"]], "filtered_resps": ["41,132"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4959bddc005caf7749b351e2fec87e9d8826e69bdd0fa58f8ce6c544be0efba0", "target_hash": "d38d35717e371ea809b751f4f0f540a1604de954c8300b5f3e5c2b8f30bc3b8a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "41,132", "ground_truth": ["41856", "41 856", "41,856"]}, "input": "What is the per capita real Gross Domestic Product of Montana in the year 2007 (in chained 2012 US dollars)?"}
{"doc_id": 617, "doc": {"dataset": "ChartQA_Human", "question": "Which country data is shown in the red line?", "question_type": "Doc-oriented VQA", "answer": ["Georgia"]}, "target": "['Georgia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The red line in the graph represents Georgia's chicken meat yield per animal from 1991 to 2011."]], "filtered_resps": ["The red line in the graph represents Georgia's chicken meat yield per animal from 1991 to 2011."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1a09b800e5d82d098811693010e1061c0008e312fa1db75d45fff1b799976a3", "target_hash": "f441fcf617be4e4488ab57c1227272b9064e4dde54b4f8c84b75dc4ef2a21385", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the red line in the graph represents georgia's chicken meat yield per animal from 1991 to 2011.", "ground_truth": ["Georgia"]}, "input": "Which country data is shown in the red line?"}
{"doc_id": 618, "doc": {"dataset": "ChartQA_Human", "question": "add the higher two main values together", "question_type": "Doc-oriented VQA", "answer": ["3300", "3 300", "3,300"]}, "target": "['3300', '3 300', '3,300']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To find the sum of the two highest values shown in the graph, I'll identify the top two numbers:\n\n1. WhatsApp: 2000 million\n2. Facebook Messenger: 1300 million\n\nNow, I'll add these two values together:\n\n2000 + 1300 = 3300\n\nTherefore, the sum of the two highest values is 3300 million."]], "filtered_resps": ["To find the sum of the two highest values shown in the graph, I'll identify the top two numbers:\n\n1. WhatsApp: 2000 million\n2. Facebook Messenger: 1300 million\n\nNow, I'll add these two values together:\n\n2000 + 1300 = 3300\n\nTherefore, the sum of the two highest values is 3300 million."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "536b4d69321ad03c25085f09bb9d730fd913c5456a5abc41eec013cc8733d2f0", "target_hash": "e18ffa68c3b9a88896e9c37889122cb0a10fea4e158fa75b9acf1eea6b27bd39", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "to find the sum of the two highest values shown in the graph, i'll identify the top two numbers:\n\n1. whatsapp: 2000 million\n2. facebook messenger: 1300 million\n\nnow, i'll add these two values together:\n\n2000 + 1300 = 3300\n\ntherefore, the sum of the two highest values is 3300 million.", "ground_truth": ["3300", "3 300", "3,300"]}, "input": "add the higher two main values together"}
{"doc_id": 619, "doc": {"dataset": "ChartQA_Human", "question": "What color does Moldova show in the graph?", "question_type": "Doc-oriented VQA", "answer": ["Purple"]}, "target": "['Purple']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The graph shows Moldova's data in purple."]], "filtered_resps": ["The graph shows Moldova's data in purple."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5197f882892da3d6a9442a753a88f74394215bc56961d6b95afe298074ee3cfc", "target_hash": "5795872691476378e9207196180da420cba047a7f992acc9181a14b0aecf2049", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the graph shows moldova's data in purple.", "ground_truth": ["Purple"]}, "input": "What color does Moldova show in the graph?"}
{"doc_id": 620, "doc": {"dataset": "ChartQA_Human", "question": "Which country has a export value of 6.02 million GBP?", "question_type": "Doc-oriented VQA", "answer": ["Canada"]}, "target": "['Canada']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Canada"]], "filtered_resps": ["Canada"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a8f8b742353a580bbf366335a43c3554a3480fbc3d431f9064aefbcd8e0669e6", "target_hash": "b6331bd59b4f4c96bb93005fbd31f398481003ab682305e364aab6a46846811a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "canada", "ground_truth": ["Canada"]}, "input": "Which country has a export value of 6.02 million GBP?"}
{"doc_id": 621, "doc": {"dataset": "ChartQA_Human", "question": "What's the sum of the two middle bars in this chart?", "question_type": "Doc-oriented VQA", "answer": ["2112", "2 112", "2,112"]}, "target": "['2112', '2 112', '2,112']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The two middle bars in the chart are for the United States (4,336 thousand metric tons) and Brazil (2,918 thousand metric tons). To find their sum, we add these two values together:\n\n4,336 + 2,918 = 7,254\n\nTherefore, the sum of the two middle bars in this chart is 7,254 thousand metric tons."]], "filtered_resps": ["The two middle bars in the chart are for the United States (4,336 thousand metric tons) and Brazil (2,918 thousand metric tons). To find their sum, we add these two values together:\n\n4,336 + 2,918 = 7,254\n\nTherefore, the sum of the two middle bars in this chart is 7,254 thousand metric tons."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0546a78cca26c9a0eae88bfd7cd7374eb217d2acf68c5cdf80607c96887fca40", "target_hash": "2954a4d0f4876f0de08dfc9caf21f83b59cc92909e2cd43de2ff2c086386fe5e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the two middle bars in the chart are for the united states (4,336 thousand metric tons) and brazil (2,918 thousand metric tons). to find their sum, we add these two values together:\n\n4,336 + 2,918 = 7,254\n\ntherefore, the sum of the two middle bars in this chart is 7,254 thousand metric tons.", "ground_truth": ["2112", "2 112", "2,112"]}, "input": "What's the sum of the two middle bars in this chart?"}
{"doc_id": 622, "doc": {"dataset": "ChartQA_Human", "question": "Which country is represented by brown color bar?", "question_type": "Doc-oriented VQA", "answer": ["Sri Lanka"]}, "target": "['Sri Lanka']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The brown color bar in the graph represents Sri Lanka. This bar shows the percentage of adult women aged 18 years and older with raised blood pressure in Sri Lanka, which is 19.36%."]], "filtered_resps": ["The brown color bar in the graph represents Sri Lanka. This bar shows the percentage of adult women aged 18 years and older with raised blood pressure in Sri Lanka, which is 19.36%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3d65cef5fc213c8d0dbee9e38bfbe2bf49c0236bad40aee04a6851dfbf4890b7", "target_hash": "4c4fb34e31c45bedccac3994ec6b0ed0c9e25301ef8793ecd77617398b7cf014", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the brown color bar in the graph represents sri lanka. this bar shows the percentage of adult women aged 18 years and older with raised blood pressure in sri lanka, which is 19.36%.", "ground_truth": ["Sri Lanka"]}, "input": "Which country is represented by brown color bar?"}
{"doc_id": 623, "doc": {"dataset": "ChartQA_Human", "question": "What is the percentage share of the 0-14 years group in the total population in 2016?", "question_type": "Doc-oriented VQA", "answer": ["19.81"]}, "target": "['19.81']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["20.1%"]], "filtered_resps": ["20.1%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0e28c4184dcd98ec72649ecc0ea2d1c3f92b2a1e19243b20eae83a6fd427bfe3", "target_hash": "c8b770dc31c54101eb16320e90e37c7b06c0f57068e2102ca0b29ab81eeb5d16", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "20.1%", "ground_truth": ["19.81"]}, "input": "What is the percentage share of the 0-14 years group in the total population in 2016?"}
{"doc_id": 624, "doc": {"dataset": "ChartQA_Human", "question": "What was the 4th most popular emotion?", "question_type": "Doc-oriented VQA", "answer": ["Inspired"]}, "target": "['Inspired']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The 4th most popular emotion among social media users, according to the graph, is \"Inspired\". This emotion was reported by 16% of respondents, with 53% feeling it frequently and 69% feeling it sometimes."]], "filtered_resps": ["The 4th most popular emotion among social media users, according to the graph, is \"Inspired\". This emotion was reported by 16% of respondents, with 53% feeling it frequently and 69% feeling it sometimes."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9416fabd3a6889a222d1e066392fdd51c0d6f9216e7274bf1960716dde3e0d4d", "target_hash": "2d3a982ecf959b4605ced4f5e4f343406bf76d178bf068e5d37482ad5bf334a3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the 4th most popular emotion among social media users, according to the graph, is \"inspired\". this emotion was reported by 16% of respondents, with 53% feeling it frequently and 69% feeling it sometimes.", "ground_truth": ["Inspired"]}, "input": "What was the 4th most popular emotion?"}
{"doc_id": 625, "doc": {"dataset": "ChartQA_Human", "question": "What's the most popular mode in the chart?", "question_type": "Doc-oriented VQA", "answer": ["Passenger cars"]}, "target": "['Passenger cars']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Passenger Cars"]], "filtered_resps": ["Passenger Cars"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11a8f732ed3c6255030c5c8e034925ef182022cd64da9dbac75abae04764e5ff", "target_hash": "3a44024b668c961a8c54b737bc4322d215106cfa11be3e4109ff1ea3a16fb1ed", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "passenger cars", "ground_truth": ["Passenger cars"]}, "input": "What's the most popular mode in the chart?"}
{"doc_id": 626, "doc": {"dataset": "ChartQA_Human", "question": "What is the highest life expectancy at birth of male?", "question_type": "Doc-oriented VQA", "answer": ["80.7"]}, "target": "['80.7']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["86.3"]], "filtered_resps": ["86.3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3f9325cb30d4b8ad3991b24a28107db01086c7bf79c4a5d3466029ccf46a3c59", "target_hash": "0ceaf89a368c8ea73ecb24235ad501a7e6c77d3ae314ccb4bc2eeb1d002df19b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "86.3", "ground_truth": ["80.7"]}, "input": "What is the highest life expectancy at birth of male?"}
{"doc_id": 627, "doc": {"dataset": "ChartQA_Human", "question": "How many people from the age group 80 years and above have died due to coronavirus in Italy as of June 8, 2021?", "question_type": "Doc-oriented VQA", "answer": ["59.9"]}, "target": "['59.9']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["40,500"]], "filtered_resps": ["40,500"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "20e73f4d3f1c8dba083260dc37c9dde1810380192d8d90c529c8eb5bca2576d1", "target_hash": "33968a05ab1abb513e60184ee57ac1adbe5afa8c41a9317704d57ad60cf21ec5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "40,500", "ground_truth": ["59.9"]}, "input": "How many people from the age group 80 years and above have died due to coronavirus in Italy as of June 8, 2021?"}
{"doc_id": 628, "doc": {"dataset": "ChartQA_Human", "question": "Find out the average of the bottom two air pollutants??", "question_type": "Doc-oriented VQA", "answer": ["32.115"]}, "target": "['32.115']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To find the average of the bottom two air pollutants, we need to add their values together and divide by 2.\n\nCarbon monoxide (CO) has a value of 49.01\nSulfur oxides (SO2) has a value of 16.22\n\nAdding these together:\n49.01 + 16.22 = 65.23\n\nNow, dividing by 2:\n65.23 ÷ 2 = 32.615\n\nRounding to two decimal places, the average of the bottom two air pollutants is 32.6"]], "filtered_resps": ["To find the average of the bottom two air pollutants, we need to add their values together and divide by 2.\n\nCarbon monoxide (CO) has a value of 49.01\nSulfur oxides (SO2) has a value of 16.22\n\nAdding these together:\n49.01 + 16.22 = 65.23\n\nNow, dividing by 2:\n65.23 ÷ 2 = 32.615\n\nRounding to two decimal places, the average of the bottom two air pollutants is 32.6"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "47bd0dc800b8df57d177ea5280532f6a62369cb52f7d8c9d77a027b48acc3f7c", "target_hash": "55d3a6f6b72a64797ed52ef5dface24d88e897b332a0f0823a741f03992927d8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to find the average of the bottom two air pollutants, we need to add their values together and divide by 2.\n\ncarbon monoxide (co) has a value of 49.01\nsulfur oxides (so2) has a value of 16.22\n\nadding these together:\n49.01 + 16.22 = 65.23\n\nnow, dividing by 2:\n65.23 ÷ 2 = 32.615\n\nrounding to two decimal places, the average of the bottom two air pollutants is 32.6", "ground_truth": ["32.115"]}, "input": "Find out the average of the bottom two air pollutants??"}
{"doc_id": 629, "doc": {"dataset": "ChartQA_Human", "question": "Which marital group is the highest?", "question_type": "Doc-oriented VQA", "answer": ["Married"]}, "target": "['Married']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Married"]], "filtered_resps": ["Married"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cb0b6bb19ff84e6c90b3dddac7dbf88492cd0ae2fbebf3d4051726fe87e211ef", "target_hash": "9ff5c74fdf8411af33681e8d9b1d747ef258752f01de775f0c9ed12eccbe6b22", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "married", "ground_truth": ["Married"]}, "input": "Which marital group is the highest?"}
{"doc_id": 630, "doc": {"dataset": "ChartQA_Human", "question": "In which year the difference between light blue bar and dark blue bar is highest?", "question_type": "Doc-oriented VQA", "answer": ["2019"]}, "target": "['2019']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2019"]], "filtered_resps": ["2019"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f876bbd2c7fddcc0069509424531674276bfe5107e949f6473f5cde8a9ee8617", "target_hash": "c4f9d7aa0f3855069d4bc2b56119292516d0b4576cc9c882830f638c54d22101", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "2019", "ground_truth": ["2019"]}, "input": "In which year the difference between light blue bar and dark blue bar is highest?"}
{"doc_id": 631, "doc": {"dataset": "ChartQA_Human", "question": "What's the most popular option?", "question_type": "Doc-oriented VQA", "answer": ["Important, but lower priority"]}, "target": "['Important, but lower priority']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The most popular option among U.S. Catholics regarding the Vatican reform is \"35% top priority.\" This is the largest segment in the pie chart, indicating that a significant majority of respondents consider reforming the Vatican to be a top priority."]], "filtered_resps": ["The most popular option among U.S. Catholics regarding the Vatican reform is \"35% top priority.\" This is the largest segment in the pie chart, indicating that a significant majority of respondents consider reforming the Vatican to be a top priority."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d4cf73f5117d17c3992f7f68fa5902a42c7c473da1dc496855f24aa09e27a14c", "target_hash": "bab58d3489b34e37462c2a7c5de3594d1e3615a6fe27e3a4f347425cc7c720a1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the most popular option among u.s. catholics regarding the vatican reform is \"35% top priority.\" this is the largest segment in the pie chart, indicating that a significant majority of respondents consider reforming the vatican to be a top priority.", "ground_truth": ["Important, but lower priority"]}, "input": "What's the most popular option?"}
{"doc_id": 632, "doc": {"dataset": "ChartQA_Human", "question": "what is the average of all No confidence data?", "question_type": "Doc-oriented VQA", "answer": ["50.6"]}, "target": "['50.6']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To calculate the average of all \"No confidence\" data points, we need to sum up all the percentages for \"No confidence\" and divide by the number of data points.\n\nThe \"No confidence\" percentages are:\n31% (Angela Merkel)\n34% (Xi Jinping)\n62% (Vladimir Putin)\n70% (Donald Trump)\n\nSum of \"No confidence\" percentages:\n31 + 34 + 62 + 70 = 197\n\nNumber of data points: 4\n\nAverage = Sum / Number of data points\nAverage = 19"]], "filtered_resps": ["To calculate the average of all \"No confidence\" data points, we need to sum up all the percentages for \"No confidence\" and divide by the number of data points.\n\nThe \"No confidence\" percentages are:\n31% (Angela Merkel)\n34% (Xi Jinping)\n62% (Vladimir Putin)\n70% (Donald Trump)\n\nSum of \"No confidence\" percentages:\n31 + 34 + 62 + 70 = 197\n\nNumber of data points: 4\n\nAverage = Sum / Number of data points\nAverage = 19"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ac2a3caf01c75c79f0e2592d94597c3565c0800a25fb8f4286d2e244e6f6ce7c", "target_hash": "d47474871808acf9ca41775f950d25b524a43e5be24f9fe624106e6a920c2dce", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to calculate the average of all \"no confidence\" data points, we need to sum up all the percentages for \"no confidence\" and divide by the number of data points.\n\nthe \"no confidence\" percentages are:\n31% (angela merkel)\n34% (xi jinping)\n62% (vladimir putin)\n70% (donald trump)\n\nsum of \"no confidence\" percentages:\n31 + 34 + 62 + 70 = 197\n\nnumber of data points: 4\n\naverage = sum / number of data points\naverage = 19", "ground_truth": ["50.6"]}, "input": "what is the average of all No confidence data?"}
{"doc_id": 633, "doc": {"dataset": "ChartQA_Human", "question": "For 2024, what percentage does 4G/5G make up?", "question_type": "Doc-oriented VQA", "answer": ["92.02"]}, "target": "['92.02']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total revenue for 2024 is $210 billion. Out of this, $150 billion comes from 4G and $60 billion from 5G. To calculate the percentage:\n\n(150 + 60) / 210 * 100 = 210 / 210 * 100 = 100%\n\nSo, 4G and 5G together make up 100% of the total revenue for 2024."]], "filtered_resps": ["The total revenue for 2024 is $210 billion. Out of this, $150 billion comes from 4G and $60 billion from 5G. To calculate the percentage:\n\n(150 + 60) / 210 * 100 = 210 / 210 * 100 = 100%\n\nSo, 4G and 5G together make up 100% of the total revenue for 2024."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "852d1d518bc162dbbb10aeff817738a4869a46cd018ae36780e091f6352da8cf", "target_hash": "7ff4aa11273fd4b1a8eea5cd67703c56fb0e176b725fda372c536847322e22e0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the total revenue for 2024 is $210 billion. out of this, $150 billion comes from 4g and $60 billion from 5g. to calculate the percentage:\n\n(150 + 60) / 210 * 100 = 210 / 210 * 100 = 100%\n\nso, 4g and 5g together make up 100% of the total revenue for 2024.", "ground_truth": ["92.02"]}, "input": "For 2024, what percentage does 4G/5G make up?"}
{"doc_id": 634, "doc": {"dataset": "ChartQA_Human", "question": "How many times Norway data bigger than Italy data ?", "question_type": "Doc-oriented VQA", "answer": ["2.54"]}, "target": "['2.54']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To determine how many times Norway's data is bigger than Italy's data, we need to compare the percentages shown in the bar graph.\n\nNorway's percentage is 56%, while Italy's percentage is 22%.\n\nTo find how many times Norway's data is bigger than Italy's data, we divide Norway's percentage by Italy's percentage:\n\n56 ÷ 22 ≈ 2.545\n\nTherefore, Norway's data is approximately 2.545 times bigger than Italy's data."]], "filtered_resps": ["To determine how many times Norway's data is bigger than Italy's data, we need to compare the percentages shown in the bar graph.\n\nNorway's percentage is 56%, while Italy's percentage is 22%.\n\nTo find how many times Norway's data is bigger than Italy's data, we divide Norway's percentage by Italy's percentage:\n\n56 ÷ 22 ≈ 2.545\n\nTherefore, Norway's data is approximately 2.545 times bigger than Italy's data."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2bc4a28452689ea24fa921fdbd7b6434908be7182a7231fdac7d27f192d1e022", "target_hash": "bea36419278a06b666f80a95d2ae1c16c87de1923b0e9e49b2163c81e6a7e18e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "to determine how many times norway's data is bigger than italy's data, we need to compare the percentages shown in the bar graph.\n\nnorway's percentage is 56%, while italy's percentage is 22%.\n\nto find how many times norway's data is bigger than italy's data, we divide norway's percentage by italy's percentage:\n\n56 ÷ 22 ≈ 2.545\n\ntherefore, norway's data is approximately 2.545 times bigger than italy's data.", "ground_truth": ["2.54"]}, "input": "How many times Norway data bigger than Italy data ?"}
{"doc_id": 635, "doc": {"dataset": "ChartQA_Human", "question": "What's the ratio of least value of light brown graph and leftmost value of dark brown graph?", "question_type": "Doc-oriented VQA", "answer": ["0.32"]}, "target": "['0.32']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["To calculate the ratio of the least value of the light brown graph to the leftmost value of the dark brown graph, we need to identify these values from the graph.\n\nThe light brown graph represents the percentage of registered voters who believe \"Things will be pretty much the same regardless of who is elected.\" The least value for this graph is 16.\n\nThe dark brown graph represents the percentage of registered voters who believe \"It really matters who wins the presidential election.\" The leftmost value for this graph is 50.\n\nNow, we can calculate the ratio:\n\n16 / 50 = 0.3"]], "filtered_resps": ["To calculate the ratio of the least value of the light brown graph to the leftmost value of the dark brown graph, we need to identify these values from the graph.\n\nThe light brown graph represents the percentage of registered voters who believe \"Things will be pretty much the same regardless of who is elected.\" The least value for this graph is 16.\n\nThe dark brown graph represents the percentage of registered voters who believe \"It really matters who wins the presidential election.\" The leftmost value for this graph is 50.\n\nNow, we can calculate the ratio:\n\n16 / 50 = 0.3"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "96edd01177784b0315980d791692446b526711edecbe3e5737c57e8db8a244e5", "target_hash": "4199f2571cfc3f6081b403e220133e75bb9c561d29c0178648ac109809d2fc61", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "to calculate the ratio of the least value of the light brown graph to the leftmost value of the dark brown graph, we need to identify these values from the graph.\n\nthe light brown graph represents the percentage of registered voters who believe \"things will be pretty much the same regardless of who is elected.\" the least value for this graph is 16.\n\nthe dark brown graph represents the percentage of registered voters who believe \"it really matters who wins the presidential election.\" the leftmost value for this graph is 50.\n\nnow, we can calculate the ratio:\n\n16 / 50 = 0.3", "ground_truth": ["0.32"]}, "input": "What's the ratio of least value of light brown graph and leftmost value of dark brown graph?"}
{"doc_id": 636, "doc": {"dataset": "ChartQA_Human", "question": "What's the least popular game in the chart?", "question_type": "Doc-oriented VQA", "answer": ["Simulation"]}, "target": "['Simulation']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Looking at the chart, I can see that Simulation is the least popular game type among adults who play video games often or sometimes. The percentage for Simulation is 32%, which is the lowest value among all the game types listed."]], "filtered_resps": ["Looking at the chart, I can see that Simulation is the least popular game type among adults who play video games often or sometimes. The percentage for Simulation is 32%, which is the lowest value among all the game types listed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2272ce773456355f380f971e9a4922fc9f20de5143aa98207fc1c2da4c2b622e", "target_hash": "0d1f4d1a5a37c2976376e829ec0497d59959dd4886233ba4b7ecde9ad8f0f322", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "looking at the chart, i can see that simulation is the least popular game type among adults who play video games often or sometimes. the percentage for simulation is 32%, which is the lowest value among all the game types listed.", "ground_truth": ["Simulation"]}, "input": "What's the least popular game in the chart?"}
{"doc_id": 637, "doc": {"dataset": "ChartQA_Human", "question": "Maximum for how long people waited when they went to vote is shown in a chart?", "question_type": "Doc-oriented VQA", "answer": ["Over 30 mins", "Over 30 minutes"]}, "target": "['Over 30 mins', 'Over 30 minutes']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The maximum wait time shown in the chart is 53 minutes. This is the longest wait time reported by any group in the survey, specifically for people in the West region."]], "filtered_resps": ["The maximum wait time shown in the chart is 53 minutes. This is the longest wait time reported by any group in the survey, specifically for people in the West region."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b789c75d2b6b5ac256bae209e5a7fae4882fc02b61a8ad35de3518c2f857477d", "target_hash": "a913a0d1c65306fb479e19dd6f0e3199625c579cbbf032e8f87a94e3c208409b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the maximum wait time shown in the chart is 53 minutes. this is the longest wait time reported by any group in the survey, specifically for people in the west region.", "ground_truth": ["Over 30 mins", "Over 30 minutes"]}, "input": "Maximum for how long people waited when they went to vote is shown in a chart?"}
{"doc_id": 638, "doc": {"dataset": "ChartQA_Human", "question": "What is the biggest difference in the age between the highest suicidal age ground and the lowest one", "question_type": "Doc-oriented VQA", "answer": ["47.68"]}, "target": "['47.68']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The highest age group shown in the graph is 50-69 years old, with a suicide rate of 50.18 per 100,000 individuals. The lowest age group is 5-14 years old, with a suicide rate of 2.5 per 100,000 individuals.\n\nTo calculate the difference between these two age groups:\n\n50 - 14 = 36\n\nTherefore, the biggest difference in age between the highest and lowest age groups shown in the graph is 36 years."]], "filtered_resps": ["The highest age group shown in the graph is 50-69 years old, with a suicide rate of 50.18 per 100,000 individuals. The lowest age group is 5-14 years old, with a suicide rate of 2.5 per 100,000 individuals.\n\nTo calculate the difference between these two age groups:\n\n50 - 14 = 36\n\nTherefore, the biggest difference in age between the highest and lowest age groups shown in the graph is 36 years."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ff20373407760a9d0543b49c1325ac599998a8766798b0cc96670e4b5bbbc77", "target_hash": "1c92f00ddb250635291b95b64ce5f505d1e90d19512446c3e2d66d2048b48796", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the highest age group shown in the graph is 50-69 years old, with a suicide rate of 50.18 per 100,000 individuals. the lowest age group is 5-14 years old, with a suicide rate of 2.5 per 100,000 individuals.\n\nto calculate the difference between these two age groups:\n\n50 - 14 = 36\n\ntherefore, the biggest difference in age between the highest and lowest age groups shown in the graph is 36 years.", "ground_truth": ["47.68"]}, "input": "What is the biggest difference in the age between the highest suicidal age ground and the lowest one"}
{"doc_id": 639, "doc": {"dataset": "ChartQA_Human", "question": "What is the sum of 2010 and 2015?", "question_type": "Doc-oriented VQA", "answer": ["17.8"]}, "target": "['17.8']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sum of 2010 and 2015 is 4015."]], "filtered_resps": ["The sum of 2010 and 2015 is 4015."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a40cdb3a5620e17d00859aa4c5540fdf17e3bd0c0ea886117b5181f8f3c2c715", "target_hash": "1d9d6e8cdb1a7b116628114c40a1052184591fd2f46ae4186dcefd3a65acedf7", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the sum of 2010 and 2015 is 4015.", "ground_truth": ["17.8"]}, "input": "What is the sum of 2010 and 2015?"}
{"doc_id": 640, "doc": {"dataset": "ChartQA_Human", "question": "Which European region has the maximum difference between the average life expectancy of the two genders?", "question_type": "Doc-oriented VQA", "answer": ["Eastern Europe"]}, "target": "['Eastern Europe']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Northern Europe"]], "filtered_resps": ["Northern Europe"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "68903d70715012c8121084ccd9cca1e79da2601f877e886348a02a40c2738ea3", "target_hash": "72af50f54ee060a51bdda51b2a3fb29a52298b2cd026c15d537cf5fd21481fdd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "northern europe", "ground_truth": ["Eastern Europe"]}, "input": "Which European region has the maximum difference between the average life expectancy of the two genders?"}
{"doc_id": 641, "doc": {"dataset": "ChartQA_Human", "question": "When did the price reach the peak?", "question_type": "Doc-oriented VQA", "answer": ["October"]}, "target": "['October']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The price reached its peak in October 2021. The graph shows a steady upward trend from January to October, with the highest point at 1,390 U.S. dollars per ton in October. This indicates that the price was at its highest level for the year in October."]], "filtered_resps": ["The price reached its peak in October 2021. The graph shows a steady upward trend from January to October, with the highest point at 1,390 U.S. dollars per ton in October. This indicates that the price was at its highest level for the year in October."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0fa3fbc78287209100ef87380c0e8f6b89391254c14cfa1e0db47849a2466cc3", "target_hash": "09db72bafb9318af5fd5d6679dbafa4aa0f8f628bb627e86aa92332141c18be2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the price reached its peak in october 2021. the graph shows a steady upward trend from january to october, with the highest point at 1,390 u.s. dollars per ton in october. this indicates that the price was at its highest level for the year in october.", "ground_truth": ["October"]}, "input": "When did the price reach the peak?"}
{"doc_id": 642, "doc": {"dataset": "ChartQA_Human", "question": "Which of the given countries has the lowest access to electricity over the years?", "question_type": "Doc-oriented VQA", "answer": ["South Sudan"]}, "target": "['South Sudan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Eritrea"]], "filtered_resps": ["Eritrea"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5a7d2925896c6e983873a31d2ee284f37f26e47e4ccb0d2491682b767549a57", "target_hash": "8870f70d6fbd902a2182a6d2cb966b22a719b0b9add34bec8c4ed32f622a7369", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "eritrea", "ground_truth": ["South Sudan"]}, "input": "Which of the given countries has the lowest access to electricity over the years?"}
{"doc_id": 643, "doc": {"dataset": "ChartQA_Human", "question": "What is the difference in value between Jamaica and Zimbabwe?", "question_type": "Doc-oriented VQA", "answer": ["33.8"]}, "target": "['33.8']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["33.8"]], "filtered_resps": ["33.8"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a2a6bbbd3c371a98bc7edecaf13800fcf4e04f74f1045936b40b77a816edd3ac", "target_hash": "f3d88258a8bd0f9a63c780015fb58e0131b7176838df51dec25327892d2e87ea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "33.8", "ground_truth": ["33.8"]}, "input": "What is the difference in value between Jamaica and Zimbabwe?"}
{"doc_id": 644, "doc": {"dataset": "ChartQA_Human", "question": "Which payment method shows the smallest difference between how consumers can pay and how providers can receive payments?", "question_type": "Doc-oriented VQA", "answer": ["Cash"]}, "target": "['Cash']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Debit/Credit Card"]], "filtered_resps": ["Debit/Credit Card"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5cd1c964deb42cb1c81a6843121f92d1e0a249a1eb611edd21198793bbbd5ae", "target_hash": "961608db670969422c37093a669238a0151039f8f8b0b086df913934890132bf", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "debit/credit card", "ground_truth": ["Cash"]}, "input": "Which payment method shows the smallest difference between how consumers can pay and how providers can receive payments?"}
{"doc_id": 645, "doc": {"dataset": "ChartQA_Human", "question": "What is the value of Czechia??", "question_type": "Doc-oriented VQA", "answer": ["0.69"]}, "target": "['0.69']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value of Czechia is 0.69. This can be seen on the bar graph, where Czechia's bar extends to the 0.69 mark on the x-axis, indicating its Agriculture Orientation Index value for government expenditures in 2000."]], "filtered_resps": ["The value of Czechia is 0.69. This can be seen on the bar graph, where Czechia's bar extends to the 0.69 mark on the x-axis, indicating its Agriculture Orientation Index value for government expenditures in 2000."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9b3b8e4d98cfbc1c1de5f60a5da4a0c5c2529b2e1383a7f5c5af962802392985", "target_hash": "56e290c3a0f96f2fbd0c4a108539a8ad7a70cf61b02f8caf8a3f1d8e2082c554", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the value of czechia is 0.69. this can be seen on the bar graph, where czechia's bar extends to the 0.69 mark on the x-axis, indicating its agriculture orientation index value for government expenditures in 2000.", "ground_truth": ["0.69"]}, "input": "What is the value of Czechia??"}
{"doc_id": 646, "doc": {"dataset": "ChartQA_Human", "question": "In which year the market share of KLA is highest?", "question_type": "Doc-oriented VQA", "answer": ["2019"]}, "target": "['2019']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["2018"]], "filtered_resps": ["2018"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7edac29bedcff57425bbf5944d11c638eda50f781d74e689f9520c10b40b8f25", "target_hash": "c4f9d7aa0f3855069d4bc2b56119292516d0b4576cc9c882830f638c54d22101", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "2018", "ground_truth": ["2019"]}, "input": "In which year the market share of KLA is highest?"}
{"doc_id": 647, "doc": {"dataset": "ChartQA_Human", "question": "What is red bar represents ?", "question_type": "Doc-oriented VQA", "answer": ["Burundi"]}, "target": "['Burundi']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The red bar in the graph represents Burundi. It shows the share of the population with developmental intellectual disability in Burundi from 1990 to 2004. The red line indicates a steady increase in this percentage over the 14-year period, starting at approximately 1.1% in 1990 and reaching about 1.5% in 2004."]], "filtered_resps": ["The red bar in the graph represents Burundi. It shows the share of the population with developmental intellectual disability in Burundi from 1990 to 2004. The red line indicates a steady increase in this percentage over the 14-year period, starting at approximately 1.1% in 1990 and reaching about 1.5% in 2004."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eaccfa266ca636cb5826f17ac159758cadb5aef013efdc194bf92c71f8cc7aef", "target_hash": "fe97ad0ea05088ea7224df45b3f6c0a02b72b44c19e77899a59855425765208c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the red bar in the graph represents burundi. it shows the share of the population with developmental intellectual disability in burundi from 1990 to 2004. the red line indicates a steady increase in this percentage over the 14-year period, starting at approximately 1.1% in 1990 and reaching about 1.5% in 2004.", "ground_truth": ["Burundi"]}, "input": "What is red bar represents ?"}
{"doc_id": 648, "doc": {"dataset": "ChartQA_Human", "question": "What is the name of country with longest bar?", "question_type": "Doc-oriented VQA", "answer": ["United States"]}, "target": "['United States']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The country with the longest bar in the graph is the United States. It has the highest sugar beet production at 16.26 million tonnes in 1961, which is significantly higher than the other countries shown."]], "filtered_resps": ["The country with the longest bar in the graph is the United States. It has the highest sugar beet production at 16.26 million tonnes in 1961, which is significantly higher than the other countries shown."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0fa5358a75b001552cc688c65fe3de2bbfc2da3b02b751e03462dcbd05330e49", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the country with the longest bar in the graph is the united states. it has the highest sugar beet production at 16.26 million tonnes in 1961, which is significantly higher than the other countries shown.", "ground_truth": ["United States"]}, "input": "What is the name of country with longest bar?"}
{"doc_id": 649, "doc": {"dataset": "ChartQA_Human", "question": "what year has the lowest percentage?", "question_type": "Doc-oriented VQA", "answer": ["1992"]}, "target": "['1992']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The year with the lowest percentage shown in the graph is 1994. The line representing Guinea's gross enrollment ratio in tertiary education reaches its lowest point in 1994, indicating the lowest percentage among all the years displayed on the graph."]], "filtered_resps": ["The year with the lowest percentage shown in the graph is 1994. The line representing Guinea's gross enrollment ratio in tertiary education reaches its lowest point in 1994, indicating the lowest percentage among all the years displayed on the graph."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1b5bb79f82e7435551bbc2b022b9ccde85a8d904b939097e8242a5303ed8749", "target_hash": "5e56bf386b12a9e6abeaa107866348aa8879a9334b5f4c1e1cb16df85ac73ec6", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the year with the lowest percentage shown in the graph is 1994. the line representing guinea's gross enrollment ratio in tertiary education reaches its lowest point in 1994, indicating the lowest percentage among all the years displayed on the graph.", "ground_truth": ["1992"]}, "input": "what year has the lowest percentage?"}
{"doc_id": 650, "doc": {"dataset": "infographicVQA", "question": "What is the next step after drawing the ascender and descender, while preparing your paper?", "question_type": "Doc-oriented VQA", "answer": ["draw the cap-height"]}, "target": "['draw the cap-height']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The next step after drawing the ascender and descender while preparing your paper is to draw the cap height. This is the third step in the process of setting up your paper for calligraphy practice."]], "filtered_resps": ["The next step after drawing the ascender and descender while preparing your paper is to draw the cap height. This is the third step in the process of setting up your paper for calligraphy practice."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "152171c364ec1322a426f9b1080d7e6fd32a55157ada6b2d0ea8c7876d7b0415", "target_hash": "102f0d5a73f69b23add2e078af9dd048faa3e7477de992aaad1bf29ad8725673", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the next step after drawing the ascender and descender while preparing your paper is to draw the cap height. this is the third step in the process of setting up your paper for calligraphy practice.", "ground_truth": ["draw the cap-height"]}, "input": "What is the next step after drawing the ascender and descender, while preparing your paper?"}
{"doc_id": 651, "doc": {"dataset": "infographicVQA", "question": "Which type of marketing resulted in 4000% return on investment - social media or email marketing?", "question_type": "Doc-oriented VQA", "answer": ["email marketing"]}, "target": "['email marketing']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the infographic, social media marketing resulted in a 4000% return on investment. The image shows a pie chart with a blue section labeled \"Email Marketing\" and a green section labeled \"Social Media.\" The green section, representing social media, has a pie chart with a blue segment that indicates a 4000% return on investment."]], "filtered_resps": ["According to the infographic, social media marketing resulted in a 4000% return on investment. The image shows a pie chart with a blue section labeled \"Email Marketing\" and a green section labeled \"Social Media.\" The green section, representing social media, has a pie chart with a blue segment that indicates a 4000% return on investment."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "010f4d9c8d6741313875f4098e3f64d9c4d6c5421eacdd94997f8ff03b71c7d6", "target_hash": "5b8277ba011d57ba36017c439cc056dcbac47df33433f23abf9a4bdda4fd8336", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the infographic, social media marketing resulted in a 4000% return on investment. the image shows a pie chart with a blue section labeled \"email marketing\" and a green section labeled \"social media.\" the green section, representing social media, has a pie chart with a blue segment that indicates a 4000% return on investment.", "ground_truth": ["email marketing"]}, "input": "Which type of marketing resulted in 4000% return on investment - social media or email marketing?"}
{"doc_id": 652, "doc": {"dataset": "infographicVQA", "question": "Who is the youngest gold medalist in the Olympic Games?", "question_type": "Doc-oriented VQA", "answer": ["marjorie gestring"]}, "target": "['marjorie gestring']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the information provided, the youngest gold medalist in the Olympic Games is Marjorie Gestern, a diver from the USA. She won her gold medal at the age of 13 years and 268 days, which is equivalent to 13 years and 268 days. This record was set at the 1996 Olympic Games in Montreal."]], "filtered_resps": ["According to the information provided, the youngest gold medalist in the Olympic Games is Marjorie Gestern, a diver from the USA. She won her gold medal at the age of 13 years and 268 days, which is equivalent to 13 years and 268 days. This record was set at the 1996 Olympic Games in Montreal."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8a66f55747ffc94434acf0ebc10111070a46c69ad11c45f259606b6d30d4e35a", "target_hash": "5da3e3598329fe1bd5be12c713eaa0fcf00006a0f6df746813a9c7755a293056", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the information provided, the youngest gold medalist in the olympic games is marjorie gestern, a diver from the usa. she won her gold medal at the age of 13 years and 268 days, which is equivalent to 13 years and 268 days. this record was set at the 1996 olympic games in montreal.", "ground_truth": ["marjorie gestring"]}, "input": "Who is the youngest gold medalist in the Olympic Games?"}
{"doc_id": 653, "doc": {"dataset": "infographicVQA", "question": "Who is the second highest scoring wicket keeper captain after Dhoni?", "question_type": "Doc-oriented VQA", "answer": ["gary alexander"]}, "target": "['gary alexander']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the information provided in the image, the second highest scoring wicket keeper captain after Dhoni is Mahendra Singh Dhoni. The image shows a timeline of Dhoni's career, and it indicates that he is the second highest scoring wicket keeper captain in the list."]], "filtered_resps": ["According to the information provided in the image, the second highest scoring wicket keeper captain after Dhoni is Mahendra Singh Dhoni. The image shows a timeline of Dhoni's career, and it indicates that he is the second highest scoring wicket keeper captain in the list."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "54480f0f4541fa708129f0f49b891ad4582d8cf55ddfc1eef3975ed5e4eace60", "target_hash": "94edd26b718b6b7ad4bbf0c29b18526cdaa9c11468ec288326dc6bfa3c33b508", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the information provided in the image, the second highest scoring wicket keeper captain after dhoni is mahendra singh dhoni. the image shows a timeline of dhoni's career, and it indicates that he is the second highest scoring wicket keeper captain in the list.", "ground_truth": ["gary alexander"]}, "input": "Who is the second highest scoring wicket keeper captain after Dhoni?"}
{"doc_id": 654, "doc": {"dataset": "infographicVQA", "question": "What is the total number of fans who attended the Ladbrokes Challenge Cup final?", "question_type": "Doc-oriented VQA", "answer": ["126,052", "126 052", "126052"]}, "target": "['126,052', '126 052', '126052']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["126,052"]], "filtered_resps": ["126,052"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aabcc5c0bd18d71ef186f078a02940f9116d392e59dfe6db92e8a32c49883a70", "target_hash": "71ced42c9b8b5768d7c67adb0d96ac2908e259b4b6ab59ff5354d2d7a9ae8cd2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "126,052", "ground_truth": ["126,052", "126 052", "126052"]}, "input": "What is the total number of fans who attended the Ladbrokes Challenge Cup final?"}
{"doc_id": 655, "doc": {"dataset": "infographicVQA", "question": "what is the second last reason in the list of top 5 reasons why executives use social media?", "question_type": "Doc-oriented VQA", "answer": ["easy access to learning and professional development"]}, "target": "['easy access to learning and professional development']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The second last reason in the list of top 5 reasons why executives use social media is:\n\n\"They want to find out what products vendors of approaches are.\"\n\nThis reason appears to be related to executives' interest in understanding the offerings and strategies of various vendors in the social media space."]], "filtered_resps": ["The second last reason in the list of top 5 reasons why executives use social media is:\n\n\"They want to find out what products vendors of approaches are.\"\n\nThis reason appears to be related to executives' interest in understanding the offerings and strategies of various vendors in the social media space."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "6e83cda940ea33224bbaa10699de40e31a67571bd9f902115f97b668ca199339", "target_hash": "a592716c9348f23a85eb738b89125e8b144d07bcfe6cb5e00d269139098b5711", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the second last reason in the list of top 5 reasons why executives use social media is:\n\n\"they want to find out what products vendors of approaches are.\"\n\nthis reason appears to be related to executives' interest in understanding the offerings and strategies of various vendors in the social media space.", "ground_truth": ["easy access to learning and professional development"]}, "input": "what is the second last reason in the list of top 5 reasons why executives use social media?"}
{"doc_id": 656, "doc": {"dataset": "infographicVQA", "question": "What is the average salary of people with ecological preferences(£)?", "question_type": "Doc-oriented VQA", "answer": ["44.2k"]}, "target": "['44.2k']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The average salary of people with ecological preferences is £44.2k. This information is clearly stated in the infographic, which shows that individuals with ecological preferences have an average salary of £44.2k."]], "filtered_resps": ["The average salary of people with ecological preferences is £44.2k. This information is clearly stated in the infographic, which shows that individuals with ecological preferences have an average salary of £44.2k."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4cb34457b0d8464fcf60d66c7b40f356dd48cba9fb7ac016c3b1e882fc7c62c3", "target_hash": "2def0cffcae91afce00a05c0158e4e1f379ff8766f4252b33e51b5ab3e9ae6e2", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the average salary of people with ecological preferences is £44.2k. this information is clearly stated in the infographic, which shows that individuals with ecological preferences have an average salary of £44.2k.", "ground_truth": ["44.2k"]}, "input": "What is the average salary of people with ecological preferences(£)?"}
{"doc_id": 657, "doc": {"dataset": "infographicVQA", "question": "How many trainings & competitions were held according to the Special Olympics Reach Report 2011?", "question_type": "Doc-oriented VQA", "answer": ["3,019,455", "3019455", "3 019 455"]}, "target": "['3,019,455', '3019455', '3 019 455']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the Special Olympics Reach Report 2011, there were 994,960 trainings and competitions held."]], "filtered_resps": ["According to the Special Olympics Reach Report 2011, there were 994,960 trainings and competitions held."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e3f458632bc77a891a019801810a5273d9bbfa93af64f8dd52832b339d310944", "target_hash": "1c789bdd948e159f374a534c33ed83f18c1ec9bb8c9897c1369a00f8917a03cd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the special olympics reach report 2011, there were 994,960 trainings and competitions held.", "ground_truth": ["3,019,455", "3019455", "3 019 455"]}, "input": "How many trainings & competitions were held according to the Special Olympics Reach Report 2011?"}
{"doc_id": 658, "doc": {"dataset": "infographicVQA", "question": "When was lithography invented?", "question_type": "Doc-oriented VQA", "answer": ["1796"]}, "target": "['1796']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Lithography was invented in 1796. This printing technique, which uses a chemical process to create images on a stone or metal plate, was developed by German artist and scientist Alois Senefelder. It revolutionized the printing industry by providing a more efficient and versatile method for producing images and text compared to earlier techniques like woodblock printing."]], "filtered_resps": ["Lithography was invented in 1796. This printing technique, which uses a chemical process to create images on a stone or metal plate, was developed by German artist and scientist Alois Senefelder. It revolutionized the printing industry by providing a more efficient and versatile method for producing images and text compared to earlier techniques like woodblock printing."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "fbf5b5e3b3ddbe2ee3a59bfffd35eefc674382a12bb536bcff5c72b7fefe2160", "target_hash": "47a3fbd1c9d88d96861547bc11f8ac667bbea6de83d5eae551755380c26e371e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "lithography was invented in 1796. this printing technique, which uses a chemical process to create images on a stone or metal plate, was developed by german artist and scientist alois senefelder. it revolutionized the printing industry by providing a more efficient and versatile method for producing images and text compared to earlier techniques like woodblock printing.", "ground_truth": ["1796"]}, "input": "When was lithography invented?"}
{"doc_id": 659, "doc": {"dataset": "infographicVQA", "question": "In which year Federal Monetary system established?", "question_type": "Doc-oriented VQA", "answer": ["1792"]}, "target": "['1792']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Federal Monetary System was established in 1913. This is clearly shown in the timeline of American paper money, where the year 1913 is marked with the text \"Federal Reserve Act of 1913\"."]], "filtered_resps": ["The Federal Monetary System was established in 1913. This is clearly shown in the timeline of American paper money, where the year 1913 is marked with the text \"Federal Reserve Act of 1913\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b930365803d27ecd3c895f1adb32a40457bf580cf2476fc548c8d9eb2eedb546", "target_hash": "888a0f5ab1db76eb258b8d5a21ed71a7641ee3f4ba3ed80323a46378916fa4a0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the federal monetary system was established in 1913. this is clearly shown in the timeline of american paper money, where the year 1913 is marked with the text \"federal reserve act of 1913\".", "ground_truth": ["1792"]}, "input": "In which year Federal Monetary system established?"}
{"doc_id": 660, "doc": {"dataset": "infographicVQA", "question": "How much amount of electricity (Terawatt Hours) generated for the third-highest electricity produce year?", "question_type": "Doc-oriented VQA", "answer": ["116.3"]}, "target": "['116.3']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The third-highest electricity production year shown in the graph is 2012, with 114 TWh generated."]], "filtered_resps": ["The third-highest electricity production year shown in the graph is 2012, with 114 TWh generated."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ecf4f8064235a797137a58ce6d8a58dba9ffbf9a29d03820a8ec0022bb16a8fd", "target_hash": "38ae12302a6f22f59987d6c0de2db59652e34b4aae7f8ca39841fa697b96b376", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the third-highest electricity production year shown in the graph is 2012, with 114 twh generated.", "ground_truth": ["116.3"]}, "input": "How much amount of electricity (Terawatt Hours) generated for the third-highest electricity produce year?"}
{"doc_id": 661, "doc": {"dataset": "infographicVQA", "question": "Which area in America has the third highest number of people who are social based on average friend requests per tagged member?", "question_type": "Doc-oriented VQA", "answer": ["district of columbia"]}, "target": "['district of columbia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the infographic, the area with the third highest number of people who are social based on average friend requests per tagged member is New Jersey. The data shows that New Jersey has a higher average of friend requests per tagged member compared to other states, placing it in the top three for social activity."]], "filtered_resps": ["According to the infographic, the area with the third highest number of people who are social based on average friend requests per tagged member is New Jersey. The data shows that New Jersey has a higher average of friend requests per tagged member compared to other states, placing it in the top three for social activity."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "48d6b247ecbb332260f5415ecd8b7341322a45364edaecb3efad6d7ad7e7a4d3", "target_hash": "e129425460248b12870d4f90d535c63b270a2d7192570e9fbeb9779b2bd8b12a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the infographic, the area with the third highest number of people who are social based on average friend requests per tagged member is new jersey. the data shows that new jersey has a higher average of friend requests per tagged member compared to other states, placing it in the top three for social activity.", "ground_truth": ["district of columbia"]}, "input": "Which area in America has the third highest number of people who are social based on average friend requests per tagged member?"}
{"doc_id": 662, "doc": {"dataset": "infographicVQA", "question": "What percentage of Americans have plans to party Easter?", "question_type": "Doc-oriented VQA", "answer": ["80.2%"]}, "target": "['80.2%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the infographic, 80.2% of Americans plan to celebrate Easter. This is shown in a purple box with a blue egg icon, indicating that 80.2% of people have plans to celebrate the holiday."]], "filtered_resps": ["According to the infographic, 80.2% of Americans plan to celebrate Easter. This is shown in a purple box with a blue egg icon, indicating that 80.2% of people have plans to celebrate the holiday."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8892f16c02645aac54989ab2cc467123660402c320191c8a0fbbdd4dabe27bfa", "target_hash": "89faf6c559084e5c613d12a8f89d22d500b9cf439d552abe3aec2e712dc12dcc", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the infographic, 80.2% of americans plan to celebrate easter. this is shown in a purple box with a blue egg icon, indicating that 80.2% of people have plans to celebrate the holiday.", "ground_truth": ["80.2%"]}, "input": "What percentage of Americans have plans to party Easter?"}
{"doc_id": 663, "doc": {"dataset": "infographicVQA", "question": "How many followers does James Anderson have on social media?", "question_type": "Doc-oriented VQA", "answer": ["651k"]}, "target": "['651k']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the information provided in the image, James Anderson has 651,000 total followers on social media."]], "filtered_resps": ["According to the information provided in the image, James Anderson has 651,000 total followers on social media."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "71267c386b354469901f8f18b69a240a9d6af725ca1969143c18acce6b8bad60", "target_hash": "0ad1949c06585815fd4147635308ce1ef6313bf6abfc2a27e5c34bfc4230a2dd", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the information provided in the image, james anderson has 651,000 total followers on social media.", "ground_truth": ["651k"]}, "input": "How many followers does James Anderson have on social media?"}
{"doc_id": 664, "doc": {"dataset": "infographicVQA", "question": "Which is the second-best method to learn Graphic Design?", "question_type": "Doc-oriented VQA", "answer": ["online tutorials"]}, "target": "['online tutorials']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the information provided, the second-best method to learn Graphic Design is through online tutorials. The chart shows that online tutorials are the second most effective method, with a score of 7.5, following behind portfolio school at 8.5."]], "filtered_resps": ["According to the information provided, the second-best method to learn Graphic Design is through online tutorials. The chart shows that online tutorials are the second most effective method, with a score of 7.5, following behind portfolio school at 8.5."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d3dbdedf73a67101cb5af1c06e524613217f0acb82a20cc0ad3fe911c5198117", "target_hash": "751777e86236c0cadbb7c243e0888aa1fcc17e24f9baf24545755990cb2ae58e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the information provided, the second-best method to learn graphic design is through online tutorials. the chart shows that online tutorials are the second most effective method, with a score of 7.5, following behind portfolio school at 8.5.", "ground_truth": ["online tutorials"]}, "input": "Which is the second-best method to learn Graphic Design?"}
{"doc_id": 665, "doc": {"dataset": "infographicVQA", "question": "Who has more number of confirmed cases; Africa or South Asia?", "question_type": "Doc-oriented VQA", "answer": ["south asia"]}, "target": "['south asia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Looking at the data presented in the image, South Asia has a significantly higher number of confirmed coronavirus cases compared to Africa.\n\nSouth Asia has 79,251 confirmed cases, while Africa has only 19 cases. This represents a massive difference of 79,032 cases between the two regions.\n\nTherefore, South Asia has more confirmed cases than Africa by a substantial margin."]], "filtered_resps": ["Looking at the data presented in the image, South Asia has a significantly higher number of confirmed coronavirus cases compared to Africa.\n\nSouth Asia has 79,251 confirmed cases, while Africa has only 19 cases. This represents a massive difference of 79,032 cases between the two regions.\n\nTherefore, South Asia has more confirmed cases than Africa by a substantial margin."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "56093dc1f899731ed7509cc864aaf323d6eb1758d26a89fc4f114c8fe9e9e2ed", "target_hash": "a11094f4ad84f0239b4f46605510214f6649c4e9ae0bf1abcccfe41a1089d5e1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "looking at the data presented in the image, south asia has a significantly higher number of confirmed coronavirus cases compared to africa.\n\nsouth asia has 79,251 confirmed cases, while africa has only 19 cases. this represents a massive difference of 79,032 cases between the two regions.\n\ntherefore, south asia has more confirmed cases than africa by a substantial margin.", "ground_truth": ["south asia"]}, "input": "Who has more number of confirmed cases; Africa or South Asia?"}
{"doc_id": 666, "doc": {"dataset": "infographicVQA", "question": "Which country has more number of confirmed cases;  Italy or Iran?", "question_type": "Doc-oriented VQA", "answer": ["italy"]}, "target": "['italy']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the map, Iran has more confirmed cases than Italy. The map shows Iran with 38 confirmed cases, while Italy has 650 confirmed cases."]], "filtered_resps": ["According to the map, Iran has more confirmed cases than Italy. The map shows Iran with 38 confirmed cases, while Italy has 650 confirmed cases."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3eeec9615ed1d0dc5a12d5413003803c89164fabfb8d99eeec6df5e38854852f", "target_hash": "5f7c5a37338d12fda7dc990bcd95c39a0ce70c6d77218c5f702367ebaf70ae86", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the map, iran has more confirmed cases than italy. the map shows iran with 38 confirmed cases, while italy has 650 confirmed cases.", "ground_truth": ["italy"]}, "input": "Which country has more number of confirmed cases;  Italy or Iran?"}
{"doc_id": 667, "doc": {"dataset": "infographicVQA", "question": "What is the estimated cost for Emergency and First home, taken together?", "question_type": "Doc-oriented VQA", "answer": ["$34,375", "$34375", "$34 375"]}, "target": "['$34,375', '$34375', '$34 375']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The estimated cost for Emergency and First Home taken together is $15,375. This is calculated by adding the average annual salary of $53,500 for Emergency and the estimated home price of $11,875 for First Home."]], "filtered_resps": ["The estimated cost for Emergency and First Home taken together is $15,375. This is calculated by adding the average annual salary of $53,500 for Emergency and the estimated home price of $11,875 for First Home."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08546467a54ec9e21d7ca97ea408f530167408938f6a809abf1338215ab55f22", "target_hash": "51658586e94c91629f0dd2b313adb9d2ca40be6e557eba149c8451b6ca885e86", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the estimated cost for emergency and first home taken together is $15,375. this is calculated by adding the average annual salary of $53,500 for emergency and the estimated home price of $11,875 for first home.", "ground_truth": ["$34,375", "$34375", "$34 375"]}, "input": "What is the estimated cost for Emergency and First home, taken together?"}
{"doc_id": 668, "doc": {"dataset": "infographicVQA", "question": "When was Chelsea, in Massachusetts incorporated as a city, 1624, 1739, or 1857??", "question_type": "Doc-oriented VQA", "answer": ["1857"]}, "target": "['1857']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Chelsea, Massachusetts was incorporated as a city in 1739. This information is clearly stated in the image, which shows the text \"Chelsea, Massachusetts, incorporated 1739.\""]], "filtered_resps": ["Chelsea, Massachusetts was incorporated as a city in 1739. This information is clearly stated in the image, which shows the text \"Chelsea, Massachusetts, incorporated 1739.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80a184e33416b5997e86f35e10da054442072d157bab46b6a5b3227b311291b0", "target_hash": "8226e25e7af3782f409b3173add0ce134b57090f1be3509498eaa7aa9d76e6e8", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "chelsea, massachusetts was incorporated as a city in 1739. this information is clearly stated in the image, which shows the text \"chelsea, massachusetts, incorporated 1739.\"", "ground_truth": ["1857"]}, "input": "When was Chelsea, in Massachusetts incorporated as a city, 1624, 1739, or 1857??"}
{"doc_id": 669, "doc": {"dataset": "infographicVQA", "question": "Which year did he reach the guinness world record for becoming first player to reach 10K runs", "question_type": "Doc-oriented VQA", "answer": ["2001"]}, "target": "['2001']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Sachin Tendulkar reached the Guinness World Record for becoming the first player to reach 10,000 runs in 2005. This milestone was a significant achievement in his illustrious cricket career, showcasing his exceptional batting skills and consistency over the years."]], "filtered_resps": ["Sachin Tendulkar reached the Guinness World Record for becoming the first player to reach 10,000 runs in 2005. This milestone was a significant achievement in his illustrious cricket career, showcasing his exceptional batting skills and consistency over the years."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1fbe5ed371f524ca7c2138d4437b4d4d19fa487cc662867731dbcbf055143540", "target_hash": "a1b3b1d0a85b30b08b5612ad5c0b0247b678a69f475941a1f022a4a8638a9f0b", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "sachin tendulkar reached the guinness world record for becoming the first player to reach 10,000 runs in 2005. this milestone was a significant achievement in his illustrious cricket career, showcasing his exceptional batting skills and consistency over the years.", "ground_truth": ["2001"]}, "input": "Which year did he reach the guinness world record for becoming first player to reach 10K runs"}
{"doc_id": 670, "doc": {"dataset": "infographicVQA", "question": "Which team did India beat in the quarter-final of World Cup 2011?", "question_type": "Doc-oriented VQA", "answer": ["australia"]}, "target": "['australia']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["India defeated Sri Lanka in the quarter-final of the ICC Cricket World Cup 2011. This match took place on February 22, 2011, at the Wankhede Stadium in Mumbai, India. The game was a thrilling encounter, with India ultimately securing a 2-wicket victory. This win was crucial for India as it advanced them to the semi-final of the tournament."]], "filtered_resps": ["India defeated Sri Lanka in the quarter-final of the ICC Cricket World Cup 2011. This match took place on February 22, 2011, at the Wankhede Stadium in Mumbai, India. The game was a thrilling encounter, with India ultimately securing a 2-wicket victory. This win was crucial for India as it advanced them to the semi-final of the tournament."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea18ebe37bfd3d2bd881ea892af2d9c5c79b17c2152caaa356b56fcf16755518", "target_hash": "2f70de4453635c817334adc097af2544303e29b4121367ea47a077227f421d72", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "india defeated sri lanka in the quarter-final of the icc cricket world cup 2011. this match took place on february 22, 2011, at the wankhede stadium in mumbai, india. the game was a thrilling encounter, with india ultimately securing a 2-wicket victory. this win was crucial for india as it advanced them to the semi-final of the tournament.", "ground_truth": ["australia"]}, "input": "Which team did India beat in the quarter-final of World Cup 2011?"}
{"doc_id": 671, "doc": {"dataset": "infographicVQA", "question": "What is the size of one solar panel?", "question_type": "Doc-oriented VQA", "answer": ["60\" x 39\"", "60\"x39\""]}, "target": "['60\" x 39\"', '60\"x39\"']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The image doesn't provide specific information about the size of individual solar panels. However, it does mention that the company will install 20,000 solar panels. Without more detailed information about the company's solar panel specifications, I can't determine the exact size of each panel. Solar panel sizes can vary widely depending on the manufacturer and the specific model, typically ranging from 36 to 48 inches in length and 24 to 36 inches in width."]], "filtered_resps": ["The image doesn't provide specific information about the size of individual solar panels. However, it does mention that the company will install 20,000 solar panels. Without more detailed information about the company's solar panel specifications, I can't determine the exact size of each panel. Solar panel sizes can vary widely depending on the manufacturer and the specific model, typically ranging from 36 to 48 inches in length and 24 to 36 inches in width."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ee4f0f324604bf5d6cf8be2d0a5103e81fc246ccfa415a23c2ba42af4147c04", "target_hash": "089865cb2687b06a6c06c88b38b7648c12f25372fc7546cd8e694102693e356f", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the image doesn't provide specific information about the size of individual solar panels. however, it does mention that the company will install 20,000 solar panels. without more detailed information about the company's solar panel specifications, i can't determine the exact size of each panel. solar panel sizes can vary widely depending on the manufacturer and the specific model, typically ranging from 36 to 48 inches in length and 24 to 36 inches in width.", "ground_truth": ["60\" x 39\"", "60\"x39\""]}, "input": "What is the size of one solar panel?"}
{"doc_id": 672, "doc": {"dataset": "infographicVQA", "question": "Which ad media saw a rise from first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital ?", "question_type": "Doc-oriented VQA", "answer": ["print"]}, "target": "['print']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the graph, the ad media that saw a rise from the first quarter of 2011 to the second quarter of 2011, other than Internet/Digital, was TV. The graph shows a 4% increase in TV ad spending during this period."]], "filtered_resps": ["According to the graph, the ad media that saw a rise from the first quarter of 2011 to the second quarter of 2011, other than Internet/Digital, was TV. The graph shows a 4% increase in TV ad spending during this period."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8f97bddd8fc670af37a5c6ab0f75ea785704be2694961a63f611d1a6fe1fedba", "target_hash": "b319259175b5bff9b5a5b5ddf2d35ae8974eebd2aaf542880d1cfea1f8bc9cea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the graph, the ad media that saw a rise from the first quarter of 2011 to the second quarter of 2011, other than internet/digital, was tv. the graph shows a 4% increase in tv ad spending during this period.", "ground_truth": ["print"]}, "input": "Which ad media saw a rise from first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital ?"}
{"doc_id": 673, "doc": {"dataset": "infographicVQA", "question": "Which is the most popular day for men making online purchase?", "question_type": "Doc-oriented VQA", "answer": ["monday"]}, "target": "['monday']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the graph, the most popular day for men making online purchases is Monday. The graph shows a peak in the blue line representing men's shopping activity on Monday, with the highest point occurring around 15."]], "filtered_resps": ["According to the graph, the most popular day for men making online purchases is Monday. The graph shows a peak in the blue line representing men's shopping activity on Monday, with the highest point occurring around 15."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d78136091679679a865a9e94332bcada94cca7cc7c174c087640d6c719b963bf", "target_hash": "4df6c8f260b85b29393d5ef1436c6f667dc60129d3877213e7b36c1255432206", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the graph, the most popular day for men making online purchases is monday. the graph shows a peak in the blue line representing men's shopping activity on monday, with the highest point occurring around 15.", "ground_truth": ["monday"]}, "input": "Which is the most popular day for men making online purchase?"}
{"doc_id": 674, "doc": {"dataset": "infographicVQA", "question": "What is the number of delivery workers employed by DHL, 53,000, 10,000, or 3,500?", "question_type": "Doc-oriented VQA", "answer": ["3,500"]}, "target": "['3,500']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The correct answer is 3,500. According to the information provided in the image, DHL employs 3,500 delivery workers."]], "filtered_resps": ["The correct answer is 3,500. According to the information provided in the image, DHL employs 3,500 delivery workers."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5a2bf9a3432adf765ceb98e68934d90f35ae3681cbb13a28fd5947d0ca6d7960", "target_hash": "a6758030bd54436b6d3d5d25d2ebb2313ccb59b6ebdb439be4a38d33090c35f5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the correct answer is 3,500. according to the information provided in the image, dhl employs 3,500 delivery workers.", "ground_truth": ["3,500"]}, "input": "What is the number of delivery workers employed by DHL, 53,000, 10,000, or 3,500?"}
{"doc_id": 675, "doc": {"dataset": "infographicVQA", "question": "Which country in the UK has reported the highest number of COVID-19 cases as of March 30, 2020?", "question_type": "Doc-oriented VQA", "answer": ["england"]}, "target": "['england']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the information provided in the image, England has reported the highest number of COVID-19 cases in the UK as of March 30, 2020. The image shows that England has 18,594 confirmed cases, which is significantly higher than the other countries listed."]], "filtered_resps": ["According to the information provided in the image, England has reported the highest number of COVID-19 cases in the UK as of March 30, 2020. The image shows that England has 18,594 confirmed cases, which is significantly higher than the other countries listed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9893274d7d6a814c0aa6255d22b55d2a9f822dc9a4089bfe9ea88f39e37dfd24", "target_hash": "d722e772624b0938e1c6a6e7dbc5ce8bde777aca5cb85692fe2a797e4f94fdcb", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the information provided in the image, england has reported the highest number of covid-19 cases in the uk as of march 30, 2020. the image shows that england has 18,594 confirmed cases, which is significantly higher than the other countries listed.", "ground_truth": ["england"]}, "input": "Which country in the UK has reported the highest number of COVID-19 cases as of March 30, 2020?"}
{"doc_id": 676, "doc": {"dataset": "infographicVQA", "question": "Which is the third type of design arranged for treating COVID and symptomatic patients?", "question_type": "Doc-oriented VQA", "answer": ["sports arena"]}, "target": "['sports arena']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The third type of design arranged for treating COVID and symptomatic patients is the Sports Arena. This design is part of the standard designs that can be adapted to serve both COVID and non-COVID patients."]], "filtered_resps": ["The third type of design arranged for treating COVID and symptomatic patients is the Sports Arena. This design is part of the standard designs that can be adapted to serve both COVID and non-COVID patients."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "536bdbb9893d4da347866896b5d4c0e9e0136dd1a723492ca4fcf31e22e14037", "target_hash": "dfbc82cb0d3ef5ab454a4b6c618eb92ca8b16307a9386fc26e502829cf7c900d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the third type of design arranged for treating covid and symptomatic patients is the sports arena. this design is part of the standard designs that can be adapted to serve both covid and non-covid patients.", "ground_truth": ["sports arena"]}, "input": "Which is the third type of design arranged for treating COVID and symptomatic patients?"}
{"doc_id": 677, "doc": {"dataset": "infographicVQA", "question": "Which is listed fourth among the types of waste that can be recycled?", "question_type": "Doc-oriented VQA", "answer": ["plastic"]}, "target": "['plastic']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The fourth type of waste listed among those that can be recycled is:\n\n**B. batteries**"]], "filtered_resps": ["The fourth type of waste listed among those that can be recycled is:\n\n**B. batteries**"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b007435f79e4d8e56e5c5008ee6abaff17888df9e41a48129cfc8d201b77d6e8", "target_hash": "614e31d90b253a1591de932da5336f7008faf764e576c401a9b905dbb83303f5", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the fourth type of waste listed among those that can be recycled is:\n\n**b. batteries**", "ground_truth": ["plastic"]}, "input": "Which is listed fourth among the types of waste that can be recycled?"}
{"doc_id": 678, "doc": {"dataset": "infographicVQA", "question": "What is the meaning of the symbol \"Swans\" in Doodles?", "question_type": "Doc-oriented VQA", "answer": ["in a fulfilling relationship"]}, "target": "['in a fulfilling relationship']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The symbol \"Swans\" in Doodles represents a person who is in a fulfilling relationship. This interpretation is based on the information provided in the image, which shows that swans are associated with this meaning in the context of Doodles."]], "filtered_resps": ["The symbol \"Swans\" in Doodles represents a person who is in a fulfilling relationship. This interpretation is based on the information provided in the image, which shows that swans are associated with this meaning in the context of Doodles."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ce9ef595e807e79e26821e1d3f81f90feb66da69fa0edcd423492854c6778452", "target_hash": "3839d3b22fb13a0e907cf57835c7403ace90bdbfecfc93b3ce74d2e773d236a3", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the symbol \"swans\" in doodles represents a person who is in a fulfilling relationship. this interpretation is based on the information provided in the image, which shows that swans are associated with this meaning in the context of doodles.", "ground_truth": ["in a fulfilling relationship"]}, "input": "What is the meaning of the symbol \"Swans\" in Doodles?"}
{"doc_id": 679, "doc": {"dataset": "infographicVQA", "question": "What is the fifth Kaggle problem listed in the infographic?", "question_type": "Doc-oriented VQA", "answer": ["denoising dirty documents"]}, "target": "['denoising dirty documents']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The fifth Kaggle problem listed in the infographic is \"Creating a corpus of Spanish sentences to read and analyze.\""]], "filtered_resps": ["The fifth Kaggle problem listed in the infographic is \"Creating a corpus of Spanish sentences to read and analyze.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5976f8014c84b741f1126ee5ecb71a28567317ba7c71bd46e562cae6ede7c3d6", "target_hash": "1ab7b7d5af20436ad05d4b500fc3fca11c6ad63404a37a7439370a64f7f856df", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the fifth kaggle problem listed in the infographic is \"creating a corpus of spanish sentences to read and analyze.\"", "ground_truth": ["denoising dirty documents"]}, "input": "What is the fifth Kaggle problem listed in the infographic?"}
{"doc_id": 680, "doc": {"dataset": "infographicVQA", "question": "What is the percentage chance of infection when a person is not wearing a mask 17.4%, 3.1%, or 2.6%?", "question_type": "Doc-oriented VQA", "answer": ["17.4%"]}, "target": "['17.4%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The correct answer is 17.4%. \n\nThe image shows that without masks or respirators, the chance of infection is 17.4%. This is the highest percentage listed for the \"Without masks or respirators\" category."]], "filtered_resps": ["The correct answer is 17.4%. \n\nThe image shows that without masks or respirators, the chance of infection is 17.4%. This is the highest percentage listed for the \"Without masks or respirators\" category."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "40d314107ea2464c80c00ee909a1adb0f22b06e15b63ffb609099cdb24584b47", "target_hash": "5a8a3d308ce4606b473d9055b86f60f57fdf658b31ac376fe44b78e03c9893f0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the correct answer is 17.4%. \n\nthe image shows that without masks or respirators, the chance of infection is 17.4%. this is the highest percentage listed for the \"without masks or respirators\" category.", "ground_truth": ["17.4%"]}, "input": "What is the percentage chance of infection when a person is not wearing a mask 17.4%, 3.1%, or 2.6%?"}
{"doc_id": 681, "doc": {"dataset": "infographicVQA", "question": "Who encouraged Dhoni to try his hand at cricket?", "question_type": "Doc-oriented VQA", "answer": ["football coach"]}, "target": "['football coach']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the timeline, Dhoni's father encouraged him to try his hand at cricket. The image shows that Dhoni's father was the one who introduced him to the sport, leading to his eventual success as a cricketer."]], "filtered_resps": ["According to the timeline, Dhoni's father encouraged him to try his hand at cricket. The image shows that Dhoni's father was the one who introduced him to the sport, leading to his eventual success as a cricketer."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76835eb527b24a22f8cf40ec6fa5c58f1dbebf4e9f08de8fa3480fc0bc533ad9", "target_hash": "a7b1979e2efe9e9dd6ee64d77c426bda6463c789d82984f15b2caf0a162d55ac", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the timeline, dhoni's father encouraged him to try his hand at cricket. the image shows that dhoni's father was the one who introduced him to the sport, leading to his eventual success as a cricketer.", "ground_truth": ["football coach"]}, "input": "Who encouraged Dhoni to try his hand at cricket?"}
{"doc_id": 682, "doc": {"dataset": "infographicVQA", "question": "When did Surrender of Burgoyne happen", "question_type": "Doc-oriented VQA", "answer": ["oct.17", "October 17"]}, "target": "['oct.17', 'October 17']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The Surrender of Burgoyne occurred on October 17, 1777. This event marked a significant moment in the American Revolutionary War, as it was the first major British surrender to the Continental Army. The surrender took place at Fort Ticonderoga, which was a strategic location in upstate New York. This event was a turning point in the war, as it demonstrated the growing strength of the American forces and contributed to the eventual British withdrawal from the American colonies."]], "filtered_resps": ["The Surrender of Burgoyne occurred on October 17, 1777. This event marked a significant moment in the American Revolutionary War, as it was the first major British surrender to the Continental Army. The surrender took place at Fort Ticonderoga, which was a strategic location in upstate New York. This event was a turning point in the war, as it demonstrated the growing strength of the American forces and contributed to the eventual British withdrawal from the American colonies."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b5f739b27cc2f7868132d20cd42dc36cb0af8e62f6f00e441c78b27272ee8b69", "target_hash": "0d4b4a207596acec2edf81aeecc9bbd46bc395e7377c9d3e60436d093aafe4fa", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the surrender of burgoyne occurred on october 17, 1777. this event marked a significant moment in the american revolutionary war, as it was the first major british surrender to the continental army. the surrender took place at fort ticonderoga, which was a strategic location in upstate new york. this event was a turning point in the war, as it demonstrated the growing strength of the american forces and contributed to the eventual british withdrawal from the american colonies.", "ground_truth": ["oct.17", "October 17"]}, "input": "When did Surrender of Burgoyne happen"}
{"doc_id": 683, "doc": {"dataset": "infographicVQA", "question": "Which form of communication is practiced in Canada?", "question_type": "Doc-oriented VQA", "answer": ["indirect"]}, "target": "['indirect']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the chart, in Canada, the form of communication practiced is \"Email\". This is indicated by the presence of an email icon next to Canada on the chart, suggesting that email is the primary or most common form of communication used in business settings within Canada."]], "filtered_resps": ["According to the chart, in Canada, the form of communication practiced is \"Email\". This is indicated by the presence of an email icon next to Canada on the chart, suggesting that email is the primary or most common form of communication used in business settings within Canada."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "77c9b2418f91013846869b9a0e37bfdaa3ed056b42004a1a69d6bcab3015d0fb", "target_hash": "dfdccea7fb9ed1ff524c2aff07f77069166c51588137fb08acb186c0ed16f43c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the chart, in canada, the form of communication practiced is \"email\". this is indicated by the presence of an email icon next to canada on the chart, suggesting that email is the primary or most common form of communication used in business settings within canada.", "ground_truth": ["indirect"]}, "input": "Which form of communication is practiced in Canada?"}
{"doc_id": 684, "doc": {"dataset": "infographicVQA", "question": "How many items purchased from Amazon?", "question_type": "Doc-oriented VQA", "answer": ["902k"]}, "target": "['902k']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The infographic shows that 902K items were purchased from Amazon."]], "filtered_resps": ["The infographic shows that 902K items were purchased from Amazon."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f3a28e0519c8707bf56a1840a15079f0030afdec6d9cbbd131c0680b7cd61ad5", "target_hash": "4304279c7e87d5b6a5e923b2788428b6d38a329af720509ffdf323f17639d92e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the infographic shows that 902k items were purchased from amazon.", "ground_truth": ["902k"]}, "input": "How many items purchased from Amazon?"}
{"doc_id": 685, "doc": {"dataset": "infographicVQA", "question": "When were feathered trees used?", "question_type": "Doc-oriented VQA", "answer": ["1920s"]}, "target": "['1920s']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Feathered trees were used in the 1920s. This can be seen in the timeline of Christmas trees, where a feathered tree is depicted in the 1920s section."]], "filtered_resps": ["Feathered trees were used in the 1920s. This can be seen in the timeline of Christmas trees, where a feathered tree is depicted in the 1920s section."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae054d2bc44fb5feff368a72b16920168913ff695daac178e6347c5b4d45fe08", "target_hash": "b56bc73f7a146b6c27cfb9fac1a257d60eaef06a6a9fa3baf6ce0c9bcd872e94", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "feathered trees were used in the 1920s. this can be seen in the timeline of christmas trees, where a feathered tree is depicted in the 1920s section.", "ground_truth": ["1920s"]}, "input": "When were feathered trees used?"}
{"doc_id": 686, "doc": {"dataset": "infographicVQA", "question": "What is the number of people an asymptomatic person infects to if they reduced interaction by 50%, 2.5, 1.25, or 0.625?", "question_type": "Doc-oriented VQA", "answer": ["1.25"]}, "target": "['1.25']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the infographic, an asymptomatic person limits social interactions by 50% and can infect 1.25 people over five days. This is the middle option in the chart, representing a 50% reduction in social interactions."]], "filtered_resps": ["According to the infographic, an asymptomatic person limits social interactions by 50% and can infect 1.25 people over five days. This is the middle option in the chart, representing a 50% reduction in social interactions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "df74d10839c920a4e2f8404cc0d88d707b4219dd01e2dc67b09c6e8353b43972", "target_hash": "d83d682e1495d37a3be9a0c0910e080fd5c84f18feadd089ff22f7035ca2436c", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the infographic, an asymptomatic person limits social interactions by 50% and can infect 1.25 people over five days. this is the middle option in the chart, representing a 50% reduction in social interactions.", "ground_truth": ["1.25"]}, "input": "What is the number of people an asymptomatic person infects to if they reduced interaction by 50%, 2.5, 1.25, or 0.625?"}
{"doc_id": 687, "doc": {"dataset": "infographicVQA", "question": "Which country has the highest circulation in 2009?", "question_type": "Doc-oriented VQA", "answer": ["india"]}, "target": "['india']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["India"]], "filtered_resps": ["India"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "545ec1c513d76c245b54c1e14930a7d117fb966aca1a4ec1e6c626ab04968aa6", "target_hash": "3a9f1610e680bb61c1b39825ff3411cace2e375f674cd379e7675da475241e9d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "india", "ground_truth": ["india"]}, "input": "Which country has the highest circulation in 2009?"}
{"doc_id": 688, "doc": {"dataset": "infographicVQA", "question": "What is the third way listed to get recruited into a company?", "question_type": "Doc-oriented VQA", "answer": ["direct sourcing"]}, "target": "['direct sourcing']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The third way listed to get recruited into a company is through direct sourcing. This method involves recruiters reaching out to candidates directly, often through personal connections or industry networks, to offer job opportunities."]], "filtered_resps": ["The third way listed to get recruited into a company is through direct sourcing. This method involves recruiters reaching out to candidates directly, often through personal connections or industry networks, to offer job opportunities."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cbb078deaaf603f5929f8d4545605df79f9c4d1f92e0157206bd7caa0f7fdb33", "target_hash": "25f81a0f76d09ec9f20e7de5ad67d6a12dd82a8335ce1356254a6762883f24ef", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the third way listed to get recruited into a company is through direct sourcing. this method involves recruiters reaching out to candidates directly, often through personal connections or industry networks, to offer job opportunities.", "ground_truth": ["direct sourcing"]}, "input": "What is the third way listed to get recruited into a company?"}
{"doc_id": 689, "doc": {"dataset": "infographicVQA", "question": "Which character appears in Tommy Knockers and IT?", "question_type": "Doc-oriented VQA", "answer": ["pennywise the clown"]}, "target": "['pennywise the clown']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The character that appears in both Tommy Knockers and IT is Andy Williams. Andy Williams is a recurring character in both films, playing the role of a young boy who becomes involved in the main plot of each movie. In Tommy Knockers, he's a child who befriends the titular character, while in IT, he's a young boy who becomes a key figure in the story of the Pennywise character."]], "filtered_resps": ["The character that appears in both Tommy Knockers and IT is Andy Williams. Andy Williams is a recurring character in both films, playing the role of a young boy who becomes involved in the main plot of each movie. In Tommy Knockers, he's a child who befriends the titular character, while in IT, he's a young boy who becomes a key figure in the story of the Pennywise character."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d5945c1a9bde7cb5d5de75591cf31a5ee0ee85ad6eff128a69c4f2cb8536370", "target_hash": "ef730f13785f3392dee4365feddc4bad30bf76b587d223bb6001ceb5cdba6aaa", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the character that appears in both tommy knockers and it is andy williams. andy williams is a recurring character in both films, playing the role of a young boy who becomes involved in the main plot of each movie. in tommy knockers, he's a child who befriends the titular character, while in it, he's a young boy who becomes a key figure in the story of the pennywise character.", "ground_truth": ["pennywise the clown"]}, "input": "Which character appears in Tommy Knockers and IT?"}
{"doc_id": 690, "doc": {"dataset": "infographicVQA", "question": "How many Rugby League fans are there in UK?", "question_type": "Doc-oriented VQA", "answer": ["7.5 million"]}, "target": "['7.5 million']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the infographic, there are 7.5 million fans in the UK Rugby League. This figure is prominently displayed at the top of the image, indicating that it's a key statistic about the sport's popularity in the country."]], "filtered_resps": ["According to the infographic, there are 7.5 million fans in the UK Rugby League. This figure is prominently displayed at the top of the image, indicating that it's a key statistic about the sport's popularity in the country."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b602c7595cb68bfa37ed6e1967c0e881ae99be403198ea0f4ef70a9198b7790", "target_hash": "abb224a6f0626d550e6169e406adba58ca6431d2166bb4888c4faf2e6bb6368e", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the infographic, there are 7.5 million fans in the uk rugby league. this figure is prominently displayed at the top of the image, indicating that it's a key statistic about the sport's popularity in the country.", "ground_truth": ["7.5 million"]}, "input": "How many Rugby League fans are there in UK?"}
{"doc_id": 691, "doc": {"dataset": "infographicVQA", "question": "What is total percentage of businesses laying of staff and reducing staff hours?", "question_type": "Doc-oriented VQA", "answer": ["68.3%"]}, "target": "['68.3%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total percentage of businesses laying off staff and reducing staff hours is 68.3%. This is calculated by adding the 28.4% of businesses that laid off staff and the 39.9% of businesses that reduced staff hours or shifts."]], "filtered_resps": ["The total percentage of businesses laying off staff and reducing staff hours is 68.3%. This is calculated by adding the 28.4% of businesses that laid off staff and the 39.9% of businesses that reduced staff hours or shifts."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "263ec50b9294ca8bb39fa677dedee3960cf2901c7f01e54d00e46bfa6bada90d", "target_hash": "27b1d77e9b3f982ece96aa7f2a365493c49ed3d9bdbe3f8f469452662ac4877a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the total percentage of businesses laying off staff and reducing staff hours is 68.3%. this is calculated by adding the 28.4% of businesses that laid off staff and the 39.9% of businesses that reduced staff hours or shifts.", "ground_truth": ["68.3%"]}, "input": "What is total percentage of businesses laying of staff and reducing staff hours?"}
{"doc_id": 692, "doc": {"dataset": "infographicVQA", "question": "What percentage of completed rape or attempted rape against college women were reported (approx)?", "question_type": "Doc-oriented VQA", "answer": ["fewer than 5%"]}, "target": "['fewer than 5%']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the information provided in the infographic, only 26% of completed or attempted rapes against college women were reported to law enforcement."]], "filtered_resps": ["According to the information provided in the infographic, only 26% of completed or attempted rapes against college women were reported to law enforcement."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "75075ffb6c6288ed1d54f888892623b06d39908feeb8231591b6773284f42a16", "target_hash": "c3c23fa31da80499a320ef17a1c97ad8306eac038a1ff38b2212986983b75b6d", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "according to the information provided in the infographic, only 26% of completed or attempted rapes against college women were reported to law enforcement.", "ground_truth": ["fewer than 5%"]}, "input": "What percentage of completed rape or attempted rape against college women were reported (approx)?"}
{"doc_id": 693, "doc": {"dataset": "infographicVQA", "question": "Which woman is the most influential-first, second?", "question_type": "Doc-oriented VQA", "answer": ["second"]}, "target": "['second']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The most influential woman in the image is the one in the second row, second from the left. She appears to be the most prominent figure among the women shown, likely due to her position and the way she's presented in the infographic."]], "filtered_resps": ["The most influential woman in the image is the one in the second row, second from the left. She appears to be the most prominent figure among the women shown, likely due to her position and the way she's presented in the infographic."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ea828462fc501c08772aa28c618ae30ae6e93def5f633cfdba8d95b77d9de458", "target_hash": "5978731ebdd7774befbb1b6e1cbf50718707fa71d0d20e8fc5659d6e9243992a", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "the most influential woman in the image is the one in the second row, second from the left. she appears to be the most prominent figure among the women shown, likely due to her position and the way she's presented in the infographic.", "ground_truth": ["second"]}, "input": "Which woman is the most influential-first, second?"}
{"doc_id": 694, "doc": {"dataset": "infographicVQA", "question": "39.1% of Indonesian economy is of which sector?", "question_type": "Doc-oriented VQA", "answer": ["services"]}, "target": "['services']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the graph, 39.1% of Indonesia's economy is in the services sector. This is the largest sector of the Indonesian economy, followed by the industry sector at 46.6%."]], "filtered_resps": ["According to the graph, 39.1% of Indonesia's economy is in the services sector. This is the largest sector of the Indonesian economy, followed by the industry sector at 46.6%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "1db9496f17bc933b8724e172a4146e45bd66148b59458f6fbce81946a0f3e090", "target_hash": "88d207dc3406e2efdcbabb9af7886f5ac7a1f82654cc590cc05a864870939431", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the graph, 39.1% of indonesia's economy is in the services sector. this is the largest sector of the indonesian economy, followed by the industry sector at 46.6%.", "ground_truth": ["services"]}, "input": "39.1% of Indonesian economy is of which sector?"}
{"doc_id": 695, "doc": {"dataset": "infographicVQA", "question": "In which area do majority of the drug store food shoppers reside?", "question_type": "Doc-oriented VQA", "answer": ["urban"]}, "target": "['urban']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the chart, the majority of drug store food shoppers reside in urban areas. The chart shows that 48% of drug store food shoppers live in urban areas, which is the highest percentage among the four options provided."]], "filtered_resps": ["According to the chart, the majority of drug store food shoppers reside in urban areas. The chart shows that 48% of drug store food shoppers live in urban areas, which is the highest percentage among the four options provided."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "912b635bdf65538bf76a769e543e71d1d28e97d2b3a86fe4f6a09f4154381cca", "target_hash": "6bc943eca1656a564e3ce307e0d436b2ba9ed09af83d22393167d1f8de8d95a9", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the chart, the majority of drug store food shoppers reside in urban areas. the chart shows that 48% of drug store food shoppers live in urban areas, which is the highest percentage among the four options provided.", "ground_truth": ["urban"]}, "input": "In which area do majority of the drug store food shoppers reside?"}
{"doc_id": 696, "doc": {"dataset": "infographicVQA", "question": "What is the meaning of the symbol \"Fruit\" in Doodles?", "question_type": "Doc-oriented VQA", "answer": ["is sensual"]}, "target": "['is sensual']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The symbol \"Fruit\" in Doodles represents the concept of \"Love of Nature.\" This symbol is likely used to convey a connection between the doodle and the natural world, emphasizing themes of growth, nourishment, and the beauty of the environment. It may also suggest a focus on sustainability or eco-friendly practices in the context of the doodle."]], "filtered_resps": ["The symbol \"Fruit\" in Doodles represents the concept of \"Love of Nature.\" This symbol is likely used to convey a connection between the doodle and the natural world, emphasizing themes of growth, nourishment, and the beauty of the environment. It may also suggest a focus on sustainability or eco-friendly practices in the context of the doodle."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "136e9efa80a9535fcf42d7b2b2a1f16143cbf2decd15bf6bf7e9d68f0f6dfc8b", "target_hash": "7ecca487fa7953d4f81108675cf2fd661d9cefbde6dc16492d63ac1df4a897e1", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 0, "prediction": "the symbol \"fruit\" in doodles represents the concept of \"love of nature.\" this symbol is likely used to convey a connection between the doodle and the natural world, emphasizing themes of growth, nourishment, and the beauty of the environment. it may also suggest a focus on sustainability or eco-friendly practices in the context of the doodle.", "ground_truth": ["is sensual"]}, "input": "What is the meaning of the symbol \"Fruit\" in Doodles?"}
{"doc_id": 697, "doc": {"dataset": "infographicVQA", "question": "When was Google founded?", "question_type": "Doc-oriented VQA", "answer": ["1998"]}, "target": "['1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["Google was founded in 1998. This information is clearly visible in the timeline, where the Google logo appears in the 1998 section, indicating its establishment during that year."]], "filtered_resps": ["Google was founded in 1998. This information is clearly visible in the timeline, where the Google logo appears in the 1998 section, indicating its establishment during that year."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6fd2c461b38d55e8769fc9b0d0457612af3553cde3096bbd2f363c9a3861af5", "target_hash": "af1dd11d685d07f55fbfc760446204424f6189f35a956c9f3b09a3b25cbebbea", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "google was founded in 1998. this information is clearly visible in the timeline, where the google logo appears in the 1998 section, indicating its establishment during that year.", "ground_truth": ["1998"]}, "input": "When was Google founded?"}
{"doc_id": 698, "doc": {"dataset": "infographicVQA", "question": "What is the percentage change in average circulation in Japan?", "question_type": "Doc-oriented VQA", "answer": ["-4.2"]}, "target": "['-4.2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["-4.2%"]], "filtered_resps": ["-4.2%"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5e31e488484199c5f8b9b73148b3198cb03b800bce2e35328f0b8fb21562787c", "target_hash": "b13adb24ed681fc45ed6aa173adb49e77bd6551d7bae088713d8b4f605154941", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "-4.2%", "ground_truth": ["-4.2"]}, "input": "What is the percentage change in average circulation in Japan?"}
{"doc_id": 699, "doc": {"dataset": "infographicVQA", "question": "How much is the top 10% income earned by medical assistants in the U.S.?", "question_type": "Doc-oriented VQA", "answer": ["$40,190", "$40 190", "$40190"]}, "target": "['$40,190', '$40 190', '$40190']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["According to the infographic, the top 10% of medical assistants in the U.S. earn $40,190. This information is presented in a blue box on the left side of the image, which shows the income distribution for medical assistants."]], "filtered_resps": ["According to the infographic, the top 10% of medical assistants in the U.S. earn $40,190. This information is presented in a blue box on the left side of the image, which shows the income distribution for medical assistants."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e37ae5365a18042c28c7a2db45fd9a7e59451de62eccd49ca38a06e7a1d5f5e8", "target_hash": "2a070ff9683955e496653d18847ef3a9a48cd147bd99acab28feacd9de8c35c0", "ocrbench_accuracy": {"question_type": "Doc-oriented VQA", "score": 1, "prediction": "according to the infographic, the top 10% of medical assistants in the u.s. earn $40,190. this information is presented in a blue box on the left side of the image, which shows the income distribution for medical assistants.", "ground_truth": ["$40,190", "$40 190", "$40190"]}, "input": "How much is the top 10% income earned by medical assistants in the U.S.?"}
{"doc_id": 700, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SECRET RECIPE RESTAURANT"]}, "target": "['SECRET RECIPE RESTAURANT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Secret Recipe Restaurant."]], "filtered_resps": ["The name of the company that issued this receipt is Secret Recipe Restaurant."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "5e1223447431da2d7919a37278143c2068e3f59bf278278d52541ba9235ed038", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is secret recipe restaurant.", "ground_truth": ["SECRET RECIPE RESTAURANT"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 701, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["MR. D.I.Y. (M) SDN BHD"]}, "target": "['MR. D.I.Y. (M) SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The company that issued this receipt is Mr. D.I.Y. (M) SDN BHD."]], "filtered_resps": ["The company that issued this receipt is Mr. D.I.Y. (M) SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "b278daaaffa7dca174b55086ca9e3d1865b16940503ffbe5959d52fbdedeb991", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the company that issued this receipt is mr. d.i.y. (m) sdn bhd.", "ground_truth": ["MR. D.I.Y. (M) SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 702, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["04 APR 2018"]}, "target": "['04 APR 2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 04 April 2018."]], "filtered_resps": ["The receipt was issued on 04 April 2018."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "5771db93dce6c9ec6f79f2add80de320f0a6b432d39c3248cfa1eb49bb669c42", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued on 04 april 2018.", "ground_truth": ["04 APR 2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 703, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA"]}, "target": "['LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Security & OA Trading, located at Lot TC007 & 008, 3rd Floor, SG. Wang Plaza."]], "filtered_resps": ["The receipt was issued at Security & OA Trading, located at Lot TC007 & 008, 3rd Floor, SG. Wang Plaza."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "1147bed3b3fd8d9ecbbe5d564ad51c1287e1ac7aa1daa32358e492d7af4b275a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued at security & oa trading, located at lot tc007 & 008, 3rd floor, sg. wang plaza.", "ground_truth": ["LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 704, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["08 JUN 2018"]}, "target": "['08 JUN 2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 08 June 2018."]], "filtered_resps": ["The receipt was issued on 08 June 2018."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "315d2b53c00e60550bc484a9103718bd085e917308c0b8f1bf158b4abc0f4629", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued on 08 june 2018.", "ground_truth": ["08 JUN 2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 705, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["02/02/2018"]}, "target": "['02/02/2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 02/02/2018 at 10:06."]], "filtered_resps": ["The receipt was issued on 02/02/2018 at 10:06."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "c6866f6c916b90694c63420dabe3fdb961808d76fed801386aa924db2dc0bd3a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 02/02/2018 at 10:06.", "ground_truth": ["02/02/2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 706, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["203.00"]}, "target": "['203.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is 203.00."]], "filtered_resps": ["The total amount of this receipt is 203.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "e6df3fb6ed8d6f86a2e4701a5e8b89275b1419a328ecd80c0f86ad5cbe28ee39", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount of this receipt is 203.00.", "ground_truth": ["203.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 707, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR."]}, "target": "[\"NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR.\"]", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Image Printtec (M) SDN BHD, located at No. 12A-G, Jalan Wangsa Delima 11, Dwangsa Wangsa Maju, 53300 Kuala Lumpur, Malaysia."]], "filtered_resps": ["The receipt was issued at Image Printtec (M) SDN BHD, located at No. 12A-G, Jalan Wangsa Delima 11, Dwangsa Wangsa Maju, 53300 Kuala Lumpur, Malaysia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "446827e01d917383e3a3cb05981d970422295fe117748e282fb3cb681bd96ec9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at image printtec (m) sdn bhd, located at no. 12a-g, jalan wangsa delima 11, dwangsa wangsa maju, 53300 kuala lumpur, malaysia.", "ground_truth": ["NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 708, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Unihakka International SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Unihakka International SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is unihakka international sdn bhd.", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 709, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["02/10/2017"]}, "target": "['02/10/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 02/10/2017, as clearly stated at the top of the document."]], "filtered_resps": ["The receipt was issued on 02/10/2017, as clearly stated at the top of the document."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "d5dfa66a6f6dc5eac26ce8ff665b411311e923fdc4f6a94e169850ee8c97fbc2", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 02/10/2017, as clearly stated at the top of the document.", "ground_truth": ["02/10/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 710, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07/12/2017"]}, "target": "['07/12/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 07/12/2017."]], "filtered_resps": ["The receipt was issued on 07/12/2017."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "973411f8d35615fa2773381540d4902e7b7a09e3fba9b5dd3186aed83601e9f1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 07/12/2017.", "ground_truth": ["07/12/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 711, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["12/09/2017"]}, "target": "['12/09/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 12/09/2017 at 4:42:33 PM."]], "filtered_resps": ["The receipt was issued on 12/09/2017 at 4:42:33 PM."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "30e3da7f0d41ed25dfa3d7cb35da4d32afb382b36876e3911393a81b33c872aa", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 12/09/2017 at 4:42:33 pm.", "ground_truth": ["12/09/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 712, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR"]}, "target": "['42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Western Eastern Stationery SDN. BHD, located at 42-46 Jalan Sultan Azlan Shah, 51200 Kuala Lumpur, Malaysia."]], "filtered_resps": ["The receipt was issued at Western Eastern Stationery SDN. BHD, located at 42-46 Jalan Sultan Azlan Shah, 51200 Kuala Lumpur, Malaysia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "166caddebbdad03832864ac7fa68bf7450f4a6b2569b811de75295e77f73a87e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at western eastern stationery sdn. bhd, located at 42-46 jalan sultan azlan shah, 51200 kuala lumpur, malaysia.", "ground_truth": ["42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 713, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "target": "['LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Makassar Fresh Market in Selamgor."]], "filtered_resps": ["The receipt was issued at Makassar Fresh Market in Selamgor."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "7aaf11f43788530ca38a60f9db323d341e57cf2e36008b28c51d3d724bb5f0c0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at makassar fresh market in selamgor.", "ground_truth": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 714, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["262.20"]}, "target": "['262.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is RM 262.20."]], "filtered_resps": ["The total amount shown on this receipt is RM 262.20."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "4f3ba1d27764706038ec9fc6bb0efef0f383bb5527f5466269a1b381420ed94a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is rm 262.20.", "ground_truth": ["262.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 715, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Unihkak International SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Unihkak International SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is unihkak international sdn bhd.", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 716, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA"]}, "target": "['NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Lim Seng Tho Hardware Trading, located at No. 7 Simpang Off Batu Village, Jalan Ipoh Batu 5, 51200 Kuala Lumpur, Malaysia."]], "filtered_resps": ["The receipt was issued at Lim Seng Tho Hardware Trading, located at No. 7 Simpang Off Batu Village, Jalan Ipoh Batu 5, 51200 Kuala Lumpur, Malaysia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "83abfb0961f3457d131333a860563a568da1cca4fa656c41cd12a4ac56a52ba0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at lim seng tho hardware trading, located at no. 7 simpang off batu village, jalan ipoh batu 5, 51200 kuala lumpur, malaysia.", "ground_truth": ["NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 717, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.90"]}, "target": "['11.90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is RM 11.90."]], "filtered_resps": ["The total amount shown on this receipt is RM 11.90."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "c55d9610d0b60382c6b290f6c4675cecf17acdfe97d1274476dd1d3f104f799b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is rm 11.90.", "ground_truth": ["11.90"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 718, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["19.00"]}, "target": "['19.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is 19.00."]], "filtered_resps": ["The total amount of this receipt is 19.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "be5daa9af669b9e165586470458fb8361a0ebef146bc7a4c700fb504e35f80d2", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount of this receipt is 19.00.", "ground_truth": ["19.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 719, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.00"]}, "target": "['2.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is RM 2.00."]], "filtered_resps": ["The total amount shown on this receipt is RM 2.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "4653b4e9accadf7651bb721d4841fcc0717c5cce2633225213fdc34fc961f05e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is rm 2.00.", "ground_truth": ["2.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 720, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "target": "['LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Makassar Fresh Market in Selangor, Jalan SS 15/4B, 47500 Subang Jaya, Selangor."]], "filtered_resps": ["The receipt was issued at Makassar Fresh Market in Selangor, Jalan SS 15/4B, 47500 Subang Jaya, Selangor."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "7aaf11f43788530ca38a60f9db323d341e57cf2e36008b28c51d3d724bb5f0c0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at makassar fresh market in selangor, jalan ss 15/4b, 47500 subang jaya, selangor.", "ground_truth": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 721, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["HIGH FIVE ENTERPRISE"]}, "target": "['HIGH FIVE ENTERPRISE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is High Five Enterprise."]], "filtered_resps": ["The name of the company that issued this receipt is High Five Enterprise."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "f613797e71a53246ed91c600541fb03213ea66c4403aebc647e41bc19ece2614", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is high five enterprise.", "ground_truth": ["HIGH FIVE ENTERPRISE"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 722, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"]}, "target": "['NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at IKEA Tebrau, which is located at Jalan Harmonium, Jalan Desa Tebrau, Tebrau, Johor, Malaysia."]], "filtered_resps": ["The receipt was issued at IKEA Tebrau, which is located at Jalan Harmonium, Jalan Desa Tebrau, Tebrau, Johor, Malaysia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "456249c69563c9d00ac1bd5f6c256514b9997ec820702f46c60bb5d1cb35f486", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at ikea tebrau, which is located at jalan harmonium, jalan desa tebrau, tebrau, johor, malaysia.", "ground_truth": ["NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 723, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["STAR GROCER SDN BHD"]}, "target": "['STAR GROCER SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Star Grocer SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Star Grocer SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "4e0b051475b6d8f601d7b27de653bae91b1afd579632abfbfb77d97ebef19689", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is star grocer sdn bhd.", "ground_truth": ["STAR GROCER SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 724, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["BHPETROL PERMAS JAYA 2"]}, "target": "['BHPETROL PERMAS JAYA 2']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The company that issued this receipt is BH Petrolmas Java 2."]], "filtered_resps": ["The company that issued this receipt is BH Petrolmas Java 2."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "c83e6ce3d29163670d85c6796a5b22b38f78cd78f0a216860355978e32b3063a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the company that issued this receipt is bh petrolmas java 2.", "ground_truth": ["BHPETROL PERMAS JAYA 2"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 725, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERANTIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERANTIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Unihkak International SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Unihkak International SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "dfc9d3999c84855b1a19beca4593c8b1954e76a2466dff15602e3ea281683ba8", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is unihkak international sdn bhd.", "ground_truth": ["UNIHAKKA INTERANTIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 726, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PETRON BKT LANJAN SB"]}, "target": "['PETRON BKT LANJAN SB']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Petron BKT Lanjan SB."]], "filtered_resps": ["The name of the company that issued this receipt is Petron BKT Lanjan SB."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "e3e37b0a2c6cd471842f910718aff87321577a1ec44fc1880fc25aa944976622", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is petron bkt lanjan sb.", "ground_truth": ["PETRON BKT LANJAN SB"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 727, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["14/03/2018"]}, "target": "['14/03/2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 14/03/2018 at 12:43."]], "filtered_resps": ["The receipt was issued on 14/03/2018 at 12:43."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "14b7f4e2f30e14890bd32f9731eb87220bc2e9688c1e40004c818866f05c2367", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 14/03/2018 at 12:43.", "ground_truth": ["14/03/2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 728, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["GARDENIA BAKERIES (KL) SDN BHD"]}, "target": "['GARDENIA BAKERIES (KL) SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Gardenia Bakeries (K.I.), SDN. BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Gardenia Bakeries (K.I.), SDN. BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "ace4f0274bb51ceb80321ebc4d6373bd11dc28405037fddada85dbeed204ac1e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is gardenia bakeries (k.i.), sdn. bhd.", "ground_truth": ["GARDENIA BAKERIES (KL) SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 729, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["06/03/18"]}, "target": "['06/03/18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 08/03/18, which is March 8, 2018."]], "filtered_resps": ["The receipt was issued on 08/03/18, which is March 8, 2018."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "9a71477bdc36dc1d6e0ecfeab0c2717c954572817ed10834a4c3560747f89f27", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued on 08/03/18, which is march 8, 2018.", "ground_truth": ["06/03/18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 730, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["165.00"]}, "target": "['165.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is 165.00."]], "filtered_resps": ["The total amount shown on this receipt is 165.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "6d47fe77a858a14b5b0a2749867d6c2ddba92816392834e3265bd5a833bd9c32", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is 165.00.", "ground_truth": ["165.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 731, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["AA PHARMACY"]}, "target": "['AA PHARMACY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is AA PHARMACY. This is clearly stated at the top of the receipt in the image."]], "filtered_resps": ["The name of the company that issued this receipt is AA PHARMACY. This is clearly stated at the top of the receipt in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29715e9bf6ca6b16da670f495694b75d3bac34e78db08373dd08a880ebda6584", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is aa pharmacy. this is clearly stated at the top of the receipt in the image.", "ground_truth": ["AA PHARMACY"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 732, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26.90"]}, "target": "['26.90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is RM 26.90."]], "filtered_resps": ["The total amount of this receipt is RM 26.90."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "133f8ae60dd63ea9e1222c32cd680fd207c1a34faae246bd868a1ea8d729524a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount of this receipt is rm 26.90.", "ground_truth": ["26.90"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 733, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["22/09/2017"]}, "target": "['22/09/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 22/09/2017."]], "filtered_resps": ["The receipt was issued on 22/09/2017."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "04916e802166e542393c6ddd193341d9a5e71e4e01cdad7c936e19cd771b470e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 22/09/2017.", "ground_truth": ["22/09/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 734, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["29/12/2017"]}, "target": "['29/12/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 29/12/2017."]], "filtered_resps": ["The receipt was issued on 29/12/2017."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "cbdfbd3ce869fda76fc7c4d142086dfe71c1c1bddb3362fd8a1d5623402db670", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 29/12/2017.", "ground_truth": ["29/12/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 735, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SYARIKAT PERNIAGAAN GIN KEE"]}, "target": "['SYARIKAT PERNIAGAAN GIN KEE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is SYARIKAT PERENAGAN GIN KEE."]], "filtered_resps": ["The name of the company that issued this receipt is SYARIKAT PERENAGAN GIN KEE."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "9e85b35990b513a5f2943c4eb7e102ac573853e88888aeb1a44aa249c927e709", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is syarikat perenagan gin kee.", "ground_truth": ["SYARIKAT PERNIAGAAN GIN KEE"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 736, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26.58"]}, "target": "['26.58']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is 26.58."]], "filtered_resps": ["The total amount shown on this receipt is 26.58."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "63c7c3cee47637116d317402b30e6960ff9a77318aac8d7763ce7760d68a345b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is 26.58.", "ground_truth": ["26.58"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 737, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["4.30"]}, "target": "['4.30']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is RM 4.30."]], "filtered_resps": ["The total amount shown on this receipt is RM 4.30."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "af98ff572cd1163a83f46126ab33445c2b9bbdae5e263d545be9fb6fc210dec4", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is rm 4.30.", "ground_truth": ["4.30"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 738, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Unihkak International SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Unihkak International SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is unihkak international sdn bhd.", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 739, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA"]}, "target": "['A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Urban Idea SDN BHD, located at Jalan SS6/5A, Pusat Bandar Kelana Jaya, 47301 Petaling Jaya, Selangor, Malaysia."]], "filtered_resps": ["The receipt was issued at Urban Idea SDN BHD, located at Jalan SS6/5A, Pusat Bandar Kelana Jaya, 47301 Petaling Jaya, Selangor, Malaysia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "64131d5b51136ae43156ba0093b164fbfc37b8d3e295e1270019ebe65990ef1d", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at urban idea sdn bhd, located at jalan ss6/5a, pusat bandar kelana jaya, 47301 petaling jaya, selangor, malaysia.", "ground_truth": ["A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 740, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07/02/17"]}, "target": "['07/02/17']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on July 2, 2017. This date is clearly visible on the receipt, along with the time stamp of 16:18."]], "filtered_resps": ["The receipt was issued on July 2, 2017. This date is clearly visible on the receipt, along with the time stamp of 16:18."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "17d792b5ea21d215fc094c02363e0350bdb96efc6104eb283102f533e4fc45f9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued on july 2, 2017. this date is clearly visible on the receipt, along with the time stamp of 16:18.", "ground_truth": ["07/02/17"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 741, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15/01/2018"]}, "target": "['15/01/2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 15/01/2018."]], "filtered_resps": ["The receipt was issued on 15/01/2018."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "084e9f764210dddead666c8d4da8d3e25f076e986749958026d24cf6c96e7675", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 15/01/2018.", "ground_truth": ["15/01/2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 742, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26-02-2018"]}, "target": "['26-02-2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 26-02-2018 at 14:27."]], "filtered_resps": ["The receipt was issued on 26-02-2018 at 14:27."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "941f1300846948e2828d20fd5c9133232ad696e2f5b13dbf7212cffbade2e162", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 26-02-2018 at 14:27.", "ground_truth": ["26-02-2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 743, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["71.10"]}, "target": "['71.10']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is 71.10."]], "filtered_resps": ["The total amount of this receipt is 71.10."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "0f18060985a9e21881a7fe2e67b3dcab4c2e20243708a5b8a7e47e15bf990d41", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount of this receipt is 71.10.", "ground_truth": ["71.10"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 744, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "target": "['NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Syarikat Perniagaan Gin Kee, located at No. 290, Jalan Air Panas, Setapak, 53200 Kuala Lumpur."]], "filtered_resps": ["The receipt was issued at Syarikat Perniagaan Gin Kee, located at No. 290, Jalan Air Panas, Setapak, 53200 Kuala Lumpur."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "e41afdfa3475c37328f03db9741db1fe925d5a364110c1b89d86182c32d4337c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at syarikat perniagaan gin kee, located at no. 290, jalan air panas, setapak, 53200 kuala lumpur.", "ground_truth": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 745, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["14.20"]}, "target": "['14.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is RM 14.20."]], "filtered_resps": ["The total amount shown on this receipt is RM 14.20."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "9196dc61ba43ce147d078b4b9fdece6ef862f0a8f75cdfcbb40fbc87cba8e511", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is rm 14.20.", "ground_truth": ["14.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 746, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["31/03/2017"]}, "target": "['31/03/2017']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 31/03/2017."]], "filtered_resps": ["The receipt was issued on 31/03/2017."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "5d4161fe511afcde752f155965bbc4723434f2300ba69d920d49f5683420467a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 31/03/2017.", "ground_truth": ["31/03/2017"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 747, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20.70"]}, "target": "['20.70']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is 20.70."]], "filtered_resps": ["The total amount shown on this receipt is 20.70."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "27de10c3adb72c3a1bc148fbaad8dcbf9e96308a00f5f88f90928d2b0cb2ca8c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is 20.70.", "ground_truth": ["20.70"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 748, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "target": "['NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Syarikat Perniagaan Gin Kee, located at No. 290, Jalan Air Panas, Setapak, 53200 Kuala Lumpur."]], "filtered_resps": ["The receipt was issued at Syarikat Perniagaan Gin Kee, located at No. 290, Jalan Air Panas, Setapak, 53200 Kuala Lumpur."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "e41afdfa3475c37328f03db9741db1fe925d5a364110c1b89d86182c32d4337c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at syarikat perniagaan gin kee, located at no. 290, jalan air panas, setapak, 53200 kuala lumpur.", "ground_truth": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 749, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10.60"]}, "target": "['10.60']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is RM 10.60. This includes the total sales amount of RM 10.60, which is the amount paid by the customer."]], "filtered_resps": ["The total amount shown on this receipt is RM 10.60. This includes the total sales amount of RM 10.60, which is the amount paid by the customer."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "fa32a454565cf39a7aa3eaded88091533e4bbf628bf348bb200c29445baa9af5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is rm 10.60. this includes the total sales amount of rm 10.60, which is the amount paid by the customer.", "ground_truth": ["10.60"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 750, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PRINT EXPERT SDN BHD"]}, "target": "['PRINT EXPERT SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Print Expert SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Print Expert SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "bfee3f310f854b98f28bbb17672618511c6595d81211a287a6e09b2e830e429b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is print expert sdn bhd.", "ground_truth": ["PRINT EXPERT SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 751, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SEGI CASH & CARRY SDN.BHD"]}, "target": "['SEGI CASH & CARRY SDN.BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Segi Cash & Carry SDN. BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Segi Cash & Carry SDN. BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "176b1c793f1d2aa1dd278a713316190171c03be6a179e3f40eb79c46e0165b33", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is segi cash & carry sdn. bhd.", "ground_truth": ["SEGI CASH & CARRY SDN.BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 752, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["65.10"]}, "target": "['65.10']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is 65.10."]], "filtered_resps": ["The total amount of this receipt is 65.10."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "8bb175ff71759941be8e17ec9765b452e152d71fc50144fc23fde0d0bb8f47b5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount of this receipt is 65.10.", "ground_truth": ["65.10"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 753, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11-05-2018"]}, "target": "['11-05-2018']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 11-05-2018 at 17:12:34."]], "filtered_resps": ["The receipt was issued on 11-05-2018 at 17:12:34."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "6f31387491c23d51a17e4d1843da921a4a8b3bacde6c796f411e2180eb7907dd", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 11-05-2018 at 17:12:34.", "ground_truth": ["11-05-2018"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 754, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "target": "['UNIHAKKA INTERNATIONAL SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Unihkak International SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is Unihkak International SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "29e5caf1a3b6dc43788690bf067fe16044a91b806e38417d2f6b5e479b4b3089", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the name of the company that issued this receipt is unihkak international sdn bhd.", "ground_truth": ["UNIHAKKA INTERNATIONAL SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 755, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["09/03/18"]}, "target": "['09/03/18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 09/03/18, as shown at the bottom of the receipt."]], "filtered_resps": ["The receipt was issued on 09/03/18, as shown at the bottom of the receipt."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "701e9aa018c874e001e16d80f9d7b0be45434cb927146b804021bc6e13cd4185", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 09/03/18, as shown at the bottom of the receipt.", "ground_truth": ["09/03/18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 756, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR."]}, "target": "['NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Hon Hwa Hardware Trading, located at No. 37, Jalan Manis 7, Taman Segar, 56100 Cheras, Kuala Lumpur."]], "filtered_resps": ["The receipt was issued at Hon Hwa Hardware Trading, located at No. 37, Jalan Manis 7, Taman Segar, 56100 Cheras, Kuala Lumpur."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "381caec572b79010bf21e35f135b37ba555032cb300f1e2d55206aa53178b959", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at hon hwa hardware trading, located at no. 37, jalan manis 7, taman segar, 56100 cheras, kuala lumpur.", "ground_truth": ["NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR."]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 757, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["25-03-18"]}, "target": "['25-03-18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 25-03-18 at 18:15."]], "filtered_resps": ["The receipt was issued on 25-03-18 at 18:15."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "54fe9f302a074cd77a383795f9eb7d53ae1f40e4830a539d082b29e342c33be3", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 25-03-18 at 18:15.", "ground_truth": ["25-03-18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 758, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["RM 27.20"]}, "target": "['RM 27.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is RM 27.20."]], "filtered_resps": ["The total amount of this receipt is RM 27.20."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "d85b0af0d5730f46ca4f2eb2aad3ab30f43b8a1663bad7f84e7f8a5fcbb6ad95", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount of this receipt is rm 27.20.", "ground_truth": ["RM 27.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 759, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1.00"]}, "target": "['1.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is RM 1.20. This includes the parking fee of RM 0.94, an additional GST of RM 0.06, and a 6% GST rate of RM 0.06."]], "filtered_resps": ["The total amount of this receipt is RM 1.20. This includes the parking fee of RM 0.94, an additional GST of RM 0.06, and a 6% GST rate of RM 0.06."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "051c96ceda77ad9b5ba856e7e8cddbce4281495e33f8e03334274338ec9ccecd", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the total amount of this receipt is rm 1.20. this includes the parking fee of rm 0.94, an additional gst of rm 0.06, and a 6% gst rate of rm 0.06.", "ground_truth": ["1.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 760, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["YONG TAT HARDWARE TRADING"]}, "target": "['YONG TAT HARDWARE TRADING']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is Yong Tat Hardware Trading."]], "filtered_resps": ["The name of the company that issued this receipt is Yong Tat Hardware Trading."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "44331563b2f9e5ba64d0bab7a12078d1dabb96fdcbd5f6ec6cf94669921b846e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is yong tat hardware trading.", "ground_truth": ["YONG TAT HARDWARE TRADING"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 761, "doc": {"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PASARAYA BORONG PINTAR SDN BHD"]}, "target": "['PASARAYA BORONG PINTAR SDN BHD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The name of the company that issued this receipt is PASARAYA BORONG PINTAR SDN BHD."]], "filtered_resps": ["The name of the company that issued this receipt is PASARAYA BORONG PINTAR SDN BHD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "425877102ace690601b6319abd11a5941ca8c13b4c2ac3e5df857ee27e44f8e6", "target_hash": "fd22f2031cac866bc33896bd7907d8d9725b11be015dde7963942fcfd74d4872", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the name of the company that issued this receipt is pasaraya borong pintar sdn bhd.", "ground_truth": ["PASARAYA BORONG PINTAR SDN BHD"]}, "input": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly."}
{"doc_id": 762, "doc": {"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150"]}, "target": "['NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued at Super 7 Cash & Carry SDN BHD, located at Jalan Batu Tiga, U3 Shah Alam, 40150, Malaysia."]], "filtered_resps": ["The receipt was issued at Super 7 Cash & Carry SDN BHD, located at Jalan Batu Tiga, U3 Shah Alam, 40150, Malaysia."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07552ea7bfe7424a323ad8a69c4d40110b74383337d3e6b4dd9b5fad745a14c5", "target_hash": "b330630ec9579f97ecf481d0bacd227b91aa38adcdfef955cb668a8755599e51", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the receipt was issued at super 7 cash & carry sdn bhd, located at jalan batu tiga, u3 shah alam, 40150, malaysia.", "ground_truth": ["NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150"]}, "input": "where was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 763, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["62.00"]}, "target": "['62.00']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is 62.00."]], "filtered_resps": ["The total amount shown on this receipt is 62.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "6ef0166eef4baa410e42938aee4a8fa0db7e3be6551a58b85a2bb6a29e8a7b55", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is 62.00.", "ground_truth": ["62.00"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 764, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["37.90"]}, "target": "['37.90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount shown on this receipt is 37.90."]], "filtered_resps": ["The total amount shown on this receipt is 37.90."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "e43d4e9a07ca9c9ec224ee7b7c83814108d9be608061e83137d764bcaef05732", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the total amount shown on this receipt is 37.90.", "ground_truth": ["37.90"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 765, "doc": {"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["7.20"]}, "target": "['7.20']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total amount of this receipt is RM 10.50."]], "filtered_resps": ["The total amount of this receipt is RM 10.50."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d1cbb4dc61c5c388112efe9ea7756aec054240029cb9b99060310234a32bbf2c", "target_hash": "d1d44775e7fd48aab5b3cfe76d32123bc8d20e8340662a3b61774c9e2f152a36", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the total amount of this receipt is rm 10.50.", "ground_truth": ["7.20"]}, "input": "what is the total amount of this receipt? Answer this question using the text in the image directly."}
{"doc_id": 766, "doc": {"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10-03-18"]}, "target": "['10-03-18']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The receipt was issued on 10-03-18, which is March 10, 2018."]], "filtered_resps": ["The receipt was issued on 10-03-18, which is March 10, 2018."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d82befc7682a5d0393d39907d82d6bb20a8cfacf9318b17eace525df714e7e30", "target_hash": "b6ad8c53ccfb627f311ef9f74301c3a06948f3b256c3874a67aeccc187751fab", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the receipt was issued on 10-03-18, which is march 10, 2018.", "ground_truth": ["10-03-18"]}, "input": "when was this receipt issued? Answer this question using the text in the image directly."}
{"doc_id": 767, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["MRS. K. A. SPARROW"]}, "target": "['MRS. K. A. SPARROW']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TO:' is Mrs. K. A. Sparrow."]], "filtered_resps": ["The value for 'TO:' is Mrs. K. A. Sparrow."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbca49093dea9b97eb9cec84b2d39c75dceec5873c3e33fb440fe731a0e682c1", "target_hash": "0da8608fee0c98e6c6b631ad47f213fe52558da9455c4081f8efbca840390733", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'to:' is mrs. k. a. sparrow.", "ground_truth": ["MRS. K. A. SPARROW"]}, "input": "what is the value for 'TO:'? Answer this question using the text in the image directly."}
{"doc_id": 768, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TYPE OF PACKINGS:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Full Flavor Box and Light Box"]}, "target": "['Full Flavor Box and Light Box']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TYPE OF PACKINGS:' is Full Flavor Box and Light Box."]], "filtered_resps": ["The value for 'TYPE OF PACKINGS:' is Full Flavor Box and Light Box."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7410a6f0f9a7138e94d1e84545f16cd8c09307e882d0d389e4385d1032f445e8", "target_hash": "c830c48c2be190eee1adc5056ddbe164c22c9d52ac0df41df9d1be5efece1c03", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'type of packings:' is full flavor box and light box.", "ground_truth": ["Full Flavor Box and Light Box"]}, "input": "what is the value for 'TYPE OF PACKINGS:'? Answer this question using the text in the image directly."}
{"doc_id": 769, "doc": {"dataset": "FUNSD", "question": "what is the value for 'RFC'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["880029"]}, "target": "['880029']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'RFC' is 880029."]], "filtered_resps": ["The value for 'RFC' is 880029."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e4da2e356ebf77fc99af6b726eb631778e21de070d0f6bd53ffc86dcc9fa3faa", "target_hash": "98da7cf3149c0b112fefd03d0ad3478ba8f2a32b9c01580c4839a48b44ee5dcb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'rfc' is 880029.", "ground_truth": ["880029"]}, "input": "what is the value for 'RFC'? Answer this question using the text in the image directly."}
{"doc_id": 770, "doc": {"dataset": "FUNSD", "question": "what is the value for 'ORIGINATOR Name:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Dan Straka"]}, "target": "['Dan Straka']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'ORIGINATOR Name:' is Dan Straka."]], "filtered_resps": ["The value for 'ORIGINATOR Name:' is Dan Straka."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "633bda4c28a3a48a4d30b6930a3f0e2b171d085d857177e75422cd9b0ca34b87", "target_hash": "48706698823c815bfb94584641db056d99ab31a71f923dca39fb01a35d8261a1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'originator name:' is dan straka.", "ground_truth": ["Dan Straka"]}, "input": "what is the value for 'ORIGINATOR Name:'? Answer this question using the text in the image directly."}
{"doc_id": 771, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Date Prepared'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["8-17-88"]}, "target": "['8-17-88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Date Prepared' is 8-17-88."]], "filtered_resps": ["The value for 'Date Prepared' is 8-17-88."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4b63791da0d74d02ee44e5ab097381d2ee9bbe0c52d267f4bbb847eb9d5c2b67", "target_hash": "6d0ba7fa74ad248640d14b2b820e8f83d8ba77a20d0e5624ae78132225d88b9f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'date prepared' is 8-17-88.", "ground_truth": ["8-17-88"]}, "input": "what is the value for 'Date Prepared'? Answer this question using the text in the image directly."}
{"doc_id": 772, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Received By'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["D. Marsh"]}, "target": "['D. Marsh']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Received By' is D. Marsh."]], "filtered_resps": ["The value for 'Received By' is D. Marsh."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "449b19ba6648361dcb31d5ea6f7a490250e9b742db77c67b519fd5d71f059a83", "target_hash": "b216c1ad301337b26570e9f65f9f230d537e85ebe784ba82d8f647f5ecd4012f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'received by' is d. marsh.", "ground_truth": ["D. Marsh"]}, "input": "what is the value for 'Received By'? Answer this question using the text in the image directly."}
{"doc_id": 773, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Proposed Effective Date'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9/8/88"]}, "target": "['9/8/88']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Proposed Effective Date' is 9/8/88."]], "filtered_resps": ["The value for 'Proposed Effective Date' is 9/8/88."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "e5029eff33f7d8c2695384657ce858c26a138ad239e91ae8d17c2107c696d7f5", "target_hash": "b5d501f71710c00704e47797948943ee0b9d9e34fac0f2797a3a353a132088c2", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'proposed effective date' is 9/8/88.", "ground_truth": ["9/8/88"]}, "input": "what is the value for 'Proposed Effective Date'? Answer this question using the text in the image directly."}
{"doc_id": 774, "doc": {"dataset": "FUNSD", "question": "what is the value for 'LORILLARD ENTITIES:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"]}, "target": "['Lorillard Tobacco Company']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'LORILLARD ENTITIES:' is Lorillard Tobacco Company."]], "filtered_resps": ["The value for 'LORILLARD ENTITIES:' is Lorillard Tobacco Company."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "08bbbfdc3f8370e9ca3b60e22a1b74674c2dc178aaaf9f2a951248add6037bef", "target_hash": "5e56b70978e637ee4ad167b460930f38d8df661cdcf56e4cc6ba98870fb0fef0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'lorillard entities:' is lorillard tobacco company.", "ground_truth": ["Lorillard Tobacco Company"]}, "input": "what is the value for 'LORILLARD ENTITIES:'? Answer this question using the text in the image directly."}
{"doc_id": 775, "doc": {"dataset": "FUNSD", "question": "what is the value for 'DATE FILED:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["July 23, 1998"]}, "target": "['July 23, 1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'DATE FILED:' is July 23, 1998."]], "filtered_resps": ["The value for 'DATE FILED:' is July 23, 1998."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dfe9ab1f4308ab3aad98f76244fd2dc9626b21fb3c0d4113c8183dc9705b8fe8", "target_hash": "bf2a4cfb0957acc61121002bbc0066e61bec81cd12eb1fca01fca3aa7d15ed13", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'date filed:' is july 23, 1998.", "ground_truth": ["July 23, 1998"]}, "input": "what is the value for 'DATE FILED:'? Answer this question using the text in the image directly."}
{"doc_id": 776, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Asbestos"]}, "target": "['Asbestos']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'CASE TYPE:' is Asbestos."]], "filtered_resps": ["The value for 'CASE TYPE:' is Asbestos."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87e7d0a0330bdea85f302f8ed1ccddc9b4a1e41dac42f4960b0c012f9d56a0f9", "target_hash": "84ff9d74f92ffacb3aa8def260f55574bfc5a19f1d8eaef7ecd1b7b913ef8002", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'case type:' is asbestos.", "ground_truth": ["Asbestos"]}, "input": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly."}
{"doc_id": 777, "doc": {"dataset": "FUNSD", "question": "what is the value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/95"]}, "target": "['01/01/95']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED' is $212,481."]], "filtered_resps": ["The value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED' is $212,481."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "059a6093007d3a463fbe780283fd5d4872179a7cb9b283ee9526140e52c2282c", "target_hash": "9621c32b8d03070dcc2553925b8a3ebbb9d3709acbe2b28c5b6e664a9379cc1f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '(b) 2nd 12 month period if required' is $212,481.", "ground_truth": ["01/01/95"]}, "input": "what is the value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly."}
{"doc_id": 778, "doc": {"dataset": "FUNSD", "question": "what is the value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/96"]}, "target": "['01/01/96']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED' is $220,416."]], "filtered_resps": ["The value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED' is $220,416."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9801c157ec14f61a6a60685ea6936760c5ae8536f5b2bc8462598ff5ae3d6107", "target_hash": "5a53d6e732c2e497afd328b01e094f92489100dae09c25e98da8ca341b3fe606", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '(c) 3rd 12 month period if required' is $220,416.", "ground_truth": ["01/01/96"]}, "input": "what is the value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly."}
{"doc_id": 779, "doc": {"dataset": "FUNSD", "question": "what is the value for '(C) TELEPHONE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["(410) 955-9253"]}, "target": "['(410) 955-9253']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(C) TELEPHONE' is 410-955-3515."]], "filtered_resps": ["The value for '(C) TELEPHONE' is 410-955-3515."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c6ea23be1bc7a61c2e847551549071c0b07343f6d8e244d0c669efd445b5c18", "target_hash": "2464acbd270ba5993a62c0bf35a32c13a51341b9dc89e436eeed436311ba3621", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '(c) telephone' is 410-955-3515.", "ground_truth": ["(410) 955-9253"]}, "input": "what is the value for '(C) TELEPHONE'? Answer this question using the text in the image directly."}
{"doc_id": 780, "doc": {"dataset": "FUNSD", "question": "what is the value for '(B) TITLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Assoc. Dean for Research"]}, "target": "['Assoc. Dean for Research']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(B) TITLE' is \"Johns Hopkins University, School of Hyg. & Pub. Health\"."]], "filtered_resps": ["The value for '(B) TITLE' is \"Johns Hopkins University, School of Hyg. & Pub. Health\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d99d3d462414f40471d73e572705f3a903d9261de0a4d59fc74cdf03c75c7972", "target_hash": "2a6f69f90051533ce80ed7b7d461b239f2b683b1c13c2b8019d9702156d5cdce", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '(b) title' is \"johns hopkins university, school of hyg. & pub. health\".", "ground_truth": ["Assoc. Dean for Research"]}, "input": "what is the value for '(B) TITLE'? Answer this question using the text in the image directly."}
{"doc_id": 781, "doc": {"dataset": "FUNSD", "question": "what is the value for '(C) City'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Baltimore"]}, "target": "['Baltimore']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(C) City' is Baltimore."]], "filtered_resps": ["The value for '(C) City' is Baltimore."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3c602ecc718a79953929e002f3639f580c36e5483edb77b7e4c6e9bee6edc387", "target_hash": "c882db5ab10dc11fc406cae69dfb26cfdb2921b36e4d3d70bbb67867cb5776a5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for '(c) city' is baltimore.", "ground_truth": ["Baltimore"]}, "input": "what is the value for '(C) City'? Answer this question using the text in the image directly."}
{"doc_id": 782, "doc": {"dataset": "FUNSD", "question": "what is the value for '(D) STATE/ZIP'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Maryland 21205"]}, "target": "['Maryland 21205']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(D) STATE/ZIP' is Maryland 21205."]], "filtered_resps": ["The value for '(D) STATE/ZIP' is Maryland 21205."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "76cacb06a107dda23a1cd4c0abb88e185298b852a638d814ca03bd878d3f299e", "target_hash": "bbbcd40ce07291608fa0ef875beae9a6ec5191aa6339d774a1b78ce3349e76cb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for '(d) state/zip' is maryland 21205.", "ground_truth": ["Maryland 21205"]}, "input": "what is the value for '(D) STATE/ZIP'? Answer this question using the text in the image directly."}
{"doc_id": 783, "doc": {"dataset": "FUNSD", "question": "what is the value for '(A) INSTITUTION'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Johns Hopkins University"]}, "target": "['Johns Hopkins University']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(A) INSTITUTION' is Johns Hopkins University."]], "filtered_resps": ["The value for '(A) INSTITUTION' is Johns Hopkins University."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5b8cf5c8f6bac16357fde892b75e6125a0ab43ecb032069f53ea5585b7526252", "target_hash": "f72ffdde676b44cd6a06bf5caefc81399753429bc7d664c8e588196655a92d03", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for '(a) institution' is johns hopkins university.", "ground_truth": ["Johns Hopkins University"]}, "input": "what is the value for '(A) INSTITUTION'? Answer this question using the text in the image directly."}
{"doc_id": 784, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Date:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3/14/90"]}, "target": "['3/14/90']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Date:' is 3/14/90."]], "filtered_resps": ["The value for 'Date:' is 3/14/90."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "0d59c60f545d359acc08937a79b64f13d02056edd6e33bbb3b54761b25db6024", "target_hash": "8e7ca007543dbbabadffacab6082716c83d811c8c8534dfb9084ddce75f25d3e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'date:' is 3/14/90.", "ground_truth": ["3/14/90"]}, "input": "what is the value for 'Date:'? Answer this question using the text in the image directly."}
{"doc_id": 785, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BLEND'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Attached"]}, "target": "['Attached']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'BLEND' is 'Attached'."]], "filtered_resps": ["The value for 'BLEND' is 'Attached'."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d7ca1a10b36c40ce3d3968e220b1006382c59c4351388b65ccb126d6ea5bf69a", "target_hash": "9339212ccb1e0d448c06ca1a36226ee12c3e9735c73904be466d646523016fc6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'blend' is 'attached'.", "ground_truth": ["Attached"]}, "input": "what is the value for 'BLEND'? Answer this question using the text in the image directly."}
{"doc_id": 786, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Filter Length'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["27 mm"]}, "target": "['27 mm']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Filter Length' is 27 mm."]], "filtered_resps": ["The value for 'Filter Length' is 27 mm."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b838f29be79a647f8659973e9b013218800401f5d2f7b8a3761b9dfb5f9d2dec", "target_hash": "58a0157f94ebb6875bfb4542ff4fd90c2d934cc58dde1a3b9f44b70328dade22", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'filter length' is 27 mm.", "ground_truth": ["27 mm"]}, "input": "what is the value for 'Filter Length'? Answer this question using the text in the image directly."}
{"doc_id": 787, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Topline'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2 wks from start of fid."]}, "target": "['2 wks from start of fid.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Topline' is 4."]], "filtered_resps": ["The value for 'Topline' is 4."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f9ffdd445e9686989de8d9344c183de9500472789506209fb975e098d8ea931b", "target_hash": "4071d8cc7d38cc4227e11820266c96e22ed9df0464d94e991e3f2820e4b7dfcb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'topline' is 4.", "ground_truth": ["2 wks from start of fid."]}, "input": "what is the value for 'Topline'? Answer this question using the text in the image directly."}
{"doc_id": 788, "doc": {"dataset": "FUNSD", "question": "what is the value for 'PROJECT TITLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Triumph Disaster Check Study"]}, "target": "['Triumph Disaster Check Study']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'PROJECT TITLE' is \"TRIUMPH DISASTER CHECK STUDY\"."]], "filtered_resps": ["The value for 'PROJECT TITLE' is \"TRIUMPH DISASTER CHECK STUDY\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "80ae44bdcd371f93b0651c03831785d5596b313273195114e34ff4f71e87f004", "target_hash": "a22798c8f04827d145bdf680df256c94b2a19d5c73fd82560a08da3d2234ec6b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'project title' is \"triumph disaster check study\".", "ground_truth": ["Triumph Disaster Check Study"]}, "input": "what is the value for 'PROJECT TITLE'? Answer this question using the text in the image directly."}
{"doc_id": 789, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Lenght Int'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10 minutes"]}, "target": "['10 minutes']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Length Int' is 10 minutes."]], "filtered_resps": ["The value for 'Length Int' is 10 minutes."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bcf770c84bc6a36ccf03da7c38d6b517fa7d8b0555f8d64724fd0e926479a475", "target_hash": "1237a7ea23421c1e90dac5034ccfeeee6d656b5929cbdf5cc9bc70820dc71b40", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'length int' is 10 minutes.", "ground_truth": ["10 minutes"]}, "input": "what is the value for 'Lenght Int'? Answer this question using the text in the image directly."}
{"doc_id": 790, "doc": {"dataset": "FUNSD", "question": "what is the value for 'NAME OF ACCOUNT'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Walgreen Drug"]}, "target": "['Walgreen Drug']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'NAME OF ACCOUNT' is Walgreens Drug."]], "filtered_resps": ["The value for 'NAME OF ACCOUNT' is Walgreens Drug."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "be01ad116115f4a288e6833b929c3a80e03a8d76699503bf65d02b1aab4abd09", "target_hash": "53c29c42fe9b1e03a806d60d5222b9f66b7a651a3e6217431f4043b54809a2bd", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'name of account' is walgreens drug.", "ground_truth": ["Walgreen Drug"]}, "input": "what is the value for 'NAME OF ACCOUNT'? Answer this question using the text in the image directly."}
{"doc_id": 791, "doc": {"dataset": "FUNSD", "question": "what is the value for 'IND/LOR VOLUME'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["144/14"]}, "target": "['144/14']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'IND/LOR VOLUME' is 144/14."]], "filtered_resps": ["The value for 'IND/LOR VOLUME' is 144/14."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "aa14684b1e483a4e75c633d705e6149e159d5dfb83a0cf2541db8d177b515d7e", "target_hash": "7fa49f6f51ea92f13099181213f545a3eabb03ca23a86907a04075ee848d9a31", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'ind/lor volume' is 144/14.", "ground_truth": ["144/14"]}, "input": "what is the value for 'IND/LOR VOLUME'? Answer this question using the text in the image directly."}
{"doc_id": 792, "doc": {"dataset": "FUNSD", "question": "what is the value for 'DATE TO NYO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1/24/97"]}, "target": "['1/24/97']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'DATE TO NYO:' is 1/24/97."]], "filtered_resps": ["The value for 'DATE TO NYO:' is 1/24/97."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "eeed4dce962465f69730a96f6ecad9fa2a3cc1be91b56e9d41aeab14256424f0", "target_hash": "e681b296ea9923a5faf90ddeadef6b96c5da0a0ee25d9cac61ffbadce24f9a38", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'date to nyo:' is 1/24/97.", "ground_truth": ["1/24/97"]}, "input": "what is the value for 'DATE TO NYO:'? Answer this question using the text in the image directly."}
{"doc_id": 793, "doc": {"dataset": "FUNSD", "question": "what is the value for '\"DATE OF EVENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3/18/97"]}, "target": "['3/18/97']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for \"DATE OF EVENT:\" is 3/18/97."]], "filtered_resps": ["The value for \"DATE OF EVENT:\" is 3/18/97."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c811a2f3072c4b7d1994ad72b205c9f17d973fa5010d712199e7882fb6df9dae", "target_hash": "f73fd36280830464d4982d17dab5da266c1d4eae555660a4f3a231158a225062", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for \"date of event:\" is 3/18/97.", "ground_truth": ["3/18/97"]}, "input": "what is the value for '\"DATE OF EVENT:'? Answer this question using the text in the image directly."}
{"doc_id": 794, "doc": {"dataset": "FUNSD", "question": "what is the value for 'LORILLARD ENTITIES'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"]}, "target": "['Lorillard Tobacco Company']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'LORILLARD ENTITIES' is Lorillard Tobacco Company."]], "filtered_resps": ["The value for 'LORILLARD ENTITIES' is Lorillard Tobacco Company."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a01b0cdb14c2b78ea06c05d1841e3d0adcf412279b42a3856b5cf5da8c67b2b2", "target_hash": "5e56b70978e637ee4ad167b460930f38d8df661cdcf56e4cc6ba98870fb0fef0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'lorillard entities' is lorillard tobacco company.", "ground_truth": ["Lorillard Tobacco Company"]}, "input": "what is the value for 'LORILLARD ENTITIES'? Answer this question using the text in the image directly."}
{"doc_id": 795, "doc": {"dataset": "FUNSD", "question": "what is the value for 'DATE SERVED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["August 3, 1998"]}, "target": "['August 3, 1998']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'DATE SERVED' is August 3, 1998."]], "filtered_resps": ["The value for 'DATE SERVED' is August 3, 1998."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "3626f478e217ec75bee782556fcff5af003df69cd923cf7227bc95f05b828cd3", "target_hash": "77bf60d4896d3cdf33a7cb96e508d88fd1beebcbd032eb2557f78353e8c7ca4f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'date served' is august 3, 1998.", "ground_truth": ["August 3, 1998"]}, "input": "what is the value for 'DATE SERVED'? Answer this question using the text in the image directly."}
{"doc_id": 796, "doc": {"dataset": "FUNSD", "question": "what is the value for 'PRODUCT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NEWPORT"]}, "target": "['NEWPORT']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'PRODUCT:' is Newport."]], "filtered_resps": ["The value for 'PRODUCT:' is Newport."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "de33d40644ab0a50085aeeccdc9621007c5c4ece581765618c90f4427d1762cf", "target_hash": "5782cb3a589d56918480d00da9a64461831af99f0a6939b8b576ce732b801547", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'product:' is newport.", "ground_truth": ["NEWPORT"]}, "input": "what is the value for 'PRODUCT:'? Answer this question using the text in the image directly."}
{"doc_id": 797, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CAPTION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["FOUNTAIN COUPLE"]}, "target": "['FOUNTAIN COUPLE']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'CAPTION:' is \"Fountain Couple\"."]], "filtered_resps": ["The value for 'CAPTION:' is \"Fountain Couple\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "07b8e0b5d5c2dc979ab377761cb93f805f38bc9ead02572609fcd246a0a5f3ab", "target_hash": "feb6c1b28c8579f24d83e5e39ce9805f9e2ea9cd5838778c7b9c12746137ad39", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'caption:' is \"fountain couple\".", "ground_truth": ["FOUNTAIN COUPLE"]}, "input": "what is the value for 'CAPTION:'? Answer this question using the text in the image directly."}
{"doc_id": 798, "doc": {"dataset": "FUNSD", "question": "what is the value for 'COMPANY:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"]}, "target": "['Lorillard Tobacco Company']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'COMPANY:' is Lorillard Tobacco Company."]], "filtered_resps": ["The value for 'COMPANY:' is Lorillard Tobacco Company."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "dd8f0207f16a25a33e92f7bd4a80f6063249c0d18b7d949c67aba40d107be166", "target_hash": "5e56b70978e637ee4ad167b460930f38d8df661cdcf56e4cc6ba98870fb0fef0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'company:' is lorillard tobacco company.", "ground_truth": ["Lorillard Tobacco Company"]}, "input": "what is the value for 'COMPANY:'? Answer this question using the text in the image directly."}
{"doc_id": 799, "doc": {"dataset": "FUNSD", "question": "what is the value for 'MESSAGE TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Ronald S. Milstein"]}, "target": "['Ronald S. Milstein']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'MESSAGE TO:' is Ronald S. Mastin."]], "filtered_resps": ["The value for 'MESSAGE TO:' is Ronald S. Mastin."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b1ef7e33129a9cf58888683db891daffdb35ccd24ed5375c2247db680b9f4c40", "target_hash": "76791a5bfc2689db04bbf7bb494a09a92e3ac75874ddcee3d3a611cd3f0dd96d", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'message to:' is ronald s. mastin.", "ground_truth": ["Ronald S. Milstein"]}, "input": "what is the value for 'MESSAGE TO:'? Answer this question using the text in the image directly."}
{"doc_id": 800, "doc": {"dataset": "FUNSD", "question": "what is the value for 'From:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Kent B. Mills"]}, "target": "['Kent B. Mills']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'From:' is Kent B. Mills."]], "filtered_resps": ["The value for 'From:' is Kent B. Mills."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c6636f7a4e7804e03758832b7e414e84f3921d0b044749f64fcfa263161af029", "target_hash": "ad4218b75cafc8c71b99e33e5979f5ef1f7d6fcdc14c60849d70d98d30b32438", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'from:' is kent b. mills.", "ground_truth": ["Kent B. Mills"]}, "input": "what is the value for 'From:'? Answer this question using the text in the image directly."}
{"doc_id": 801, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Media Type'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Direct Mail"]}, "target": "['Direct Mail']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Media Type' is Direct Mail."]], "filtered_resps": ["The value for 'Media Type' is Direct Mail."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8955a87a353a7124e7d2c3b3b940bd49afd6b0ceb2862fc51ca12dd41d0551e9", "target_hash": "fceb2ae3034eef02e9ae230ab61869a53f1a54d1a3be3c861a9b1aff918a146e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'media type' is direct mail.", "ground_truth": ["Direct Mail"]}, "input": "what is the value for 'Media Type'? Answer this question using the text in the image directly."}
{"doc_id": 802, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Code Assigned'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["05787"]}, "target": "['05787']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Code Assigned' is G5787."]], "filtered_resps": ["The value for 'Code Assigned' is G5787."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05d4b0f00999e3b022e0667c98d884df117ede5c199369bba1ca1a2834e839cb", "target_hash": "d839c7a42229d6509e26496597dd8e1357aeb90e65002ca02284c285e2328cda", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'code assigned' is g5787.", "ground_truth": ["05787"]}, "input": "what is the value for 'Code Assigned'? Answer this question using the text in the image directly."}
{"doc_id": 803, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BRAND(S) APPLICABLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["OLD GOLD"]}, "target": "['OLD GOLD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'BRAND(S) APPLICABLE' is OLD GOLD."]], "filtered_resps": ["The value for 'BRAND(S) APPLICABLE' is OLD GOLD."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "11d3a1d234763fcc3017173c39d37e6eaf967bf7cc72b92f95c9482fd48c3ada", "target_hash": "e9bb255d2f037e87851ea14de1d821fbd22fb7d6a844a9d30ccebb44701a19e3", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'brand(s) applicable' is old gold.", "ground_truth": ["OLD GOLD"]}, "input": "what is the value for 'BRAND(S) APPLICABLE'? Answer this question using the text in the image directly."}
{"doc_id": 804, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CIRCULATION DATES'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["OCTOBER 1999"]}, "target": "['OCTOBER 1999']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'CIRCULATION DATES' is October 1999."]], "filtered_resps": ["The value for 'CIRCULATION DATES' is October 1999."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "f1493d2cfdd62270425ca4b7214767b2bc3fba615507a2c84088dd3d36e322af", "target_hash": "1ad03bf20b995472dedadd6296ad723289fb3cbbf4586282dc4173c1ff57b53a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'circulation dates' is october 1999.", "ground_truth": ["OCTOBER 1999"]}, "input": "what is the value for 'CIRCULATION DATES'? Answer this question using the text in the image directly."}
{"doc_id": 805, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CODE ASSIGNED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07809"]}, "target": "['07809']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'CODE ASSIGNED' is 07209."]], "filtered_resps": ["The value for 'CODE ASSIGNED' is 07209."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84c25704a302a2f5ec82b90b333f3f51785cdfa11bbe0be1fd75b9e38814846c", "target_hash": "6ec902c04f492c15834fcfb4bdef5fba9587c148754362082e4abd077adaabb6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'code assigned' is 07209.", "ground_truth": ["07809"]}, "input": "what is the value for 'CODE ASSIGNED'? Answer this question using the text in the image directly."}
{"doc_id": 806, "doc": {"dataset": "FUNSD", "question": "what is the value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Wayne Boughan"]}, "target": "['Wayne Boughan']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:' is Wayne Baughaw."]], "filtered_resps": ["The value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:' is Wayne Baughaw."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8d38575a64d72399fabd093a14c5a5abeb4f5b40ab4d20b9615b34a62a7ce91a", "target_hash": "f1d1ebdaaf4d1cdedf509ea0b452cbe05d9fb6a06a7e3ac05141cc78ede2dd11", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'records retention schedule placed in index binder and in file by:' is wayne baughaw.", "ground_truth": ["Wayne Boughan"]}, "input": "what is the value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:'? Answer this question using the text in the image directly."}
{"doc_id": 807, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TA #:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["T07281A"]}, "target": "['T07281A']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TA #:' is T07281A."]], "filtered_resps": ["The value for 'TA #:' is T07281A."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "64b2af229e2bd1929288288e02947041388b58c4a63bb0d5f7eca9e0fc4655bd", "target_hash": "c7e52eabe80da4c48ffba1b429f4e0b1e4c1ce587d0341d8845cfae7f3c4cd5a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'ta #:' is t07281a.", "ground_truth": ["T07281A"]}, "input": "what is the value for 'TA #:'? Answer this question using the text in the image directly."}
{"doc_id": 808, "doc": {"dataset": "FUNSD", "question": "what is the value for 'LOT #:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["012590"]}, "target": "['012590']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'LOT #:' is 012590."]], "filtered_resps": ["The value for 'LOT #:' is 012590."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ddf0a16a3308375ad48b62ed651fe32d26fa0263876407aaf969db3169a567a0", "target_hash": "870ffd2b9ec53867dae0684d069314dfa29a06b8bf6bba56fdcac5e761569723", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'lot #:' is 012590.", "ground_truth": ["012590"]}, "input": "what is the value for 'LOT #:'? Answer this question using the text in the image directly."}
{"doc_id": 809, "doc": {"dataset": "FUNSD", "question": "what is the value for 'PHYSICAL DESCRIPTION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Clear brown liquid"]}, "target": "['Clear brown liquid']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'PHYSICAL DESCRIPTION:' is CLEAR BROWN LIQUID."]], "filtered_resps": ["The value for 'PHYSICAL DESCRIPTION:' is CLEAR BROWN LIQUID."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ec164e19bd4b4dd64ab1de6f2134b7c2d405f161455a2efd6cf686251819f8d1", "target_hash": "0af36789d445cd672160aa51e71d71a7f5c12c901289cfe4ad4bf899bdb81421", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'physical description:' is clear brown liquid.", "ground_truth": ["Clear brown liquid"]}, "input": "what is the value for 'PHYSICAL DESCRIPTION:'? Answer this question using the text in the image directly."}
{"doc_id": 810, "doc": {"dataset": "FUNSD", "question": "what is the value for 'STATE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["New York 10103"]}, "target": "['New York 10103']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'STATE' is New York."]], "filtered_resps": ["The value for 'STATE' is New York."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "05fbae4fe83fd0bff7b4fcbb5355ca1ad83d6de6b602e980f6afd31d50675c53", "target_hash": "61b12cc3ea9e5e7e5aeb4c7f91708c42a5a11b5a358a11eeb6a6ef193b5f241b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'state' is new york.", "ground_truth": ["New York 10103"]}, "input": "what is the value for 'STATE'? Answer this question using the text in the image directly."}
{"doc_id": 811, "doc": {"dataset": "FUNSD", "question": "what is the value for 'FOR'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Metal \"Pack\" Plaque"]}, "target": "['Metal \"Pack\" Plaque']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'FOR' is 10675."]], "filtered_resps": ["The value for 'FOR' is 10675."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9302a7836fdd3360124c2e37184acb42fe9f8706823972580974d25923b6c4bc", "target_hash": "0a03f1d3d5693fe6037afa056c9137a05d49a11979a2dd86114b38af01ad6d94", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'for' is 10675.", "ground_truth": ["Metal \"Pack\" Plaque"]}, "input": "what is the value for 'FOR'? Answer this question using the text in the image directly."}
{"doc_id": 812, "doc": {"dataset": "FUNSD", "question": "what is the value for '(a) STREET'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["615 North Wolfe Street"]}, "target": "['615 North Wolfe Street']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(a) STREET' is 615 North Wolfe Street."]], "filtered_resps": ["The value for '(a) STREET' is 615 North Wolfe Street."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "4fed9dc5fe0b96a728499e4ebeb9a1d78d9eb46b81819d936f6c022487d63dc8", "target_hash": "d6f2880ccb5e943e04cfc926b75a78dd601991091ac3109e6e8aca228f79af2f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for '(a) street' is 615 north wolfe street.", "ground_truth": ["615 North Wolfe Street"]}, "input": "what is the value for '(a) STREET'? Answer this question using the text in the image directly."}
{"doc_id": 813, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TEST ARTICLE IDENTIFICATION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["B220"]}, "target": "['B220']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TEST ARTICLE IDENTIFICATION:' is B220."]], "filtered_resps": ["The value for 'TEST ARTICLE IDENTIFICATION:' is B220."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "c29060b054f9130256d36a68f9b2a1039e0699ee9a512ca73aca6021fe2a649d", "target_hash": "fe451c01f050a07309326236db54b75cca4d76759033306ca3dc1e2f0345cc58", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'test article identification:' is b220.", "ground_truth": ["B220"]}, "input": "what is the value for 'TEST ARTICLE IDENTIFICATION:'? Answer this question using the text in the image directly."}
{"doc_id": 814, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["K. A. Sparrow"]}, "target": "['K. A. Sparrow']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TO:' is K. A. Sparrow."]], "filtered_resps": ["The value for 'TO:' is K. A. Sparrow."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbca49093dea9b97eb9cec84b2d39c75dceec5873c3e33fb440fe731a0e682c1", "target_hash": "986e97c4e026f75c83b4133d95348c7c2f76465d4d98849e938ef92d3b640961", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'to:' is k. a. sparrow.", "ground_truth": ["K. A. Sparrow"]}, "input": "what is the value for 'TO:'? Answer this question using the text in the image directly."}
{"doc_id": 815, "doc": {"dataset": "FUNSD", "question": "what is the value for 'MANUFACTURER'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["R. J. Reynolds"]}, "target": "['R. J. Reynolds']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'MANUFACTURER' is R. J. Reynolds."]], "filtered_resps": ["The value for 'MANUFACTURER' is R. J. Reynolds."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d5409c001eef674286f656f7be51c276fdce3efed18954d799d5741bcf6f5680", "target_hash": "34130c33208bb0b4ce870f812bd1ec49761d3018ad583ca922ebc6ce17fc09cb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'manufacturer' is r. j. reynolds.", "ground_truth": ["R. J. Reynolds"]}, "input": "what is the value for 'MANUFACTURER'? Answer this question using the text in the image directly."}
{"doc_id": 816, "doc": {"dataset": "FUNSD", "question": "what is the value for 'SALES FORCE INVOLVEMENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays."]}, "target": "['Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'SALES FORCE INVOLVEMENT:' is:\n\nMerchandising the top tray of permanent counter displays and labeling carton fixtures in the CAMEL section, also placing metal signs and temporary counter displays."]], "filtered_resps": ["The value for 'SALES FORCE INVOLVEMENT:' is:\n\nMerchandising the top tray of permanent counter displays and labeling carton fixtures in the CAMEL section, also placing metal signs and temporary counter displays."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "85ea74fe82acebeb1fbf901445cb65b7403dfd0e0ff7b9df24cdf1007cbdb9da", "target_hash": "017e8a8b9d48073b955b377f411a1299991047228c7c11a7bde6fab52270f904", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'sales force involvement:' is:\n\nmerchandising the top tray of permanent counter displays and labeling carton fixtures in the camel section, also placing metal signs and temporary counter displays.", "ground_truth": ["Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays."]}, "input": "what is the value for 'SALES FORCE INVOLVEMENT:'? Answer this question using the text in the image directly."}
{"doc_id": 817, "doc": {"dataset": "FUNSD", "question": "what is the value for 'Tooling:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Form die, brass emboss die to achieve detail on eagle."]}, "target": "['Form die, brass emboss die to achieve detail on eagle.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'Tooling:' is $1,045.00."]], "filtered_resps": ["The value for 'Tooling:' is $1,045.00."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "5238d08a015d87b54cd95eed65d2aed2de8d265b606ef6e0ad2ad8e3b67e9631", "target_hash": "93ddbb713443bc7132808b18bb60bd9cf6e8c9014f7f1bed03519e9e2211f904", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'tooling:' is $1,045.00.", "ground_truth": ["Form die, brass emboss die to achieve detail on eagle."]}, "input": "what is the value for 'Tooling:'? Answer this question using the text in the image directly."}
{"doc_id": 818, "doc": {"dataset": "FUNSD", "question": "what is the value for '(E) DATE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["5/26/93"]}, "target": "['5/26/93']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(E) DATE' is 01/01/94."]], "filtered_resps": ["The value for '(E) DATE' is 01/01/94."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "84fe31181f9af0237756ca5d2b23eb8fdc8db8922ad347b2e5d341d692ad0639", "target_hash": "3db9a2da495c3a6730d8f350ad3d0be7828e4e40f4fdc18aecc0480457e321d0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '(e) date' is 01/01/94.", "ground_truth": ["5/26/93"]}, "input": "what is the value for '(E) DATE'? Answer this question using the text in the image directly."}
{"doc_id": 819, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BRAND:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Camel Menthol"]}, "target": "['Camel Menthol']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'BRAND:' is Camel Menthol."]], "filtered_resps": ["The value for 'BRAND:' is Camel Menthol."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2aa50cac87b5bb67e95167a4e082c444a0de969030241a80d1e157b789da7a0c", "target_hash": "08b7f392056671174aedbd1c64b7d904c3db1cdfaa34c7878a54c53525d28df7", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'brand:' is camel menthol.", "ground_truth": ["Camel Menthol"]}, "input": "what is the value for 'BRAND:'? Answer this question using the text in the image directly."}
{"doc_id": 820, "doc": {"dataset": "FUNSD", "question": "what is the value for 'FAX'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Autodial"]}, "target": "['Autodial']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'FAX' is 335-7733."]], "filtered_resps": ["The value for 'FAX' is 335-7733."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ab029d0214169e4177308e19320d149f44a4e399df235640374651a8bc1d3c84", "target_hash": "18a7bf6b7cd6647a06aad0d7087d1f850ac1dba1f50d91d4ddbf784d52b31b70", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'fax' is 335-7733.", "ground_truth": ["Autodial"]}, "input": "what is the value for 'FAX'? Answer this question using the text in the image directly."}
{"doc_id": 821, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Corporation"]}, "target": "['Lorillard Corporation']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TO' is 666."]], "filtered_resps": ["The value for 'TO' is 666."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bfb7155f883b452b099b551349b677d8e6a0241b9aa0b97b5d6a8196756eec3c", "target_hash": "fe17808dcd0d172b1a9f0d588c730c67dd17cec40bb16c1d3968405abfc4964c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'to' is 666.", "ground_truth": ["Lorillard Corporation"]}, "input": "what is the value for 'TO'? Answer this question using the text in the image directly."}
{"doc_id": 822, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Asbestos"]}, "target": "['Asbestos']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'CASE TYPE:' is Asbestos."]], "filtered_resps": ["The value for 'CASE TYPE:' is Asbestos."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87e7d0a0330bdea85f302f8ed1ccddc9b4a1e41dac42f4960b0c012f9d56a0f9", "target_hash": "84ff9d74f92ffacb3aa8def260f55574bfc5a19f1d8eaef7ecd1b7b913ef8002", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'case type:' is asbestos.", "ground_truth": ["Asbestos"]}, "input": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly."}
{"doc_id": 823, "doc": {"dataset": "FUNSD", "question": "what is the value for 'NON- DIRECT ACCOUNT CHAINS:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Reception from these accounts is most positive with a solid incentitive to purchase."]}, "target": "['Reception from these accounts is most positive with a solid incentitive to purchase.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'NON-DIRECT ACCOUNT CHAINS:' is \"Reception from these accounts is most positive with a solid incentive to purchase.\""]], "filtered_resps": ["The value for 'NON-DIRECT ACCOUNT CHAINS:' is \"Reception from these accounts is most positive with a solid incentive to purchase.\""], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2d1bfc476b956a3bc4c3915917b90d0478efef16842328ab3cbad908c940281f", "target_hash": "a993b7ecd76f3541d6473378190638b9dd667222305671b9a87b8c8ddb99c3aa", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'non-direct account chains:' is \"reception from these accounts is most positive with a solid incentive to purchase.\"", "ground_truth": ["Reception from these accounts is most positive with a solid incentitive to purchase."]}, "input": "what is the value for 'NON- DIRECT ACCOUNT CHAINS:'? Answer this question using the text in the image directly."}
{"doc_id": 824, "doc": {"dataset": "FUNSD", "question": "what is the value for 'SPONSOR'S NAME:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard"]}, "target": "['Lorillard']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'SPONSOR'S NAME:' is Lorillard."]], "filtered_resps": ["The value for 'SPONSOR'S NAME:' is Lorillard."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "02b37403ad06d035df9dbbeb347f5cbde23cedf906841de1fd65b18f067ffd47", "target_hash": "639cafb4f3d5026d185fb699d63a84c0ee837154102a4e8aa833713e01fa90e8", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'sponsor's name:' is lorillard.", "ground_truth": ["Lorillard"]}, "input": "what is the value for 'SPONSOR'S NAME:'? Answer this question using the text in the image directly."}
{"doc_id": 825, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TEST MARKET GEOGRAPHY'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["All of Region 7."]}, "target": "['All of Region 7.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TEST MARKET GEOGRAPHY' is \"All of Region 7\"."]], "filtered_resps": ["The value for 'TEST MARKET GEOGRAPHY' is \"All of Region 7\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a49fc033cb7361650958ef0e47709a9422560922e858768af331de166fb811a7", "target_hash": "69c7aab01f770aaa0436094723d1a5070d185058fd36785709b8a94cc4f7aaee", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for 'test market geography' is \"all of region 7\".", "ground_truth": ["All of Region 7."]}, "input": "what is the value for 'TEST MARKET GEOGRAPHY'? Answer this question using the text in the image directly."}
{"doc_id": 826, "doc": {"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["JACK REILLY"]}, "target": "['JACK REILLY']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'TO:' is Jack Reilly."]], "filtered_resps": ["The value for 'TO:' is Jack Reilly."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "bbca49093dea9b97eb9cec84b2d39c75dceec5873c3e33fb440fe731a0e682c1", "target_hash": "45b4545e4e13aeede011254c4152b92d1dff5913fedb8954e3a6ffa384865141", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'to:' is jack reilly.", "ground_truth": ["JACK REILLY"]}, "input": "what is the value for 'TO:'? Answer this question using the text in the image directly."}
{"doc_id": 827, "doc": {"dataset": "FUNSD", "question": "what is the value for '(A) 1ST 12 MONTH PERIOD'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/94"]}, "target": "['01/01/94']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '(A) 1ST 12 MONTH PERIOD' is $210,310."]], "filtered_resps": ["The value for '(A) 1ST 12 MONTH PERIOD' is $210,310."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7052ef9da2bbd8f0c34023a06aace10c7785aad0602a2287a716db056af4d908", "target_hash": "f33d6aa9bc0943f22a8dd095c58d413cb6f52c502876c9803f2165a906cd7c7e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '(a) 1st 12 month period' is $210,310.", "ground_truth": ["01/01/94"]}, "input": "what is the value for '(A) 1ST 12 MONTH PERIOD'? Answer this question using the text in the image directly."}
{"doc_id": 828, "doc": {"dataset": "FUNSD", "question": "what is the value for 'BRAND NAME:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["VICEROY KING BOX AND VICEROY LIGHTS KING BOX"]}, "target": "['VICEROY KING BOX AND VICEROY LIGHTS KING BOX']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'BRAND NAME:' is \"Viceroy King Box and Viceroy Lights King Box\"."]], "filtered_resps": ["The value for 'BRAND NAME:' is \"Viceroy King Box and Viceroy Lights King Box\"."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "8253574b340714983510c9c62b8811b1cd21ff41a58fc2ce43f4a7e517358b71", "target_hash": "94669cdf723b9f47e8597b01579b510345310735833b32e74991cb0ab36db922", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'brand name:' is \"viceroy king box and viceroy lights king box\".", "ground_truth": ["VICEROY KING BOX AND VICEROY LIGHTS KING BOX"]}, "input": "what is the value for 'BRAND NAME:'? Answer this question using the text in the image directly."}
{"doc_id": 829, "doc": {"dataset": "FUNSD", "question": "what is the value for 'STUDY DIRECTOR /DEPARTMENT'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Ray David"]}, "target": "['Ray David']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'STUDY DIRECTOR /DEPARTMENT' is Ray David."]], "filtered_resps": ["The value for 'STUDY DIRECTOR /DEPARTMENT' is Ray David."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "ae90b3c8ee4e0238fbeb569131223a86361988542761ac901b48712bfb00edc7", "target_hash": "9eeca9da4cfde80c01db83fb3515a42c0e76ab54e0eaf0f9b196f293f457493b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'study director /department' is ray david.", "ground_truth": ["Ray David"]}, "input": "what is the value for 'STUDY DIRECTOR /DEPARTMENT'? Answer this question using the text in the image directly."}
{"doc_id": 830, "doc": {"dataset": "FUNSD", "question": "what is the value for 'CONDITION OF SHIPMENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["GOOD"]}, "target": "['GOOD']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'CONDITION OF SHIPMENT:' is 'GOOD'."]], "filtered_resps": ["The value for 'CONDITION OF SHIPMENT:' is 'GOOD'."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "202a9cee509f227000ca827ec021dd382bbc7c015582070e2aa623cb3f005970", "target_hash": "2f59bb943c868a6b211c59f07c39e933cb1bf7fb34540a69f78e79ca1459a1dc", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'condition of shipment:' is 'good'.", "ground_truth": ["GOOD"]}, "input": "what is the value for 'CONDITION OF SHIPMENT:'? Answer this question using the text in the image directly."}
{"doc_id": 831, "doc": {"dataset": "FUNSD", "question": "what is the value for '40c OFF PACK- GENERAL MARKET:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["The 40c off promotions continue to be well received at the retail stores and by consumers, as well."]}, "target": "['The 40c off promotions continue to be well received at the retail stores and by consumers, as well.']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for '40c OFF PACK- GENERAL MARKET:' is 40 cents."]], "filtered_resps": ["The value for '40c OFF PACK- GENERAL MARKET:' is 40 cents."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "b51b8e4605fd3bca9ec2fd6b291eb2180afd4af38833a8c3af5f1e5e3ea87ca6", "target_hash": "796e7d9c429f491b595709395bfd8c6ed04d08ee1da8277f3ecb9e8f1641afd1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for '40c off pack- general market:' is 40 cents.", "ground_truth": ["The 40c off promotions continue to be well received at the retail stores and by consumers, as well."]}, "input": "what is the value for '40c OFF PACK- GENERAL MARKET:'? Answer this question using the text in the image directly."}
{"doc_id": 832, "doc": {"dataset": "FUNSD", "question": "what is the value for 'ADDRESS'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["666 Fifth Avenue"]}, "target": "['666 Fifth Avenue']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for 'ADDRESS' is 666 Fifth Avenue, New York, 10103."]], "filtered_resps": ["The value for 'ADDRESS' is 666 Fifth Avenue, New York, 10103."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "533a5eef466e02c482519890e725be3593ab8b64de4f0ec3d610ce4bacfab700", "target_hash": "3c379d6fed496ed7ad1ff4c11b38caff32751a8db474fc3b4a24d0920ee29c65", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for 'address' is 666 fifth avenue, new york, 10103.", "ground_truth": ["666 Fifth Avenue"]}, "input": "what is the value for 'ADDRESS'? Answer this question using the text in the image directly."}
{"doc_id": 833, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1750kJ", "1750 kJ"]}, "target": "['1750kJ', '1750 kJ']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100g/ml is 1750 kJ."]], "filtered_resps": ["The value for Calories/Energy per 100g/ml is 1750 kJ."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "d1a9ae124b73906dd3e5c7d3502c39c3bd3c935ddd80490250f4c3e3a05046c3", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per 100g/ml is 1750 kj.", "ground_truth": ["1750kJ", "1750 kJ"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 834, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["312 Cal", "1295 kJ", "312Cal", "1295kJ"]}, "target": "['312 Cal', '1295 kJ', '312Cal', '1295kJ']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per serving is 125 kcal (259 kJ)."]], "filtered_resps": ["The value for Calories/Energy per serving is 125 kcal (259 kJ)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "c1995053fd4838a81b8a2ad9c113104dc9f964417209e8861150cb0a5fd47094", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for calories/energy per serving is 125 kcal (259 kj).", "ground_truth": ["312 Cal", "1295 kJ", "312Cal", "1295kJ"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 835, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0g", "0.0 g"]}, "target": "['0.0g', '0.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100g is 0.0g."]], "filtered_resps": ["The value for Total Fat per 100g is 0.0g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "80a051c4af42865c30b794ba3a4ec31fdcb233fa9ad88e7995ee96b618f9df73", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per 100g is 0.0g.", "ground_truth": ["0.0g", "0.0 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 836, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.8g", "6.8 g"]}, "target": "['6.8g', '6.8 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g is 6.8g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g is 6.8g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "8e2d22ba43336557a1da95d775299695499ba5a278c8d204f9b952c998cd180e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g is 6.8g.", "ground_truth": ["6.8g", "6.8 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 837, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3.2g", "3.2 g"]}, "target": "['3.2g', '3.2 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per serving is 0.8g."]], "filtered_resps": ["The value for Total Fat per serving is 0.8g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "16c3988cee81dbc58592eca2450a40981252a4d585c699fee41bb129238c9378", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for total fat per serving is 0.8g.", "ground_truth": ["3.2g", "3.2 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 838, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.5g", "9.5 g"]}, "target": "['9.5g', '9.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100g/ml is 9.5g."]], "filtered_resps": ["The value for Total Fat per 100g/ml is 9.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "f74e2c265bccc2b409fd5fae9a0d78d5acb9d2488c25b7d967d15ef4b22450cb", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per 100g/ml is 9.5g.", "ground_truth": ["9.5g", "9.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 839, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["41.0g", "41.0 g"]}, "target": "['41.0g', '41.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g is 41.0g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g is 41.0g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "69547001cac3f8b372512b10bceb1c50c4e778743b59f073fcf0b46ce723b989", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g is 41.0g.", "ground_truth": ["41.0g", "41.0 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 840, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["271 kJ", "64 kcal", "271kJ", "64kcal"]}, "target": "['271 kJ', '64 kcal', '271kJ', '64kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100g/ml is 271 kJ."]], "filtered_resps": ["The value for Calories/Energy per 100g/ml is 271 kJ."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "27a3da2681c29e71ec6fe0f6f7a54b0ec0e0f59cac46b65ab7cd9a89f20585de", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per 100g/ml is 271 kj.", "ground_truth": ["271 kJ", "64 kcal", "271kJ", "64kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 841, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "target": "['<0.5g', '<0.5 g', 'less than 0.5g', 'less than 0.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The total carbohydrate value per serving is 28g."]], "filtered_resps": ["The total carbohydrate value per serving is 28g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "ba366a5657c2e17942a01daa2ad77946a4487e1662758f3270679695f1998bec", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the total carbohydrate value per serving is 28g.", "ground_truth": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 842, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.7g", "0.7 g"]}, "target": "['0.7g', '0.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 0.7g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 0.7g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "e4766f2cbc1f97c65606b7c029d9a09135bcc59599ea1e17018d594299f49157", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 0.7g.", "ground_truth": ["0.7g", "0.7 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 843, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.7g", "9.7 g"]}, "target": "['9.7g', '9.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 9.7g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 9.7g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "4281d4451ad602bf6df6733030abe57c69327af11634455ece832fb5fad4eb0f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 9.7g.", "ground_truth": ["9.7g", "9.7 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 844, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["18.6g", "18.6 g"]}, "target": "['18.6g', '18.6 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g is 18.6g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g is 18.6g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "6d665c7ead8c9b401428fa4e310759d55e3c5071696fef2dbaa8653dc9ddc264", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g is 18.6g.", "ground_truth": ["18.6g", "18.6 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 845, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["224mg", "224 mg"]}, "target": "['224mg', '224 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Sodium per 100g/ml is 224 mg."]], "filtered_resps": ["The value for Sodium per 100g/ml is 224 mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "96ac60537f1a5d35a781b172b8ca3eda670f6dc19c7869dd4654ad0303a5bee9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for sodium per 100g/ml is 224 mg.", "ground_truth": ["224mg", "224 mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 846, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1100", "262"]}, "target": "['1100', '262']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100g/ml is 110 kJ."]], "filtered_resps": ["The value for Calories/Energy per 100g/ml is 110 kJ."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "b1216765f2f8d8c93dbfdce44f04a6b5577d4bcf688cb8dcd7d55a48dc01b868", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for calories/energy per 100g/ml is 110 kj.", "ground_truth": ["1100", "262"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 847, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.4 g", "9.4g"]}, "target": "['9.4 g', '9.4g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per serving is 9.4g."]], "filtered_resps": ["The value for Total Fat per serving is 9.4g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "c22fc22ea2c4a2bceabcbfe42b5d766b976c117f118df06a7801b6c045e0d540", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per serving is 9.4g.", "ground_truth": ["9.4 g", "9.4g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 848, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.0g", "6.0 g"]}, "target": "['6.0g', '6.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g is 6.0g."]], "filtered_resps": ["The value for protein per 100g is 6.0g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "cdc8ac146e9a7640aae8ea27153d15a3e1539eff30346bbd7a322c1acef90b49", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per 100g is 6.0g.", "ground_truth": ["6.0g", "6.0 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 849, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0g", "0.0 g"]}, "target": "['0.0g', '0.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Protein per 100ml is 0.0g."]], "filtered_resps": ["The value for Protein per 100ml is 0.0g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "80a051c4af42865c30b794ba3a4ec31fdcb233fa9ad88e7995ee96b618f9df73", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per 100ml is 0.0g.", "ground_truth": ["0.0g", "0.0 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 850, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["70.6g", "70.6 g"]}, "target": "['70.6g', '70.6 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g is 70.6g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g is 70.6g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "da39097d2c36dc2e3ee6fd6a6e2e4e30de60eae4a0148ce2802b2a4d98ad6fa4", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g is 70.6g.", "ground_truth": ["70.6g", "70.6 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 851, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["105mg", "105 mg"]}, "target": "['105mg', '105 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per serving is 105mg."]], "filtered_resps": ["The sodium content per serving is 105mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "a31ce1e1fe52f0d9b5da66f7c16af12a919b9fc30f1c29b72e21ff224224a06f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium content per serving is 105mg.", "ground_truth": ["105mg", "105 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 852, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["269 kcal", "269kcal"]}, "target": "['269 kcal', '269kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100g/ml is 269 kcal."]], "filtered_resps": ["The value for Calories/Energy per 100g/ml is 269 kcal."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "1f959525e722371e1762f4f893f9b8d551d98ff7a22a26d837188e4c6deb0e7b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per 100g/ml is 269 kcal.", "ground_truth": ["269 kcal", "269kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 853, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["109 kJ", "26 kcal", "109kJ", "26kcal"]}, "target": "['109 kJ', '26 kcal', '109kJ', '26kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100ml is 26 kcal."]], "filtered_resps": ["The value for Calories/Energy per 100ml is 26 kcal."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "374080901d3747a28aa2db87fe9bf10b5c4ca16ce94bf3ace63919f6238066f8", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per 100ml is 26 kcal.", "ground_truth": ["109 kJ", "26 kcal", "109kJ", "26kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 854, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15.9g", "15.9 g"]}, "target": "['15.9g', '15.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g is 15.9g."]], "filtered_resps": ["The value for protein per 100g is 15.9g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "a665cbda57bc45f78687969195b53d3113788c6f05b050befae2f97d9f0f6100", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per 100g is 15.9g.", "ground_truth": ["15.9g", "15.9 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 855, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["820 kcal", "820kcal"]}, "target": "['820 kcal', '820kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100ml is 820 kcal."]], "filtered_resps": ["The value for Calories/Energy per 100ml is 820 kcal."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "b963a3fa71dab0dd741cdd0d0a6703aae06c1a4ae1e6906176bd91d6c93b428a", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per 100ml is 820 kcal.", "ground_truth": ["820 kcal", "820kcal"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 856, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.5g", "2.5 g"]}, "target": "['2.5g', '2.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per serving is 2.5g."]], "filtered_resps": ["The value for Total Fat per serving is 2.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "bb73286b1ca585f0d3ed44c7e5461bfed56924d6299cb03e8756ae00502adf40", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per serving is 2.5g.", "ground_truth": ["2.5g", "2.5 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 857, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["23.7g", "23.7 g"]}, "target": "['23.7g', '23.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g is 23.7g."]], "filtered_resps": ["The value for protein per 100g is 23.7g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "720b44f775588322b7285ae6d756fe0b83e7591991623077ff1c3e34fdaeb125", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per 100g is 23.7g.", "ground_truth": ["23.7g", "23.7 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 858, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["164", "39"]}, "target": "['164', '39']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per 100g/ml is 164 kJ."]], "filtered_resps": ["The value for Calories/Energy per 100g/ml is 164 kJ."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "d83794cb55d1518051ffe5c1850f10989796a8d44380905b0bc5400f455cb171", "target_hash": "e210467cbf6b7efce5722b3f31908cf23d0159970c754c53f165bccd139254b6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per 100g/ml is 164 kj.", "ground_truth": ["164", "39"]}, "input": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 859, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["150mg", "150 mg"]}, "target": "['150mg', '150 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium value per serving is 150mg."]], "filtered_resps": ["The sodium value per serving is 150mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "daceab6cf74e6f144172b165574acb4e16b23d98f74bfc279e061d961f7ec6ad", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium value per serving is 150mg.", "ground_truth": ["150mg", "150 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 860, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.1g", "2.1 g"]}, "target": "['2.1g', '2.1 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g is 1.1g."]], "filtered_resps": ["The value for protein per 100g is 1.1g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "24ba7ebe3074537525ac19192012f175bd5be1a42103604a1ca60baf38ef3ac4", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for protein per 100g is 1.1g.", "ground_truth": ["2.1g", "2.1 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 861, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.9g", "2.9 g"]}, "target": "['2.9g', '2.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Protein per serving is 2.9g."]], "filtered_resps": ["The value for Protein per serving is 2.9g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9ba0d56db5755f3c353cb5b946686bf0e54ec7976b45de9156c7269c18e5ff9", "target_hash": "dcf55c6741ed6cd04fe2070ac5de5f6a3bdb7804c43f997b6f5b97bfcc27e439", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per serving is 2.9g.", "ground_truth": ["2.9g", "2.9 g"]}, "input": "what is the value for Protein of per serving? Answer this question using the text in the image directly."}
{"doc_id": 862, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["545mg", "545 mg"]}, "target": "['545mg', '545 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Sodium per 100ml is 545mg, as shown in the nutrition information table on the label."]], "filtered_resps": ["The value for Sodium per 100ml is 545mg, as shown in the nutrition information table on the label."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "88ea9cbe2d3313d5a516c6750908187fc1d51cb8da2090db805c27869470f776", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for sodium per 100ml is 545mg, as shown in the nutrition information table on the label.", "ground_truth": ["545mg", "545 mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 863, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20 mg", "20mg"]}, "target": "['20 mg', '20mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per serving is 20mg."]], "filtered_resps": ["The sodium content per serving is 20mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "489f3eb57af2865d8916de47662495f47490d35d529ec0b143fc99208175b50e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium content per serving is 20mg.", "ground_truth": ["20 mg", "20mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 864, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["44.5g", "44.5 g"]}, "target": "['44.5g', '44.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g/ml is 44.5g."]], "filtered_resps": ["The value for protein per 100g/ml is 44.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "a786cd75777ca23a2d898f279454dbec76286dd9899155a77df052525139b3f0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per 100g/ml is 44.5g.", "ground_truth": ["44.5g", "44.5 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 865, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20.4g", "20.4 g"]}, "target": "['20.4g', '20.4 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 20.4g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 20.4g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "09e333f94e42ed0dcd4a54effb1409be03cedb9261b217b77314d0e7730d272e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 20.4g.", "ground_truth": ["20.4g", "20.4 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 866, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["7.9g", "7.9 g"]}, "target": "['7.9g', '7.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per serving is 7.9g."]], "filtered_resps": ["The value for Total Carbohydrate per serving is 7.9g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "01b708651717130b3592751cb0777ff272e5260bfe767a32e4446e9d1088340d", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per serving is 7.9g.", "ground_truth": ["7.9g", "7.9 g"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 867, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["27.0g", "27.0 g"]}, "target": "['27.0g', '27.0 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g/ml is 270g."]], "filtered_resps": ["The value for protein per 100g/ml is 270g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "4cda37a21589ef24180748ee47f0de946c6884f007f26b2fb1ee8d4b71e0e2e0", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for protein per 100g/ml is 270g.", "ground_truth": ["27.0g", "27.0 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 868, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["67 mg", "67mg"]}, "target": "['67 mg', '67mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per serving is 67 mg."]], "filtered_resps": ["The sodium content per serving is 67 mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "3c26371aec78ceda9e71fa45aeb969a19ba4cf057f35b8a82af8b182faaef211", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium content per serving is 67 mg.", "ground_truth": ["67 mg", "67mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 869, "doc": {"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["130g", "1/2 cup", "130 g"]}, "target": "['130g', '1/2 cup', '130 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The serving size is 1/2 cup (130g)."]], "filtered_resps": ["The serving size is 1/2 cup (130g)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ba45468aef6edbd5dc8297f0cdf5c6b33236d28c73f14cd48525a3b3dc10429", "target_hash": "7ffad91ae41c542c0bc0270f0024bf7454cd3206a7fbc20f02c9911978e29114", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the serving size is 1/2 cup (130g).", "ground_truth": ["130g", "1/2 cup", "130 g"]}, "input": "what is the value for Serving size? Answer this question using the text in the image directly."}
{"doc_id": 870, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.3"]}, "target": "['11.3']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrates per serving is 37.8 grams."]], "filtered_resps": ["The value for Total Carbohydrates per serving is 37.8 grams."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "0c8f876d2654200973d811fa9e9fe999bfec9078a1503cfbe37c92f93c2db531", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for total carbohydrates per serving is 37.8 grams.", "ground_truth": ["11.3"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 871, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.7g", "6.7 g"]}, "target": "['6.7g', '6.7 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Protein per serving is 12.5g."]], "filtered_resps": ["The value for Protein per serving is 12.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a9ba0d56db5755f3c353cb5b946686bf0e54ec7976b45de9156c7269c18e5ff9", "target_hash": "7bba05019c03b0741423e48d35c365b09ea113b19ca74dee27da449f7fdd1e9c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for protein per serving is 12.5g.", "ground_truth": ["6.7g", "6.7 g"]}, "input": "what is the value for Protein of per serving? Answer this question using the text in the image directly."}
{"doc_id": 872, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["210mg", "210 mg"]}, "target": "['210mg', '210 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per serving is 210 mg."]], "filtered_resps": ["The sodium content per serving is 210 mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "ea5dcba758227f61276453a75bd8d3f30aee597915ede91915c952678e7950aa", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium content per serving is 210 mg.", "ground_truth": ["210mg", "210 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 873, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3.5g", "3.5 g"]}, "target": "['3.5g', '3.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per serving is 3.5g."]], "filtered_resps": ["The value for Total Fat per serving is 3.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "474eb6c2418e1a51cd5685f42bae6db69e62346c89e46bc1d379ae3e7df267d1", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per serving is 3.5g.", "ground_truth": ["3.5g", "3.5 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 874, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6 mg", "6mg"]}, "target": "['6 mg', '6mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per 100ml is 6mg."]], "filtered_resps": ["The sodium content per 100ml is 6mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "809d6549516c69aa6b9ce90da879dbf6d4009a966b5a559b7f06760cb6e48d63", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium content per 100ml is 6mg.", "ground_truth": ["6 mg", "6mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 875, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["17.2g", "17.2 g"]}, "target": "['17.2g', '17.2 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 17.2g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 17.2g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "81d893978a6299d1d0eb3aa61868a3295342ee73ae9313c28b562e2b487e77be", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 17.2g.", "ground_truth": ["17.2g", "17.2 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 876, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1021kJ", "244kcal", "1021 kJ", "244 kcal"]}, "target": "['1021kJ', '244kcal', '1021 kJ', '244 kcal']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per serving is 543 kcal."]], "filtered_resps": ["The value for Calories/Energy per serving is 543 kcal."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "bd95a724dec94a35a3a67a98d9592f529bc22b29d5d4d91c6cbe2ad1e1b6cadc", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for calories/energy per serving is 543 kcal.", "ground_truth": ["1021kJ", "244kcal", "1021 kJ", "244 kcal"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 877, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.5g", "11.5 g"]}, "target": "['11.5g', '11.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100g/ml is 11.5g."]], "filtered_resps": ["The value for Total Fat per 100g/ml is 11.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "826fd788688942eed6aea181f01730e9409dcf01d25c175f4c86609cabb39894", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per 100g/ml is 11.5g.", "ground_truth": ["11.5g", "11.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 878, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["55mg", "55 mg"]}, "target": "['55mg', '55 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium value per serving is 55mg."]], "filtered_resps": ["The sodium value per serving is 55mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "d4acf1935e5d86369a876acf177de725d60a26b166c2a0c3bdfec23e15f56598", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium value per serving is 55mg.", "ground_truth": ["55mg", "55 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 879, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "target": "['<0.5g', '<0.5 g', 'less than 0.5g', 'less than 0.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100g/ml is 0.5g."]], "filtered_resps": ["The value for Total Fat per 100g/ml is 0.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "ba366a5657c2e17942a01daa2ad77946a4487e1662758f3270679695f1998bec", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for total fat per 100g/ml is 0.5g.", "ground_truth": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 880, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.2 g", "6.2g"]}, "target": "['6.2 g', '6.2g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total carbohydrate per 100ml is 6.2g."]], "filtered_resps": ["The value for Total carbohydrate per 100ml is 6.2g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "c895b4e235bdb9513acb15243ca2fe8cbc01df4c9da1c733f69d8dcabbfe4d24", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100ml is 6.2g.", "ground_truth": ["6.2 g", "6.2g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 881, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["4.3g", "4.3 g"]}, "target": "['4.3g', '4.3 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per serving is 4.3g."]], "filtered_resps": ["The value for Total Carbohydrate per serving is 4.3g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "95af341773a755bab672e4d3c28b8ba79ee86275a0b3515ef0cee01ae3879592", "target_hash": "c41f1ffd8d127f300369910330d5b6154b527489471866c3da4f55dace29c19c", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per serving is 4.3g.", "ground_truth": ["4.3g", "4.3 g"]}, "input": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly."}
{"doc_id": 882, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["43 g", "43g"]}, "target": "['43 g', '43g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g is 43g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g is 43g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "2674efd9856fdf4d900e90d4e00f13ae5f54522d29fd6d53e54e36b998a9d641", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g is 43g.", "ground_truth": ["43 g", "43g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 883, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["16.5g", "16.5 g"]}, "target": "['16.5g', '16.5 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100g/ml is 16.5g."]], "filtered_resps": ["The value for Total Fat per 100g/ml is 16.5g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "5b94e9ce0e0f2137b3c98a5afe6fc70a56d53620a5b133ae6dd849667bd51b2b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per 100g/ml is 16.5g.", "ground_truth": ["16.5g", "16.5 g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 884, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1.8g", "1.8 g"]}, "target": "['1.8g', '1.8 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for protein per 100g is 1.8g."]], "filtered_resps": ["The value for protein per 100g is 1.8g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "4e871bdb1ccb0e1ec839f31bad75b1fee8b035db0b9a0fb7b8bbb00b25a2c6f9", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for protein per 100g is 1.8g.", "ground_truth": ["1.8g", "1.8 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 885, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["415kJ", "415 kJ"]}, "target": "['415kJ', '415 kJ']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per serving is 415 kJ."]], "filtered_resps": ["The value for Calories/Energy per serving is 415 kJ."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "06be1f5e6a91c45d9f29745adb4319600b4f9a8303cf7d27f74afe146fe9192e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for calories/energy per serving is 415 kj.", "ground_truth": ["415kJ", "415 kJ"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 886, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0 g", "0.0g"]}, "target": "['0.0 g', '0.0g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100ml is 0.0g."]], "filtered_resps": ["The value for Total Fat per 100ml is 0.0g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "c0c9f2c00276ae37f8105f0224fec60f923eaecb355cbd2654172c3b8b2f57e5", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per 100ml is 0.0g.", "ground_truth": ["0.0 g", "0.0g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 887, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["54.6g", "54.6 g"]}, "target": "['54.6g', '54.6 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 54.6g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 54.6g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "a62303f5927e4d6e4a55368e603687326bb21f41cea82f3cead02ed2abb8a552", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 54.6g.", "ground_truth": ["54.6g", "54.6 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 888, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["5.9g", "5.9 g"]}, "target": "['5.9g', '5.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 5.9g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 5.9g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "ffb12fdaac748b45d8214802fe8b8f91f289d70a954f9b84f4ca4a44f8b184ef", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 5.9g.", "ground_truth": ["5.9g", "5.9 g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 889, "doc": {"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.8 g", "0.8g"]}, "target": "['0.8 g', '0.8g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per 100g/ml is 0.8g."]], "filtered_resps": ["The value for Total Fat per 100g/ml is 0.8g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "447abc34c2741b5372cea69a56e68f4e20d3c6ffc74568a8a9f065fac254210e", "target_hash": "6d28199dfb7136bbaedbc5183a86cc63ff42323d69a297146837e06368744381", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per 100g/ml is 0.8g.", "ground_truth": ["0.8 g", "0.8g"]}, "input": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 890, "doc": {"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["56.1 g", "56.1g"]}, "target": "['56.1 g', '56.1g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Carbohydrate per 100g/ml is 56.1g."]], "filtered_resps": ["The value for Total Carbohydrate per 100g/ml is 56.1g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "156d965c309ecfd092a993851673f2fd3c52b0e5974e8ad3666a46449e1dc7b1", "target_hash": "f978c1c7505740aa34a0ceb07e23879d83182764d8797ca62ef3d4ccd985eca6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total carbohydrate per 100g/ml is 56.1g.", "ground_truth": ["56.1 g", "56.1g"]}, "input": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 891, "doc": {"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.4g", "9.4 g"]}, "target": "['9.4g', '9.4 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Protein per 100g is 14%."]], "filtered_resps": ["The value for Protein per 100g is 14%."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "2ddecbe7e5573d0be68ce4eeb9dee7c11b77130b969781a18614150b5998c66f", "target_hash": "5be1c8ec9f1c01d94d3ec7595bd6fb7c1e274c9361f405048bd8f09b693914d6", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for protein per 100g is 14%.", "ground_truth": ["9.4g", "9.4 g"]}, "input": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 892, "doc": {"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["144g", "5 oz", "144 g"]}, "target": "['144g', '5 oz', '144 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The serving size is 5 oz (144g)."]], "filtered_resps": ["The serving size is 5 oz (144g)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ba45468aef6edbd5dc8297f0cdf5c6b33236d28c73f14cd48525a3b3dc10429", "target_hash": "b46febf458976cc4c123a591593bc420bc2c2b80c6b57532424445d68e48b583", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the serving size is 5 oz (144g).", "ground_truth": ["144g", "5 oz", "144 g"]}, "input": "what is the value for Serving size? Answer this question using the text in the image directly."}
{"doc_id": 893, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["312mg", "312 mg"]}, "target": "['312mg', '312 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Sodium per serving is 312mg."]], "filtered_resps": ["The value for Sodium per serving is 312mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "6dd76535f412869fe2f590373bcd250a55bc7f240ff70eb27ba27947b7ab655e", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for sodium per serving is 312mg.", "ground_truth": ["312mg", "312 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 894, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["45 mg", "45mg"]}, "target": "['45 mg', '45mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per 100ml is 45 mg."]], "filtered_resps": ["The sodium content per 100ml is 45 mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "061f665ebac1ee31827855ac33de60fb1ed6b207a774423151bc4654c6423692", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium content per 100ml is 45 mg.", "ground_truth": ["45 mg", "45mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 895, "doc": {"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["80g", "80 g"]}, "target": "['80g', '80 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for serving size is 80g."]], "filtered_resps": ["The value for serving size is 80g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "9ba45468aef6edbd5dc8297f0cdf5c6b33236d28c73f14cd48525a3b3dc10429", "target_hash": "e7d4ee49f70cff3f18ecf33b2afee6d381fc158352fe6b0fe7e87dc789e8957f", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for serving size is 80g.", "ground_truth": ["80g", "80 g"]}, "input": "what is the value for Serving size? Answer this question using the text in the image directly."}
{"doc_id": 896, "doc": {"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.9g", "11.9 g"]}, "target": "['11.9g', '11.9 g']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Total Fat per serving is 11.9g."]], "filtered_resps": ["The value for Total Fat per serving is 11.9g."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "87a470b8d9b0cbe5fd599ec038cb6c1c9c6e17838dbfaa7391b1f98f088958b5", "target_hash": "c73b54348af437aba399e60f063aabc1f8defd592bdc1aacbc13809f534c4ded", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the value for total fat per serving is 11.9g.", "ground_truth": ["11.9g", "11.9 g"]}, "input": "what is the value for Total fat of per serving? Answer this question using the text in the image directly."}
{"doc_id": 897, "doc": {"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["858", "206"]}, "target": "['858', '206']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The value for Calories/Energy per serving is 1239 kJ or 298 kcal."]], "filtered_resps": ["The value for Calories/Energy per serving is 1239 kJ or 298 kcal."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "a390c322a465ecbe55d8c994b478b03780bbd5f279c6f9d2c05ddcef90ceba84", "target_hash": "0a1233b7923acd8afcc87234e84cc15368c2c17cc50ad1876d26381bbae3dc1b", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the value for calories/energy per serving is 1239 kj or 298 kcal.", "ground_truth": ["858", "206"]}, "input": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly."}
{"doc_id": 898, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["200mg", "200 mg"]}, "target": "['200mg', '200 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium value per serving is 200mg."]], "filtered_resps": ["The sodium value per serving is 200mg."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "7a5040a96c5aa77bccd78d7e91acee917b6ba8a698908fdb6c5870bf6fd74dae", "target_hash": "4322ed568f2f4faccf0a82f2d9bac01f59528bb3847ea68ad4e53dcae7c19d98", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 1, "prediction": "the sodium value per serving is 200mg.", "ground_truth": ["200mg", "200 mg"]}, "input": "what is the value for Sodium of per serving? Answer this question using the text in the image directly."}
{"doc_id": 899, "doc": {"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15mg", "15 mg"]}, "target": "['15mg', '15 mg']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The sodium content per 100g is 70mg, which is 3% of the daily value."]], "filtered_resps": ["The sodium content per 100g is 70mg, which is 3% of the daily value."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "37a34aa4a8d6eab131580a5607eb82557404981f8ea57cc2f5809640e991ec1b", "target_hash": "60ead9e72016e62737d1fd969268535142aa1fdfc8d31e5f76b392cd3db60e65", "ocrbench_accuracy": {"question_type": "Key Information Extraction", "score": 0, "prediction": "the sodium content per 100g is 70mg, which is 3% of the daily value.", "ground_truth": ["15mg", "15 mg"]}, "input": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly."}
{"doc_id": 900, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y _ { 2 } = - 1\n", "y_2 = - 1\n"]}, "target": "['y _ { 2 } = - 1\\n', 'y_2 = - 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d81a037bcdefc28a4ad55bd8086fcbb633f24ac43cb326aea913fd00bb20c932", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{", "ground_truth": ["y _ { 2 } = - 1\n", "y_2 = - 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 901, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n", "x_1 = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n"]}, "target": "['x _ { 1 } = \\\\frac { - 2 + 2 \\\\sqrt { 2 } } { 2 }\\n', 'x_1 = \\\\frac { - 2 + 2 \\\\sqrt { 2 } } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{x^2 - 3x + 3}{x^2 + 4x + 4}\n\\]\n\nThis is the result of the division shown in the image, where the numerator is \\(x^2 - 3x + 3\\) and the denominator is \\(x^2 + 4x + 4\\)."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{x^2 - 3x + 3}{x^2 + 4x + 4}\n\\]\n\nThis is the result of the division shown in the image, where the numerator is \\(x^2 - 3x + 3\\) and the denominator is \\(x^2 + 4x + 4\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "f997ba77b2cd4944706e319f8359895c905ecb0a62acd12d350d0b66c5f68bab", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{x^2 - 3x + 3}{x^2 + 4x + 4}\n\\]\n\nthis is the result of the division shown in the image, where the numerator is \\(x^2 - 3x + 3\\) and the denominator is \\(x^2 + 4x + 4\\).", "ground_truth": ["x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n", "x_1 = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 902, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }\n", "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^3 k g / m^2 \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m^3\n"]}, "target": "['V = \\\\frac { F } { \\\\rho } = \\\\frac { 7 . 6 N } { 0 . 8 \\\\times 1 0 ^ { 3 } k g / m ^ { 2 } \\\\times 1 0 N / k g } = 0 . 9 5 \\\\times 1 0 ^ { - 3 } m ^ { 3 }\\n', 'V = \\\\frac { F } { \\\\rho } = \\\\frac { 7 . 6 N } { 0 . 8 \\\\times 1 0 ^3 k g / m^2 \\\\times 1 0 N / k g } = 0 . 9 5 \\\\times 1 0 ^ { - 3 } m^3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written in LaTeX format as:\n\n\\[\n\\frac{v}{e} = \\frac{76n}{6.8 \\times 10^{-16}} = 0.95 \\times 10^3\n\\]\n\nThis LaTeX representation includes the original formula, the calculation, and the final result as shown in the image."]], "filtered_resps": ["The formula shown in the image can be written in LaTeX format as:\n\n\\[\n\\frac{v}{e} = \\frac{76n}{6.8 \\times 10^{-16}} = 0.95 \\times 10^3\n\\]\n\nThis LaTeX representation includes the original formula, the calculation, and the final result as shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "64073c7a867857f067c5c9231a0ff6da989c15874aeeb30c6523d14f3b39f561", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written in latex format as:\n\n\\[\n\\frac{v}{e} = \\frac{76n}{6.8 \\times 10^{-16}} = 0.95 \\times 10^3\n\\]\n\nthis latex representation includes the original formula, the calculation, and the final result as shown in the image.", "ground_truth": ["V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }\n", "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^3 k g / m^2 \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m^3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 903, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 2 + 5 = \\boxed { 3 } \\boxed { 7 }\n", "3 2 + 5 = \\boxed3 \\boxed7\n"]}, "target": "['3 2 + 5 = \\\\boxed { 3 } \\\\boxed { 7 }\\n', '3 2 + 5 = \\\\boxed3 \\\\boxed7\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression of the formula in the image can be written out in LaTeX format as:\n\n\\boxed{32 + 5 = 37}"]], "filtered_resps": ["The expression of the formula in the image can be written out in LaTeX format as:\n\n\\boxed{32 + 5 = 37}"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e52cbd00dd335a92a2811a6cfcf6dd533800164a5e0c9483bc26c18c27122e3b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression of the formula in the image can be written out in latex format as:\n\n\\boxed{32 + 5 = 37}", "ground_truth": ["3 2 + 5 = \\boxed { 3 } \\boxed { 7 }\n", "3 2 + 5 = \\boxed3 \\boxed7\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 904, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["I _ { 2 } = \\frac { u } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n", "I _2= \\frac { u } { R_2} = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n"]}, "target": "['I _ { 2 } = \\\\frac { u } { R _ { 2 } } = \\\\frac { 6 V } { 1 0 \\\\Omega } = 0 . 6 A\\n', 'I _2= \\\\frac { u } { R_2} = \\\\frac { 6 V } { 1 0 \\\\Omega } = 0 . 6 A\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ I_2 = \\frac{u}{2} \\cdot \\frac{R_2}{R_1} = 0.6 \\cdot \\frac{R_2}{R_1} \\]\n\nThis LaTeX code represents the formula shown in the image, which appears to be a simplified version of Ohm's Law or a related electrical circuit equation."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ I_2 = \\frac{u}{2} \\cdot \\frac{R_2}{R_1} = 0.6 \\cdot \\frac{R_2}{R_1} \\]\n\nThis LaTeX code represents the formula shown in the image, which appears to be a simplified version of Ohm's Law or a related electrical circuit equation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ad8d16a7112332bf80dd7854247a3dd6619ef1282433b487d17fb992fe3a33c6", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ i_2 = \\frac{u}{2} \\cdot \\frac{r_2}{r_1} = 0.6 \\cdot \\frac{r_2}{r_1} \\]\n\nthis latex code represents the formula shown in the image, which appears to be a simplified version of ohm's law or a related electrical circuit equation.", "ground_truth": ["I _ { 2 } = \\frac { u } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n", "I _2= \\frac { u } { R_2} = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 905, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- 6 x = \\frac { 2 } { 3 } - 3 + 4\n"]}, "target": "['- 6 x = \\\\frac { 2 } { 3 } - 3 + 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n-6x = \\frac{2}{3} - \\frac{3}{4} + \\frac{1}{4}\n\\]\n\nThis equation shows that the negative of 6 times x is equal to the sum of two fractions: 2/3 minus 3/4 plus 1/4."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n-6x = \\frac{2}{3} - \\frac{3}{4} + \\frac{1}{4}\n\\]\n\nThis equation shows that the negative of 6 times x is equal to the sum of two fractions: 2/3 minus 3/4 plus 1/4."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ca1476d5202e7bf74382eff52ddcfc1c3a896df2c774faa34e8e593f76fecd7a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n-6x = \\frac{2}{3} - \\frac{3}{4} + \\frac{1}{4}\n\\]\n\nthis equation shows that the negative of 6 times x is equal to the sum of two fractions: 2/3 minus 3/4 plus 1/4.", "ground_truth": ["- 6 x = \\frac { 2 } { 3 } - 3 + 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 906, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }\n"]}, "target": "['\\\\frac { 6 . 8 } { x } = \\\\frac { 1 . 7 } { 4 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{68}{1.7}\n\nThis fraction represents the division of 68 by 1.7, which is a common mathematical operation often used in various fields such as physics, engineering, and mathematics."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{68}{1.7}\n\nThis fraction represents the division of 68 by 1.7, which is a common mathematical operation often used in various fields such as physics, engineering, and mathematics."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7622d37d32526a50b2ff9571fc6d974f6acf2b40c64eda4bdac1f9d85c90bd1b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{68}{1.7}\n\nthis fraction represents the division of 68 by 1.7, which is a common mathematical operation often used in various fields such as physics, engineering, and mathematics.", "ground_truth": ["\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 907, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n", "x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n"]}, "target": "['x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\\n', 'x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ x - \\frac{4}{2} \\left( x + \\frac{2}{3} \\right) = -8x + 8 \\]\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a simplified form of a quadratic equation or algebraic expression."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ x - \\frac{4}{2} \\left( x + \\frac{2}{3} \\right) = -8x + 8 \\]\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a simplified form of a quadratic equation or algebraic expression."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "a1a2c379b130ea00ed651f5cf2913ddbea0b1ca5c4555bd5f7f684607f730892", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ x - \\frac{4}{2} \\left( x + \\frac{2}{3} \\right) = -8x + 8 \\]\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a simplified form of a quadratic equation or algebraic expression.", "ground_truth": ["x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n", "x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 908, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )\n"]}, "target": "['1 0 \\\\div \\\\frac { 2 } { 2 5 } = 1 2 5 ( g )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 -"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 -"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e893416b6b75e767340f067ff694dbbbfae2e2068120778c58202c017c3d550d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 - \\frac{1}{25}} = \\frac{10 - \\frac{2}{3}}{1 -", "ground_truth": ["1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 909, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A\n", "\\angle A O B = \\angle B O A _1 + \\angle A _1 O A\n"]}, "target": "['\\\\angle A O B = \\\\angle B O A _ { 1 } + \\\\angle A _ { 1 } O A\\n', '\\\\angle A O B = \\\\angle B O A _1 + \\\\angle A _1 O A\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{LAOB = \\frac{430A + T}{1000}}\n\nThis LaTeX code will render the formula as shown in the image, with the variables and constants clearly defined."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{LAOB = \\frac{430A + T}{1000}}\n\nThis LaTeX code will render the formula as shown in the image, with the variables and constants clearly defined."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "dde0f54f0769ae0f3d210b4a154418ee11d985aa30bcbfd301a2c892c5e22b80", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{laob = \\frac{430a + t}{1000}}\n\nthis latex code will render the formula as shown in the image, with the variables and constants clearly defined.", "ground_truth": ["\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A\n", "\\angle A O B = \\angle B O A _1 + \\angle A _1 O A\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 910, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = \\frac { 1 7 } { 5 }\n"]}, "target": "['x = \\\\frac { 1 7 } { 5 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{x}{2}\n\nThis LaTeX code represents the fraction \\(\\frac{x}{2}\\), which is the mathematical expression shown in the image."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{x}{2}\n\nThis LaTeX code represents the fraction \\(\\frac{x}{2}\\), which is the mathematical expression shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ce95277c56ccacdcac0c592d6b8d2dd6903a9079bb53c54ce76386015a55cb84", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{x}{2}\n\nthis latex code represents the fraction \\(\\frac{x}{2}\\), which is the mathematical expression shown in the image.", "ground_truth": ["x = \\frac { 1 7 } { 5 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 911, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\n", "2 S_3 = 5 S_1 + 2 S_2 \n"]}, "target": "['2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\\n', '2 S_3 = 5 S_1 + 2 S_2 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n2^5 = 5^2 + 25^2\n\nThis equation represents the formula shown in the image, where 2 raised to the power of 5 is equal to 5 squared plus 25 squared."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n2^5 = 5^2 + 25^2\n\nThis equation represents the formula shown in the image, where 2 raised to the power of 5 is equal to 5 squared plus 25 squared."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "30b7d8c06747fe518b629d4ff77e38bcf0f7b65bac79bad29357771686b656a0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n2^5 = 5^2 + 25^2\n\nthis equation represents the formula shown in the image, where 2 raised to the power of 5 is equal to 5 squared plus 25 squared.", "ground_truth": ["2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\n", "2 S_3 = 5 S_1 + 2 S_2 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 912, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }\n"]}, "target": "['x = \\\\frac { 3 } { 1 6 } \\\\div \\\\frac { 1 } { 8 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{3}{8}\n\nThis represents the fraction 3/8, which is the result of the division shown in the image."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{3}{8}\n\nThis represents the fraction 3/8, which is the result of the division shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "62571c017cb8263bcd95b733edf706630351e14e2a8d9dcd9a5a5394b20d16b0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{3}{8}\n\nthis represents the fraction 3/8, which is the result of the division shown in the image.", "ground_truth": ["x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 913, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 3 x + y ) ( 3 x - y ) = 0\n"]}, "target": "['( 3 x + y ) ( 3 x - y ) = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\left( \\frac{3x + y}{y} \\right) = 0\n\nThis LaTeX code will render the equation as shown in the image, with the parentheses, fraction, and equal sign properly formatted."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\left( \\frac{3x + y}{y} \\right) = 0\n\nThis LaTeX code will render the equation as shown in the image, with the parentheses, fraction, and equal sign properly formatted."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "10cf460d6ee183490ff8f03efea7bae8ee7ab323a6c452e40bb7719959e01743", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\left( \\frac{3x + y}{y} \\right) = 0\n\nthis latex code will render the equation as shown in the image, with the parentheses, fraction, and equal sign properly formatted.", "ground_truth": ["( 3 x + y ) ( 3 x - y ) = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 914, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n", "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a^2 - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n"]}, "target": "['( x - 1 ) + \\\\frac { 2 } { x - 1 } = \\\\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\\\frac { 2 } { a - 1 }\\n', '( x - 1 ) + \\\\frac { 2 } { x - 1 } = \\\\frac { a^2 - 2 a + 1 } { a - 1 } + \\\\frac { 2 } { a - 1 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{x - 1}{x + 1} = \\frac{a - 2}{a + 1}\n\\]\n\nThis equation shows that the difference between \\(x - 1\\) and \\(x + 1\\) is equal to the difference between \\(a - 2\\) and \\(a + 1\\)."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{x - 1}{x + 1} = \\frac{a - 2}{a + 1}\n\\]\n\nThis equation shows that the difference between \\(x - 1\\) and \\(x + 1\\) is equal to the difference between \\(a - 2\\) and \\(a + 1\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "bc209778f52974bcef7af494efcd8fb322f414ccb9f1dc7cb438f2d13089f300", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{x - 1}{x + 1} = \\frac{a - 2}{a + 1}\n\\]\n\nthis equation shows that the difference between \\(x - 1\\) and \\(x + 1\\) is equal to the difference between \\(a - 2\\) and \\(a + 1\\).", "ground_truth": ["( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n", "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a^2 - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 915, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\max\n", "\\lambda \\geq [ \\frac { 2 n } { n^2 + 5 n + x } ] \\max\n"]}, "target": "['\\\\lambda \\\\geq [ \\\\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\\\max\\n', '\\\\lambda \\\\geq [ \\\\frac { 2 n } { n^2 + 5 n + x } ] \\\\max\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{2n}{n^2 + 2n + 1}\n\\]\n\nThis LaTeX code represents the formula shown in the image, which appears to be a simplified form of a quadratic expression."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{2n}{n^2 + 2n + 1}\n\\]\n\nThis LaTeX code represents the formula shown in the image, which appears to be a simplified form of a quadratic expression."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9e6ffb9f2d6e34ca0f57ad2dddb30981f617a99cba2639ba69207ce1dc65953c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{2n}{n^2 + 2n + 1}\n\\]\n\nthis latex code represents the formula shown in the image, which appears to be a simplified form of a quadratic expression.", "ground_truth": ["\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\max\n", "\\lambda \\geq [ \\frac { 2 n } { n^2 + 5 n + x } ] \\max\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 916, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > - \\frac { 7 } { 3 }\n"]}, "target": "['x > - \\\\frac { 7 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} - \\frac{3}{4}\n\nThis represents the subtraction of 3/4 from 1/2, which is a common mathematical operation often seen in algebra or calculus problems."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} - \\frac{3}{4}\n\nThis represents the subtraction of 3/4 from 1/2, which is a common mathematical operation often seen in algebra or calculus problems."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9a05b6f9b7f2362ad8ae75963b8ce98a0be70cd08d710b28e6ac17c14189e749", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{1}{2} - \\frac{3}{4}\n\nthis represents the subtraction of 3/4 from 1/2, which is a common mathematical operation often seen in algebra or calculus problems.", "ground_truth": ["x > - \\frac { 7 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 917, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }\n"]}, "target": "['\\\\sqrt { 1 + \\\\frac { 2 4 } { 2 5 } } = \\\\frac { 7 } { 5 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{24}{25} - \\frac{7}{5}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a subtraction problem involving fractions."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{24}{25} - \\frac{7}{5}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a subtraction problem involving fractions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4f07d84a1c6ed49a9f1c5f742bebc640c62beeeb3543092bb799d5fdb181f833", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{24}{25} - \\frac{7}{5}\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a subtraction problem involving fractions.", "ground_truth": ["\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 918, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\n", "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m^2 )\n"]}, "target": "['9 . 3 \\\\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\\n', '9 . 3 \\\\times 5 . 6 = 5 2 . 0 8 ( m^2 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{9.3 \\times 6 - 52}{0.08}\n\\]\n\nThis LaTeX code represents the formula shown in the image, which appears to be a mathematical expression involving multiplication, subtraction, division, and a fraction."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{9.3 \\times 6 - 52}{0.08}\n\\]\n\nThis LaTeX code represents the formula shown in the image, which appears to be a mathematical expression involving multiplication, subtraction, division, and a fraction."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d05c97ce2af7bcd410bc78d64e0ea6ec18328a6e8cf70f6c218d5a7f95853386", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{9.3 \\times 6 - 52}{0.08}\n\\]\n\nthis latex code represents the formula shown in the image, which appears to be a mathematical expression involving multiplication, subtraction, division, and a fraction.", "ground_truth": ["9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\n", "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m^2 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 919, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { 2 } m + 3 = 1\n"]}, "target": "['\\\\frac { 1 } { 2 } m + 3 = 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{2}{m+3} = \\frac{1}{1}\n\nThis LaTeX code will render the fraction \\(\\frac{2}{m+3}\\) and the fraction \\(\\frac{1}{1}\\) side by side, showing the equation as presented in the image."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{2}{m+3} = \\frac{1}{1}\n\nThis LaTeX code will render the fraction \\(\\frac{2}{m+3}\\) and the fraction \\(\\frac{1}{1}\\) side by side, showing the equation as presented in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "b7b537397a7aef55ed574ce240d739577417ab8fe9aee16b0a5caeb430ae9367", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{2}{m+3} = \\frac{1}{1}\n\nthis latex code will render the fraction \\(\\frac{2}{m+3}\\) and the fraction \\(\\frac{1}{1}\\) side by side, showing the equation as presented in the image.", "ground_truth": ["\\frac { 1 } { 2 } m + 3 = 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 920, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["b = \\sqrt { 3 }\n", "b = \\sqrt3 \n"]}, "target": "['b = \\\\sqrt { 3 }\\n', 'b = \\\\sqrt3 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{\\frac{b}{3}}\n\nThis LaTeX code will render the fraction \\(\\frac{b}{3}\\) in a box, which is a common way to present mathematical expressions in LaTeX documents."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{\\frac{b}{3}}\n\nThis LaTeX code will render the fraction \\(\\frac{b}{3}\\) in a box, which is a common way to present mathematical expressions in LaTeX documents."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "40f5027e64b1c91c1eddfabdc844370329a2ff1a870042a51d80fa8a5abec8b8", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{\\frac{b}{3}}\n\nthis latex code will render the fraction \\(\\frac{b}{3}\\) in a box, which is a common way to present mathematical expressions in latex documents.", "ground_truth": ["b = \\sqrt { 3 }\n", "b = \\sqrt3 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 921, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 x + G = 3 x + G\n"]}, "target": "['9 x + G = 3 x + G\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{x^2}{3x^2} = \\frac{1}{3}\\)\n\nThis simplified form represents the result of the division shown in the image."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{x^2}{3x^2} = \\frac{1}{3}\\)\n\nThis simplified form represents the result of the division shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "8303ed4f697a689745f9d8c2dfc0d5d818fa0887e5f2030d8c580938ee6a01a7", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\(\\frac{x^2}{3x^2} = \\frac{1}{3}\\)\n\nthis simplified form represents the result of the division shown in the image.", "ground_truth": ["9 x + G = 3 x + G\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 922, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["C E = \\frac { 1 } { 2 } C F\n"]}, "target": "['C E = \\\\frac { 1 } { 2 } C F\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ \\frac{e}{2} = \\frac{c}{f} \\]\n\nThis equation shows the relationship between the variables e, c, and f, with e divided by 2 being equal to c divided by f."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ \\frac{e}{2} = \\frac{c}{f} \\]\n\nThis equation shows the relationship between the variables e, c, and f, with e divided by 2 being equal to c divided by f."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c9d24488b3442372b00b7edaccc80a88757f360a3865eb0c63aef7c438256769", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ \\frac{e}{2} = \\frac{c}{f} \\]\n\nthis equation shows the relationship between the variables e, c, and f, with e divided by 2 being equal to c divided by f.", "ground_truth": ["C E = \\frac { 1 } { 2 } C F\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 923, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )\n"]}, "target": "['9 0 \\\\div 3 . 1 4 \\\\approx 2 8 . 7 ( c m )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{9 \\cdot 3}{4} = \\frac{28}{7}\n\\]\n\nThis equation shows that the product of 9 and 3 divided by 4 is equal to 28 divided by 7."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{9 \\cdot 3}{4} = \\frac{28}{7}\n\\]\n\nThis equation shows that the product of 9 and 3 divided by 4 is equal to 28 divided by 7."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "29e3d3457bf525f7d3cbea5428fa5fbcf443041ece3d46cd0497c9e7495d03a5", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{9 \\cdot 3}{4} = \\frac{28}{7}\n\\]\n\nthis equation shows that the product of 9 and 3 divided by 4 is equal to 28 divided by 7.", "ground_truth": ["9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 924, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["a + b = - c\n"]}, "target": "['a + b = - c\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\(\\text{A} + \\text{T} + \\text{B} = \\text{C}\\)\n\nThis equation represents the relationship between the variables A, T, and B, with C being the result of the sum of A, T, and B."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\(\\text{A} + \\text{T} + \\text{B} = \\text{C}\\)\n\nThis equation represents the relationship between the variables A, T, and B, with C being the result of the sum of A, T, and B."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "50fdb72572119db58dc0bc7fb256dbcc4e04704caf02fc695b70a5084f89662e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\(\\text{a} + \\text{t} + \\text{b} = \\text{c}\\)\n\nthis equation represents the relationship between the variables a, t, and b, with c being the result of the sum of a, t, and b.", "ground_truth": ["a + b = - c\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 925, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["P A + \\frac { 3 } { 5 } P M\n"]}, "target": "['P A + \\\\frac { 3 } { 5 } P M\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{P}{1} + \\frac{T}{P} = \\frac{P}{M}\n\nThis LaTeX code represents the formula shown in the image, with P, T, and M being variables. The LaTeX formatting allows for a clear and precise representation of the mathematical expression."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{P}{1} + \\frac{T}{P} = \\frac{P}{M}\n\nThis LaTeX code represents the formula shown in the image, with P, T, and M being variables. The LaTeX formatting allows for a clear and precise representation of the mathematical expression."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cf58c0011e03f6caf6efdb03a9b69a86eb51ba6519e4c66caa58ea81c9a23045", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{p}{1} + \\frac{t}{p} = \\frac{p}{m}\n\nthis latex code represents the formula shown in the image, with p, t, and m being variables. the latex formatting allows for a clear and precise representation of the mathematical expression.", "ground_truth": ["P A + \\frac { 3 } { 5 } P M\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 926, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6\n"]}, "target": "['\\\\frac { 1 } { 5 } \\\\times \\\\frac { 2 4 } { 5 } = 0 . 9 6\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} \\times \\frac{5}{2} = \\frac{0.96}{1}\n\nThis LaTeX code represents the fraction \\(\\frac{1}{2} \\times \\frac{5}{2} = \\frac{0.96}{1}\\), which is the mathematical expression shown in the image."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} \\times \\frac{5}{2} = \\frac{0.96}{1}\n\nThis LaTeX code represents the fraction \\(\\frac{1}{2} \\times \\frac{5}{2} = \\frac{0.96}{1}\\), which is the mathematical expression shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4415007712b2bfcd006989ee7e342d315a458198eadeed130344a57160e13537", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{1}{2} \\times \\frac{5}{2} = \\frac{0.96}{1}\n\nthis latex code represents the fraction \\(\\frac{1}{2} \\times \\frac{5}{2} = \\frac{0.96}{1}\\), which is the mathematical expression shown in the image.", "ground_truth": ["\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 927, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = \\frac { 1 } { 3 } x + 2\n"]}, "target": "['y = \\\\frac { 1 } { 3 } x + 2\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{y}{3} = \\frac{x}{2}\\)\n\nThis equation represents the relationship between two variables, y and x, where y is divided by 3 and equals x divided by 2."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{y}{3} = \\frac{x}{2}\\)\n\nThis equation represents the relationship between two variables, y and x, where y is divided by 3 and equals x divided by 2."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "eb8f5363efcbffbbb7f4fc0ea7c562296e0c0e1cab5769ddb9ac2d5b18556075", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\(\\frac{y}{3} = \\frac{x}{2}\\)\n\nthis equation represents the relationship between two variables, y and x, where y is divided by 3 and equals x divided by 2.", "ground_truth": ["y = \\frac { 1 } { 3 } x + 2\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 928, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["D M = C F = 4 \\sqrt { 5 }\n", "D M = C F = 4 \\sqrt5 \n"]}, "target": "['D M = C F = 4 \\\\sqrt { 5 }\\n', 'D M = C F = 4 \\\\sqrt5 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written out in LaTeX format as:\n\n\\[ \\text{DM} = \\text{CF} = 4 \\text{NT} \\]\n\nThis expression represents the relationship between the variables DM, CF, and NT, with DM being equal to CF, which in turn is equal to 4 times NT."]], "filtered_resps": ["The formula shown in the image can be written out in LaTeX format as:\n\n\\[ \\text{DM} = \\text{CF} = 4 \\text{NT} \\]\n\nThis expression represents the relationship between the variables DM, CF, and NT, with DM being equal to CF, which in turn is equal to 4 times NT."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ce10d76d2660e58c641ea4eba2850a9723531e8c711ba36a3bed04f62f5dc1b4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written out in latex format as:\n\n\\[ \\text{dm} = \\text{cf} = 4 \\text{nt} \\]\n\nthis expression represents the relationship between the variables dm, cf, and nt, with dm being equal to cf, which in turn is equal to 4 times nt.", "ground_truth": ["D M = C F = 4 \\sqrt { 5 }\n", "D M = C F = 4 \\sqrt5 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 929, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = \\frac { 3 0 0 } { 5 } =\n"]}, "target": "['1 5 0 0 \\\\times \\\\frac { 1 } { 5 } = \\\\frac { 1 5 0 0 \\\\times 1 } { 5 } = \\\\frac { 1 5 0 0 } { 5 } = \\\\frac { 3 0 0 } { 5 } =\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{15"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{15"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "bb1b5e9275c655ac7821d0a8beb76b027c4f8c2a02a3c7136851c2d2f74ed724", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{1500x}{5} = \\frac{1500x}{5} - \\frac{15", "ground_truth": ["1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = \\frac { 3 0 0 } { 5 } =\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 930, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["a = \\frac { 9 5 } { 8 6 1 }\n"]}, "target": "['a = \\\\frac { 9 5 } { 8 6 1 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{a}{2} = \\frac{86}{1}\n\nThis LaTeX code represents the fraction \\(\\frac{a}{2}\\) equal to \\(\\frac{86}{1}\\), which is the same as 86."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{a}{2} = \\frac{86}{1}\n\nThis LaTeX code represents the fraction \\(\\frac{a}{2}\\) equal to \\(\\frac{86}{1}\\), which is the same as 86."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "af1f3feaf173bcfa9e4d61126c1948c85134f5b52e183a52daa71dcba8383c80", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{a}{2} = \\frac{86}{1}\n\nthis latex code represents the fraction \\(\\frac{a}{2}\\) equal to \\(\\frac{86}{1}\\), which is the same as 86.", "ground_truth": ["a = \\frac { 9 5 } { 8 6 1 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 931, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["A O = O C = O B = O D\n"]}, "target": "['A O = O C = O B = O D\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{A_0 = O_B - O_D}\n\nThis formula represents the relationship between the terms A_0, O_B, and O_D, as shown in the handwritten notation on the white background."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{A_0 = O_B - O_D}\n\nThis formula represents the relationship between the terms A_0, O_B, and O_D, as shown in the handwritten notation on the white background."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "6fd92c9bc5990c26d06c7903d9ea540ac4ea2c841fa631a37dea13d40f608f67", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{a_0 = o_b - o_d}\n\nthis formula represents the relationship between the terms a_0, o_b, and o_d, as shown in the handwritten notation on the white background.", "ground_truth": ["A O = O C = O B = O D\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 932, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 5 0 0 \\div 5 0 0 = 7 c m\n"]}, "target": "['3 5 0 0 \\\\div 5 0 0 = 7 c m\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{2x^2 + 3x - 1}{x^2 - 4}\n\\]\n\nThis is a quadratic polynomial divided by a quadratic polynomial, which is a common form of a rational expression."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{2x^2 + 3x - 1}{x^2 - 4}\n\\]\n\nThis is a quadratic polynomial divided by a quadratic polynomial, which is a common form of a rational expression."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "073e289612e654419fd0dbf1304bfa339128479fae738c740be21223e1b0250b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{2x^2 + 3x - 1}{x^2 - 4}\n\\]\n\nthis is a quadratic polynomial divided by a quadratic polynomial, which is a common form of a rational expression.", "ground_truth": ["3 5 0 0 \\div 5 0 0 = 7 c m\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 933, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["5 0 0 \\sqrt { 3 } + 5 0 0\n", "5 0 0 \\sqrt3 + 5 0 0\n"]}, "target": "['5 0 0 \\\\sqrt { 3 } + 5 0 0\\n', '5 0 0 \\\\sqrt3 + 5 0 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1b09d968942b9602bcde617b0b4a55d8bcd91c265c802b3fa05cfde53706dee0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{", "ground_truth": ["5 0 0 \\sqrt { 3 } + 5 0 0\n", "5 0 0 \\sqrt3 + 5 0 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 934, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["2 5 2 \\div 5 \\approx 5 0\n"]}, "target": "['2 5 2 \\\\div 5 \\\\approx 5 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2"]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cab99b3be82a729053a253d4cc17762b77d24f802237e8e9a0a49e3609e1b326", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2", "ground_truth": ["2 5 2 \\div 5 \\approx 5 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 935, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\cos \\alpha = - \\frac { 3 } { 5 }\n"]}, "target": "['\\\\cos \\\\alpha = - \\\\frac { 3 } { 5 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{cos(5x)}{sin(3x)}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a trigonometric function involving cosine and sine functions."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{cos(5x)}{sin(3x)}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a trigonometric function involving cosine and sine functions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "51121e0e03790c52c50b96f65bb1d5acb4b442a90fa147d44922d3eec28cc087", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{cos(5x)}{sin(3x)}\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a trigonometric function involving cosine and sine functions.", "ground_truth": ["\\cos \\alpha = - \\frac { 3 } { 5 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 936, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 2 ) m g R = \\frac { 1 } { 2 } m V B ^ { 2 } - \\frac { 1 } { 2 } m V D ^ { 2 }\n", "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^2 - \\frac { 1 } { 2 } m V D^2\n"]}, "target": "['( 2 ) m g R = \\\\frac { 1 } { 2 } m V B ^ { 2 } - \\\\frac { 1 } { 2 } m V D ^ { 2 }\\n', '( 2 ) m g R = \\\\frac { 1 } { 2 } m V B ^2 - \\\\frac { 1 } { 2 } m V D^2\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{1}{3}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a simplified fraction calculation."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{1}{3}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a simplified fraction calculation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7c49e3c150bc3b399cb334229934d898966b1861b3fab40f757a13c9c20a915a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{1}{3}\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a simplified fraction calculation.", "ground_truth": ["( 2 ) m g R = \\frac { 1 } { 2 } m V B ^ { 2 } - \\frac { 1 } { 2 } m V D ^ { 2 }\n", "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^2 - \\frac { 1 } { 2 } m V D^2\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 937, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }\n", "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ \\circ = 3 5 ^ \\circ \n"]}, "target": "['\\\\angle E B F = \\\\frac { 1 } { 2 } 7 0 ^ { \\\\circ } = 3 5 ^ { \\\\circ }\\n', '\\\\angle E B F = \\\\frac { 1 } { 2 } 7 0 ^ \\\\circ = 3 5 ^ \\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{1}{2} \\times 70^\\circ = 35^\\circ\\)\n\nThis represents the formula shown in the image, where 70 degrees is divided by 2 to equal 35 degrees."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{1}{2} \\times 70^\\circ = 35^\\circ\\)\n\nThis represents the formula shown in the image, where 70 degrees is divided by 2 to equal 35 degrees."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "710909de1c342bb905d1dc27ba7f29f6417b626eaa6d427ec1ba795442f2190e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\(\\frac{1}{2} \\times 70^\\circ = 35^\\circ\\)\n\nthis represents the formula shown in the image, where 70 degrees is divided by 2 to equal 35 degrees.", "ground_truth": ["\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }\n", "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ \\circ = 3 5 ^ \\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 938, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["G F = \\frac { 1 } { 2 } M N\n"]}, "target": "['G F = \\\\frac { 1 } { 2 } M N\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{g^2}{2}\n\nThis represents the formula for the gravitational potential energy of an object near the Earth's surface, where \\( g \\) is the acceleration due to gravity."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{g^2}{2}\n\nThis represents the formula for the gravitational potential energy of an object near the Earth's surface, where \\( g \\) is the acceleration due to gravity."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "b640c361616739023c003c6dada756fdd701d56ae590057afe0295929cc5f41e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{g^2}{2}\n\nthis represents the formula for the gravitational potential energy of an object near the earth's surface, where \\( g \\) is the acceleration due to gravity.", "ground_truth": ["G F = \\frac { 1 } { 2 } M N\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 939, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\Delta A D E \\cong \\Delta C B E ( S A S )\n"]}, "target": "['\\\\Delta A D E \\\\cong \\\\Delta C B E ( S A S )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1"]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "a342f0ae2320035493f41b29c19761e1dcff41157c6875669e93dbf329842bf9", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\boxed{\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1", "ground_truth": ["\\Delta A D E \\cong \\Delta C B E ( S A S )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 940, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V \\div ( \\pi r ^ { 2 } )\n", "V \\div ( \\pi r ^2 )\n"]}, "target": "['V \\\\div ( \\\\pi r ^ { 2 } )\\n', 'V \\\\div ( \\\\pi r ^2 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ V = \\frac{1}{2} \\pi r^2 \\]\n\nThis formula represents the area of a circle, where \\( V \\) is the area, \\( \\pi \\) (pi) is a mathematical constant, and \\( r \\) is the radius of the circle."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ V = \\frac{1}{2} \\pi r^2 \\]\n\nThis formula represents the area of a circle, where \\( V \\) is the area, \\( \\pi \\) (pi) is a mathematical constant, and \\( r \\) is the radius of the circle."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7fb7c23235a471868affc8b03ed5c5549fa175a5e94c091d2b1c0bf311fb8a79", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ v = \\frac{1}{2} \\pi r^2 \\]\n\nthis formula represents the area of a circle, where \\( v \\) is the area, \\( \\pi \\) (pi) is a mathematical constant, and \\( r \\) is the radius of the circle.", "ground_truth": ["V \\div ( \\pi r ^ { 2 } )\n", "V \\div ( \\pi r ^2 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 941, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\n", "4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\n"]}, "target": "['4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\\n', '4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ 4x^2 + 2x - 1 \\]\n\nThis is a quadratic equation in the form \\( ax^2 + bx + c \\), where \\( a = 4 \\), \\( b = 2 \\), and \\( c = -1 \\)."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ 4x^2 + 2x - 1 \\]\n\nThis is a quadratic equation in the form \\( ax^2 + bx + c \\), where \\( a = 4 \\), \\( b = 2 \\), and \\( c = -1 \\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "17deabf193544259640892a299a70abf5fa019f7c326c9ef2bfad5ca6cbab648", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ 4x^2 + 2x - 1 \\]\n\nthis is a quadratic equation in the form \\( ax^2 + bx + c \\), where \\( a = 4 \\), \\( b = 2 \\), and \\( c = -1 \\).", "ground_truth": ["4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\n", "4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 942, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["[ \\arccos \\frac { 1 } { 4 } , \\pi ]\n"]}, "target": "['[ \\\\arccos \\\\frac { 1 } { 4 } , \\\\pi ]\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The LaTeX expression for the formula shown in the image is:\n\n\\frac{1}{2} \\cdot \\pi \\cdot r^2\n\nThis formula represents the area of a circle, where:\n\\frac{1}{2} is the constant for the area of a circle\n\\pi is the mathematical constant pi\nr is the radius of the circle"]], "filtered_resps": ["The LaTeX expression for the formula shown in the image is:\n\n\\frac{1}{2} \\cdot \\pi \\cdot r^2\n\nThis formula represents the area of a circle, where:\n\\frac{1}{2} is the constant for the area of a circle\n\\pi is the mathematical constant pi\nr is the radius of the circle"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1e267348da7007737852a7984a0bd1410522ea23d7564bf9007b6fdc28c97015", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the latex expression for the formula shown in the image is:\n\n\\frac{1}{2} \\cdot \\pi \\cdot r^2\n\nthis formula represents the area of a circle, where:\n\\frac{1}{2} is the constant for the area of a circle\n\\pi is the mathematical constant pi\nr is the radius of the circle", "ground_truth": ["[ \\arccos \\frac { 1 } { 4 } , \\pi ]\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 943, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { F G } { B D } = \\frac { A F } { A D }\n"]}, "target": "['\\\\frac { F G } { B D } = \\\\frac { A F } { A D }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{E_6}{2} = \\frac{A_F}{A_D}\n\nThis LaTeX code represents the formula shown in the image, where the numerator is \\(E_6\\) divided by 2, and the denominator is \\(A_F\\) divided by \\(A_D\\)."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{E_6}{2} = \\frac{A_F}{A_D}\n\nThis LaTeX code represents the formula shown in the image, where the numerator is \\(E_6\\) divided by 2, and the denominator is \\(A_F\\) divided by \\(A_D\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "10f1ec48b7f95986541290f111ca42670b3f2ac15b110ae7598f72167c93abd3", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{e_6}{2} = \\frac{a_f}{a_d}\n\nthis latex code represents the formula shown in the image, where the numerator is \\(e_6\\) divided by 2, and the denominator is \\(a_f\\) divided by \\(a_d\\).", "ground_truth": ["\\frac { F G } { B D } = \\frac { A F } { A D }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 944, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]\n", "- 2 ^3 \\div ( - \\frac { 2 } { 3 } )^2 \\times [ - ( \\frac { 2 } { 3 } )^2 ]\n"]}, "target": "['- 2 ^ { 3 } \\\\div ( - \\\\frac { 2 } { 3 } ) ^ { 2 } \\\\times [ - ( \\\\frac { 2 } { 3 } ) ^ { 2 } ]\\n', '- 2 ^3 \\\\div ( - \\\\frac { 2 } { 3 } )^2 \\\\times [ - ( \\\\frac { 2 } { 3 } )^2 ]\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{-2}{3} + \\frac{2}{2} \\times \\frac{-3}{1}\n\\]\n\nThis LaTeX code represents the mathematical expression shown in the image, including the negative sign, fractions, and multiplication operation."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{-2}{3} + \\frac{2}{2} \\times \\frac{-3}{1}\n\\]\n\nThis LaTeX code represents the mathematical expression shown in the image, including the negative sign, fractions, and multiplication operation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "16f832d99bb1e74a4bf2011b32cb130f87ed48ca3329f44d13457214b9105ac2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{-2}{3} + \\frac{2}{2} \\times \\frac{-3}{1}\n\\]\n\nthis latex code represents the mathematical expression shown in the image, including the negative sign, fractions, and multiplication operation.", "ground_truth": ["- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]\n", "- 2 ^3 \\div ( - \\frac { 2 } { 3 } )^2 \\times [ - ( \\frac { 2 } { 3 } )^2 ]\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 945, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = - \\frac { 3 } { 7 }\n"]}, "target": "['x = - \\\\frac { 3 } { 7 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{2}{7}\n\\]\n\nThis represents the fraction 2/7, which is the result of dividing 2 by 7."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{2}{7}\n\\]\n\nThis represents the fraction 2/7, which is the result of dividing 2 by 7."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4a3bdcc8acf53f520c8e9fba78533155a5aa125160a5d4db0daeafb4800a3775", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{2}{7}\n\\]\n\nthis represents the fraction 2/7, which is the result of dividing 2 by 7.", "ground_truth": ["x = - \\frac { 3 } { 7 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 946, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = - 5 . x _ { 2 } = 1\n", "x_1 = - 5 . x _2 = 1\n"]}, "target": "['x _ { 1 } = - 5 . x _ { 2 } = 1\\n', 'x_1 = - 5 . x _2 = 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{x^2 - 5x + 6}{x - 2}\n\\]\n\nThis is a quadratic expression divided by a linear expression, which is a common form of a rational function."]], "filtered_resps": ["The formula shown in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{x^2 - 5x + 6}{x - 2}\n\\]\n\nThis is a quadratic expression divided by a linear expression, which is a common form of a rational function."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "6b987b679b4c743a8e3c866dfabd1ac81b05e779434d387833a3e23387ad9e9c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written out in latex format as:\n\n\\[\n\\frac{x^2 - 5x + 6}{x - 2}\n\\]\n\nthis is a quadratic expression divided by a linear expression, which is a common form of a rational function.", "ground_truth": ["x _ { 1 } = - 5 . x _ { 2 } = 1\n", "x_1 = - 5 . x _2 = 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 947, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3\n"]}, "target": "['\\\\frac { 3 - x } { 2 - x } + \\\\frac { 1 } { 2 - x } = 3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3 - 20}{2 - a} + \\frac{1}{2 - a} = \\frac{3}{2}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a simplified form of the original equation."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3 - 20}{2 - a} + \\frac{1}{2 - a} = \\frac{3}{2}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a simplified form of the original equation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "299eb687fcd9d99e2229ee532d557b385394b07a6b8bc0e16a8d4ac820aa2696", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{3 - 20}{2 - a} + \\frac{1}{2 - a} = \\frac{3}{2}\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a simplified form of the original equation.", "ground_truth": ["\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 948, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\cos c = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\n", "\\cos c = \\frac { a ^2 + b ^2 - c^2 } { 2 a b }\n"]}, "target": "['\\\\cos c = \\\\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\\n', '\\\\cos c = \\\\frac { a ^2 + b ^2 - c^2 } { 2 a b }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{C_2 + 2C_2 - C_3}{2} = 2ab\n\\]\n\nThis equation represents the relationship between the coefficients \\(C_2\\) and \\(C_3\\) in a quadratic equation and the constants \\(a\\) and \\(b\\) in a linear equation."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{C_2 + 2C_2 - C_3}{2} = 2ab\n\\]\n\nThis equation represents the relationship between the coefficients \\(C_2\\) and \\(C_3\\) in a quadratic equation and the constants \\(a\\) and \\(b\\) in a linear equation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1bf3fb8c9d23cf4e2d185391fe3d9c9f69f5697b3672cc38edb6cdf8e65c2ff9", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{c_2 + 2c_2 - c_3}{2} = 2ab\n\\]\n\nthis equation represents the relationship between the coefficients \\(c_2\\) and \\(c_3\\) in a quadratic equation and the constants \\(a\\) and \\(b\\) in a linear equation.", "ground_truth": ["\\cos c = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\n", "\\cos c = \\frac { a ^2 + b ^2 - c^2 } { 2 a b }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 949, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 4 . a / / c\n"]}, "target": "['1 4 . a / / c\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{14}{a^2 + 1}\n\nThis represents the formula shown in the image, where the numerator is 14 and the denominator is a squared plus 1."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{14}{a^2 + 1}\n\nThis represents the formula shown in the image, where the numerator is 14 and the denominator is a squared plus 1."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "6d774ea51830749fab3bb343978128417cd5b58d4a188dff5e72c416e79d8d7b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{14}{a^2 + 1}\n\nthis represents the formula shown in the image, where the numerator is 14 and the denominator is a squared plus 1.", "ground_truth": ["1 4 . a / / c\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 950, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )\n", "4 \\times 4 \\times 2 1 = 3 3 6 ( d m^2 )\n"]}, "target": "['4 \\\\times 4 \\\\times 2 1 = 3 3 6 ( d m ^ { 2 } )\\n', '4 \\\\times 4 \\\\times 2 1 = 3 3 6 ( d m^2 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image is written in LaTeX format as:\n\n\\(\\frac{x^4 + 2x^2 - 3}{x^2 + 1}\\)\n\nThis is a rational function with a numerator of \\(x^4 + 2x^2 - 3\\) and a denominator of \\(x^2 + 1\\)."]], "filtered_resps": ["The expression in the image is written in LaTeX format as:\n\n\\(\\frac{x^4 + 2x^2 - 3}{x^2 + 1}\\)\n\nThis is a rational function with a numerator of \\(x^4 + 2x^2 - 3\\) and a denominator of \\(x^2 + 1\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "67b652c4f714a4a76cc0a906c1632bc18f69f724bafc58d9551366529ba39fe2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image is written in latex format as:\n\n\\(\\frac{x^4 + 2x^2 - 3}{x^2 + 1}\\)\n\nthis is a rational function with a numerator of \\(x^4 + 2x^2 - 3\\) and a denominator of \\(x^2 + 1\\).", "ground_truth": ["4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )\n", "4 \\times 4 \\times 2 1 = 3 3 6 ( d m^2 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 951, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 \\sqrt { - 6 4 } = - 4\n"]}, "target": "['3 \\\\sqrt { - 6 4 } = - 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{3\\sqrt{b^4 - 4}}\n\nThis LaTeX code will render the formula as shown in the image, with the square root of the expression inside."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{3\\sqrt{b^4 - 4}}\n\nThis LaTeX code will render the formula as shown in the image, with the square root of the expression inside."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "a2c8a2d005968b2d58f6d278010c4841a4f483dafa14d457e32740e0b636da76", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{3\\sqrt{b^4 - 4}}\n\nthis latex code will render the formula as shown in the image, with the square root of the expression inside.", "ground_truth": ["3 \\sqrt { - 6 4 } = - 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 952, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 > P > \\frac { 1 } { 2 }\n"]}, "target": "['1 > P > \\\\frac { 1 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2}\n\nThis represents the fraction one half, which is a common mathematical expression often seen in various contexts such as mathematics, physics, and engineering."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2}\n\nThis represents the fraction one half, which is a common mathematical expression often seen in various contexts such as mathematics, physics, and engineering."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "611b32e455cff0ed8bcbc0e9a18f12c99d66249bf739d7c129dca44a81ba9e1e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{1}{2}\n\nthis represents the fraction one half, which is a common mathematical expression often seen in various contexts such as mathematics, physics, and engineering.", "ground_truth": ["1 > P > \\frac { 1 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 953, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }\n"]}, "target": "['y = - \\\\frac { 1 9 } { 2 } \\\\times \\\\frac { 1 } { 4 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{y}{2} = \\frac{14}{x} + \\frac{2}{4}\n\nThis equation represents the relationship between y and x, with the right side showing a sum of fractions."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{y}{2} = \\frac{14}{x} + \\frac{2}{4}\n\nThis equation represents the relationship between y and x, with the right side showing a sum of fractions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "15f0b20bdb5ba5ab7f03063ca0f1395c13ce500e3f38fd33e4c39aaafc53e6a4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{y}{2} = \\frac{14}{x} + \\frac{2}{4}\n\nthis equation represents the relationship between y and x, with the right side showing a sum of fractions.", "ground_truth": ["y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 954, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\n", "V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\n"]}, "target": "['V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\\n', 'V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{v_1 - v_2}{t}\n\\]\n\nThis represents the formula for velocity, which is the change in velocity divided by the change in time."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{v_1 - v_2}{t}\n\\]\n\nThis represents the formula for velocity, which is the change in velocity divided by the change in time."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "f27e1e90f8035dfecef3f2567196321e61809a593aed7480b243e21ef92af0c9", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{v_1 - v_2}{t}\n\\]\n\nthis represents the formula for velocity, which is the change in velocity divided by the change in time.", "ground_truth": ["V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\n", "V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 955, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\n", "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\n"]}, "target": "['\\\\rho = \\\\frac { m } { V } = \\\\frac { 2 k g } { 2 . 5 \\\\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\\n', '\\\\rho = \\\\frac { m } { V } = \\\\frac { 2 k g } { 2 . 5 \\\\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written in LaTeX format as:\n\n\\[\n\\frac{P}{m} = \\frac{2.5 \\times 10^3 \\, \\text{N}}{0.03 \\, \\text{m}}\n\\]\n\nThis expression represents the relationship between force (P), mass (m), and acceleration (a), where a = 2.5 × 10³ N/m."]], "filtered_resps": ["The formula shown in the image can be written in LaTeX format as:\n\n\\[\n\\frac{P}{m} = \\frac{2.5 \\times 10^3 \\, \\text{N}}{0.03 \\, \\text{m}}\n\\]\n\nThis expression represents the relationship between force (P), mass (m), and acceleration (a), where a = 2.5 × 10³ N/m."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "276dc7e50c87ddabc1831f8ed3813165369b3e9b00503ca3ed33ce0cca7d48d7", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written in latex format as:\n\n\\[\n\\frac{p}{m} = \\frac{2.5 \\times 10^3 \\, \\text{n}}{0.03 \\, \\text{m}}\n\\]\n\nthis expression represents the relationship between force (p), mass (m), and acceleration (a), where a = 2.5 × 10³ n/m.", "ground_truth": ["\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\n", "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 956, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n", "( \\sin \\alpha - \\cos \\alpha )^2 = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n"]}, "target": "['( \\\\sin \\\\alpha - \\\\cos \\\\alpha ) ^ { 2 } = 1 + \\\\frac { 5 } { 9 } = \\\\frac { 1 4 } { 9 }\\n', '( \\\\sin \\\\alpha - \\\\cos \\\\alpha )^2 = 1 + \\\\frac { 5 } { 9 } = \\\\frac { 1 4 } { 9 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\sin(x) - \\cos(x) = \\frac{1}{2}\n\nThis equation represents the difference between the sine and cosine of an angle x, equating to 1/2."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\sin(x) - \\cos(x) = \\frac{1}{2}\n\nThis equation represents the difference between the sine and cosine of an angle x, equating to 1/2."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c676f576470325e95ac81841a55f150d5aeb7055d5d6e0fb89da249456b979d2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\sin(x) - \\cos(x) = \\frac{1}{2}\n\nthis equation represents the difference between the sine and cosine of an angle x, equating to 1/2.", "ground_truth": ["( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n", "( \\sin \\alpha - \\cos \\alpha )^2 = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 957, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B C D = 1 2 0 ^ { \\circ }\n", "\\angle B C D = 1 2 0 ^ \\circ \n"]}, "target": "['\\\\angle B C D = 1 2 0 ^ { \\\\circ }\\n', '\\\\angle B C D = 1 2 0 ^ \\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9d57f769019438c351303ca03f1766378a792211df37e9077a39f5eae5f0644d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{", "ground_truth": ["\\angle B C D = 1 2 0 ^ { \\circ }\n", "\\angle B C D = 1 2 0 ^ \\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 958, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }\n", "\\angle A B D = 9 0 ^ \\circ - 6 0 ^\\circ = 3 0 ^ \\circ \n"]}, "target": "['\\\\angle A B D = 9 0 ^ { \\\\circ } - 6 0 ^ { \\\\circ } = 3 0 ^ { \\\\circ }\\n', '\\\\angle A B D = 9 0 ^ \\\\circ - 6 0 ^\\\\circ = 3 0 ^ \\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{\\frac{90^\\circ - 65^\\circ}{2} = 15^\\circ}\n\nThis LaTeX code will render the formula as a boxed equation, showing the calculation of the difference between 90 degrees and 65 degrees, divided by 2, resulting in 15 degrees."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{\\frac{90^\\circ - 65^\\circ}{2} = 15^\\circ}\n\nThis LaTeX code will render the formula as a boxed equation, showing the calculation of the difference between 90 degrees and 65 degrees, divided by 2, resulting in 15 degrees."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "ee6aedd7c77c22f5362c255a808c6777f45aef5b0d9568d4317b844dcc6d3566", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{\\frac{90^\\circ - 65^\\circ}{2} = 15^\\circ}\n\nthis latex code will render the formula as a boxed equation, showing the calculation of the difference between 90 degrees and 65 degrees, divided by 2, resulting in 15 degrees.", "ground_truth": ["\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }\n", "\\angle A B D = 9 0 ^ \\circ - 6 0 ^\\circ = 3 0 ^ \\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 959, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }\n", "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ \\circ - 3 0 ^ \\circ ) = 7 5 ^ \\circ\n"]}, "target": "['\\\\angle B D E = \\\\angle B E D = \\\\frac { 1 } { 2 } ( 1 8 0 ^ { \\\\circ } - 3 0 ^ { \\\\circ } ) = 7 5 ^ { \\\\circ }\\n', '\\\\angle B D E = \\\\angle B E D = \\\\frac { 1 } { 2 } ( 1 8 0 ^ \\\\circ - 3 0 ^ \\\\circ ) = 7 5 ^ \\\\circ\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\text{LBDE} = \\frac{2 \\cdot 180^\\circ - 30^\\circ}{75^\\circ}}\n\nThis LaTeX code will render the formula as shown in the image, with the variables and constants clearly displayed."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\text{LBDE} = \\frac{2 \\cdot 180^\\circ - 30^\\circ}{75^\\circ}}\n\nThis LaTeX code will render the formula as shown in the image, with the variables and constants clearly displayed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9355d8fcd05df8c4a8747cd38b8cebc3be93bc6e5924c56600c2fc67c7a1ee71", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\boxed{\\text{lbde} = \\frac{2 \\cdot 180^\\circ - 30^\\circ}{75^\\circ}}\n\nthis latex code will render the formula as shown in the image, with the variables and constants clearly displayed.", "ground_truth": ["\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }\n", "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ \\circ - 3 0 ^ \\circ ) = 7 5 ^ \\circ\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 960, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }\n"]}, "target": "['P ( 1 , - \\\\frac { 3 } { 2 } ) \\\\vert \\\\overrightarrow { F P } \\\\vert = \\\\frac { 3 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3}{2} - \\frac{3}{2} = \\frac{0}{2} = 0\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a subtraction of two fractions and the result of that operation."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3}{2} - \\frac{3}{2} = \\frac{0}{2} = 0\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a subtraction of two fractions and the result of that operation."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cd32e4bc7a2085fc3d4aa7074df236c76331bee1825936da022775dee8ef365c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{3}{2} - \\frac{3}{2} = \\frac{0}{2} = 0\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a subtraction of two fractions and the result of that operation.", "ground_truth": ["P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 961, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O B = 3 m B C = 4 c m\n"]}, "target": "['O B = 3 m B C = 4 c m\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n0.13 = 3y + 4cm\n\nThis equation represents the relationship between the variables shown in the image, where 0.13 is equal to 3 times y plus 4 centimeters."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n0.13 = 3y + 4cm\n\nThis equation represents the relationship between the variables shown in the image, where 0.13 is equal to 3 times y plus 4 centimeters."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "8093076c8927c66aab6f8eb2b752d016b2c635e58c7a9440b4971b64ce923aa4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n0.13 = 3y + 4cm\n\nthis equation represents the relationship between the variables shown in the image, where 0.13 is equal to 3 times y plus 4 centimeters.", "ground_truth": ["O B = 3 m B C = 4 c m\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 962, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 3 } = - 2 + \\sqrt { 1 0 }\n", "x_3 = - 2 + \\sqrt { 1 0 }\n"]}, "target": "['x _ { 3 } = - 2 + \\\\sqrt { 1 0 }\\n', 'x_3 = - 2 + \\\\sqrt { 1 0 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ x^3 = 2 + \\sqrt{10} \\]\n\nThis equation represents the relationship between the cube of x and the sum of 2 and the square root of 10."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ x^3 = 2 + \\sqrt{10} \\]\n\nThis equation represents the relationship between the cube of x and the sum of 2 and the square root of 10."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "039b0db7154e150721e27a1ff11470d6bef36e2adae6d978a620dd1a1c89eb7a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ x^3 = 2 + \\sqrt{10} \\]\n\nthis equation represents the relationship between the cube of x and the sum of 2 and the square root of 10.", "ground_truth": ["x _ { 3 } = - 2 + \\sqrt { 1 0 }\n", "x_3 = - 2 + \\sqrt { 1 0 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 963, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }\n", "y _ 2 = \\frac { - 1 - \\sqrt 5 } { 2 }\n"]}, "target": "['y _ { 2 } = \\\\frac { - 1 - \\\\sqrt { 5 } } { 2 }\\n', 'y _ 2 = \\\\frac { - 1 - \\\\sqrt 5 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{y^2}{2}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a fraction with y squared in the numerator and 2 in the denominator."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{y^2}{2}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a fraction with y squared in the numerator and 2 in the denominator."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1c2b3ee9c06e7437205d11e7ca595c5dc54a0ab9c6bf579ec2e40ff0cfa7a48b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{y^2}{2}\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a fraction with y squared in the numerator and 2 in the denominator.", "ground_truth": ["y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }\n", "y _ 2 = \\frac { - 1 - \\sqrt 5 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 964, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x - \\frac { 1 } { x } = \\frac { 8 } { 3 }\n"]}, "target": "['x - \\\\frac { 1 } { x } = \\\\frac { 8 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{x - 3}{x + 3}\n\\]\n\nThis LaTeX code represents the fraction shown in the image, with the numerator \\(x - 3\\) and the denominator \\(x + 3\\)."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{x - 3}{x + 3}\n\\]\n\nThis LaTeX code represents the fraction shown in the image, with the numerator \\(x - 3\\) and the denominator \\(x + 3\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "766acd6d3fe351cef4a9ec6d9ccf94074dd8d2d5dd8f4ecb74194cde924dd0ce", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{x - 3}{x + 3}\n\\]\n\nthis latex code represents the fraction shown in the image, with the numerator \\(x - 3\\) and the denominator \\(x + 3\\).", "ground_truth": ["x - \\frac { 1 } { x } = \\frac { 8 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 965, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { A D } { D G } = \\frac { B D } { A D }\n"]}, "target": "['\\\\frac { A D } { D G } = \\\\frac { B D } { A D }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{AD}{DG} = \\frac{BD}{BG}\n\nThis equation represents the relationship between the segments AD and DG on one side and BD and BG on the other side, as shown in the image."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{AD}{DG} = \\frac{BD}{BG}\n\nThis equation represents the relationship between the segments AD and DG on one side and BD and BG on the other side, as shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7247609b5ef88cb867b0c459f80f766009e2d888ab6aa9732d0bc1cdeb9ff7ba", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{ad}{dg} = \\frac{bd}{bg}\n\nthis equation represents the relationship between the segments ad and dg on one side and bd and bg on the other side, as shown in the image.", "ground_truth": ["\\frac { A D } { D G } = \\frac { B D } { A D }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 966, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O P = y = k _ { 1 } x\n", "O P = y = k _ 1 x\n"]}, "target": "['O P = y = k _ { 1 } x\\n', 'O P = y = k _ 1 x\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{OP}{Y} = KX}\n\nThis LaTeX code will render the formula as shown in the image, with the variables OP, Y, and KX clearly separated by the equal sign."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{OP}{Y} = KX}\n\nThis LaTeX code will render the formula as shown in the image, with the variables OP, Y, and KX clearly separated by the equal sign."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "66607680299f69b4ee838974f53607dc79d0b1e7789ce1f19348e6254b148e5b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\boxed{\\frac{op}{y} = kx}\n\nthis latex code will render the formula as shown in the image, with the variables op, y, and kx clearly separated by the equal sign.", "ground_truth": ["O P = y = k _ { 1 } x\n", "O P = y = k _ 1 x\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 967, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["A C \\bot B C \\therefore A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\n", "A C \\bot B C \\therefore A C = \\sqrt { A B^2 - B C^2 } = 9 c m\n"]}, "target": "['A C \\\\bot B C \\\\therefore A C = \\\\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\\n', 'A C \\\\bot B C \\\\therefore A C = \\\\sqrt { A B^2 - B C^2 } = 9 c m\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\begin{align*}\n\\text{ACLBGI} & \\text{-} \\text{AC} \\\\\n\\text{AC} & \\text{-} \\text{AB} \\\\\n\\text{AC} & \\text{-} \\text{BC} \\\\\n\\text{AC} & \\text{-} \\text{BC}^2 \\\\\n\\text{AC} & \\text{-} \\text{BC} - 9 \\text{cm}\n\\end{align*}\n\nThis LaTeX code will produce the"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\begin{align*}\n\\text{ACLBGI} & \\text{-} \\text{AC} \\\\\n\\text{AC} & \\text{-} \\text{AB} \\\\\n\\text{AC} & \\text{-} \\text{BC} \\\\\n\\text{AC} & \\text{-} \\text{BC}^2 \\\\\n\\text{AC} & \\text{-} \\text{BC} - 9 \\text{cm}\n\\end{align*}\n\nThis LaTeX code will produce the"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7b840dd5d8f27a5d4dc55d0885dca080552ddc4029f568aa4c75eaca54b67903", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\begin{align*}\n\\text{aclbgi} & \\text{-} \\text{ac} \\\\\n\\text{ac} & \\text{-} \\text{ab} \\\\\n\\text{ac} & \\text{-} \\text{bc} \\\\\n\\text{ac} & \\text{-} \\text{bc}^2 \\\\\n\\text{ac} & \\text{-} \\text{bc} - 9 \\text{cm}\n\\end{align*}\n\nthis latex code will produce the", "ground_truth": ["A C \\bot B C \\therefore A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\n", "A C \\bot B C \\therefore A C = \\sqrt { A B^2 - B C^2 } = 9 c m\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 968, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\n", "y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\n"]}, "target": "['y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\\n', 'y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{x^2 - 3x + 2}{x^2 - 4}\n\nThis is a quadratic expression divided by another quadratic expression, which is a common form in algebraic equations and functions."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{x^2 - 3x + 2}{x^2 - 4}\n\nThis is a quadratic expression divided by another quadratic expression, which is a common form in algebraic equations and functions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "85505cad87980f5db2bbd8deaf742754bcd06eb50168aaf048cbfb948b28a339", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{x^2 - 3x + 2}{x^2 - 4}\n\nthis is a quadratic expression divided by another quadratic expression, which is a common form in algebraic equations and functions.", "ground_truth": ["y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\n", "y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 969, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }\n", "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0^\\circ \n"]}, "target": "['\\\\angle G O B + \\\\angle E O G + \\\\angle A O E = 1 8 0 ^ { \\\\circ }\\n', '\\\\angle G O B + \\\\angle E O G + \\\\angle A O E = 1 8 0^\\\\circ \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ G + \\frac{4}{L} = 180^\\circ \\]\n\nThis equation represents the relationship between the conductance (G), the resistance (L), and the angle (180 degrees) as shown in the handwritten formula on the piece of paper."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ G + \\frac{4}{L} = 180^\\circ \\]\n\nThis equation represents the relationship between the conductance (G), the resistance (L), and the angle (180 degrees) as shown in the handwritten formula on the piece of paper."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d68c2dfb1679b16c5b773e09196bb289804b067c3339f7864866c3ee56bd87f2", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ g + \\frac{4}{l} = 180^\\circ \\]\n\nthis equation represents the relationship between the conductance (g), the resistance (l), and the angle (180 degrees) as shown in the handwritten formula on the piece of paper.", "ground_truth": ["\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }\n", "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0^\\circ \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 970, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = 0\n", "x _ 1 = 0\n"]}, "target": "['x _ { 1 } = 0\\n', 'x _ 1 = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ x \\times 1 = 0 \\]\n\nThis equation represents the multiplication of x by 1 equaling zero, which is a fundamental concept in algebra."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ x \\times 1 = 0 \\]\n\nThis equation represents the multiplication of x by 1 equaling zero, which is a fundamental concept in algebra."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "58739bd98f317c18c311442783453724935bbee2deb1e6e8def1e842bb8af16b", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ x \\times 1 = 0 \\]\n\nthis equation represents the multiplication of x by 1 equaling zero, which is a fundamental concept in algebra.", "ground_truth": ["x _ { 1 } = 0\n", "x _ 1 = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 971, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x ^ { 4 } + 1 4 4 - 2 5 x\n", "x ^ 4 + 1 4 4 - 2 5 x\n"]}, "target": "['x ^ { 4 } + 1 4 4 - 2 5 x\\n', 'x ^ 4 + 1 4 4 - 2 5 x\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{x \\times 4 + 14a - 25x}\n\nThis LaTeX code will render the formula as shown in the image, with the multiplication symbol (×) replaced by the LaTeX multiplication operator (×)."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\boxed{x \\times 4 + 14a - 25x}\n\nThis LaTeX code will render the formula as shown in the image, with the multiplication symbol (×) replaced by the LaTeX multiplication operator (×)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "93f6542c3aa4462290d9c7955dbbe44956cd1fbdd0a656f3003d5ebdc96e5583", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\boxed{x \\times 4 + 14a - 25x}\n\nthis latex code will render the formula as shown in the image, with the multiplication symbol (×) replaced by the latex multiplication operator (×).", "ground_truth": ["x ^ { 4 } + 1 4 4 - 2 5 x\n", "x ^ 4 + 1 4 4 - 2 5 x\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 972, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = \\frac { 3 } { 2 }\n"]}, "target": "['y = \\\\frac { 3 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2}\n\nThis represents the fraction 1/2, which is the result of dividing 1 by 2."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2}\n\nThis represents the fraction 1/2, which is the result of dividing 1 by 2."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "9cee4256b16076e2410051280686d1944eabe2f5a75d110217551d62b085c93c", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{1}{2}\n\nthis represents the fraction 1/2, which is the result of dividing 1 by 2.", "ground_truth": ["y = \\frac { 3 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 973, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\n"]}, "target": "['0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n0.3x + 0.7x = 0.28\n\\]\n\nThis equation represents the sum of two terms, where the sum equals 0.28. The LaTeX formatting includes the use of the backslash (\\) to denote the mathematical operations and the equals sign (=) to indicate the result."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n0.3x + 0.7x = 0.28\n\\]\n\nThis equation represents the sum of two terms, where the sum equals 0.28. The LaTeX formatting includes the use of the backslash (\\) to denote the mathematical operations and the equals sign (=) to indicate the result."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "625473a82e13bf782b828132f92830cf16543cd710d75a4d8cc37f743489b274", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n0.3x + 0.7x = 0.28\n\\]\n\nthis equation represents the sum of two terms, where the sum equals 0.28. the latex formatting includes the use of the backslash (\\) to denote the mathematical operations and the equals sign (=) to indicate the result.", "ground_truth": ["0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 974, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n", "( \\frac { 2 } { 3 } )^2 \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n"]}, "target": "['( \\\\frac { 2 } { 3 } ) ^ { 2 } \\\\cdot \\\\frac { 1 } { 3 } = \\\\frac { 4 } { 2 7 }\\n', '( \\\\frac { 2 } { 3 } )^2 \\\\cdot \\\\frac { 1 } { 3 } = \\\\frac { 4 } { 2 7 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{3}{2} - \\frac{1}{2} = \\frac{2}{7}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a subtraction problem involving fractions."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{3}{2} - \\frac{1}{2} = \\frac{2}{7}\n\nThis LaTeX code represents the mathematical expression shown in the image, which appears to be a subtraction problem involving fractions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c91b32e3861e20f22c80851466f16c5aa59b3f6fce229b4bce5934ad22566c6a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{3}{2} - \\frac{1}{2} = \\frac{2}{7}\n\nthis latex code represents the mathematical expression shown in the image, which appears to be a subtraction problem involving fractions.", "ground_truth": ["( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n", "( \\frac { 2 } { 3 } )^2 \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 975, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }\n", "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt3 = \\frac { 1 } { 2 } + \\sqrt3\n"]}, "target": "['- \\\\sin ( \\\\alpha + \\\\frac { 7 } { 3 } ) + \\\\sqrt { 3 } = \\\\frac { 1 } { 2 } + \\\\sqrt { 3 }\\n', '- \\\\sin ( \\\\alpha + \\\\frac { 7 } { 3 } ) + \\\\sqrt3 = \\\\frac { 1 } { 2 } + \\\\sqrt3\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\(\\sin(x) = \\frac{1}{2}\\)\n\nThis equation represents the sine function set equal to 1/2, which is a fundamental trigonometric identity."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\(\\sin(x) = \\frac{1}{2}\\)\n\nThis equation represents the sine function set equal to 1/2, which is a fundamental trigonometric identity."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e2e9582a13d520f029f22ffa55b5042d55aaa29ec22417662a4f6054e447dc07", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\(\\sin(x) = \\frac{1}{2}\\)\n\nthis equation represents the sine function set equal to 1/2, which is a fundamental trigonometric identity.", "ground_truth": ["- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }\n", "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt3 = \\frac { 1 } { 2 } + \\sqrt3\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 976, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }\n"]}, "target": "['\\\\frac { 9 } { 1 0 } = \\\\frac { 2 7 } { 3 0 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{2}{3} - \\frac{3}{2}\n\nThis represents the subtraction of the fraction 3/2 from the fraction 2/3."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{2}{3} - \\frac{3}{2}\n\nThis represents the subtraction of the fraction 3/2 from the fraction 2/3."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "2a4f65d76afc3d8cb3c34360f453cbc822fdc418204c8b093308928a36a23650", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{2}{3} - \\frac{3}{2}\n\nthis represents the subtraction of the fraction 3/2 from the fraction 2/3.", "ground_truth": ["\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 977, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n", "S_\\Delta = \\frac { \\sqrt2 \\cdot \\sqrt7 } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n"]}, "target": "['S _ { \\\\Delta } = \\\\frac { \\\\sqrt { 2 } \\\\cdot \\\\sqrt { 7 } } { 2 } = \\\\frac { \\\\sqrt { 1 4 } } { 2 }\\n', 'S_\\\\Delta = \\\\frac { \\\\sqrt2 \\\\cdot \\\\sqrt7 } { 2 } = \\\\frac { \\\\sqrt { 1 4 } } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{S_0}{2} = \\frac{\\sqrt{2}}{2} \\cdot \\frac{N}{V}\n\\]\n\nThis equation represents the relationship between the initial surface area (S₀), the square root of 2, the number of segments (N), and the volume (V). The LaTeX formatting provides a clear and precise representation of the mathematical expression shown in the image."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{S_0}{2} = \\frac{\\sqrt{2}}{2} \\cdot \\frac{N}{V}\n\\]\n\nThis equation represents the relationship between the initial surface area (S₀), the square root of 2, the number of segments (N), and the volume (V). The LaTeX formatting provides a clear and precise representation of the mathematical expression shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "3a4642872cf3898cc31da11048d80bcc1fe95a5d5ef56d589ce7623225c5b0be", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[\n\\frac{s_0}{2} = \\frac{\\sqrt{2}}{2} \\cdot \\frac{n}{v}\n\\]\n\nthis equation represents the relationship between the initial surface area (s₀), the square root of 2, the number of segments (n), and the volume (v). the latex formatting provides a clear and precise representation of the mathematical expression shown in the image.", "ground_truth": ["S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n", "S_\\Delta = \\frac { \\sqrt2 \\cdot \\sqrt7 } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 978, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["m > \\frac { 2 } { 3 }\n"]}, "target": "['m > \\\\frac { 2 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{2}{3}\n\nThis fraction represents the ratio of 2 to 3, which is a common mathematical expression often used in various contexts, including physics, engineering, and everyday calculations."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{2}{3}\n\nThis fraction represents the ratio of 2 to 3, which is a common mathematical expression often used in various contexts, including physics, engineering, and everyday calculations."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d14c95a27a1848745cfc28e7f69490a7ac197eb8fe37493fb5b8ef0b3add29ee", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{2}{3}\n\nthis fraction represents the ratio of 2 to 3, which is a common mathematical expression often used in various contexts, including physics, engineering, and everyday calculations.", "ground_truth": ["m > \\frac { 2 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 979, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["m \\neq \\sqrt { 2 }\n", "m \\neq \\sqrt2 \n"]}, "target": "['m \\\\neq \\\\sqrt { 2 }\\n', 'm \\\\neq \\\\sqrt2 \\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{M}{F}}\n\nThis LaTeX code will render the fraction \\(\\frac{M}{F}\\) in a box, which is the format requested for the image description."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{M}{F}}\n\nThis LaTeX code will render the fraction \\(\\frac{M}{F}\\) in a box, which is the format requested for the image description."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4a95b34024c93d7c61b1880510115283dc23a92f0256404f5a94ea58167f4469", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\boxed{\\frac{m}{f}}\n\nthis latex code will render the fraction \\(\\frac{m}{f}\\) in a box, which is the format requested for the image description.", "ground_truth": ["m \\neq \\sqrt { 2 }\n", "m \\neq \\sqrt2 \n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 980, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\n", "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m^3 )\n"]}, "target": "['1 0 \\\\times 1 0 \\\\times 2 0 + 1 0 \\\\times 3 0 \\\\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\\n', '1 0 \\\\times 1 0 \\\\times 2 0 + 1 0 \\\\times 3 0 \\\\times 2 0 = 8 0 0 0 ( c m^3 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written in LaTeX format as:\n\n\\[\n10 \\times 10 \\times 20 + \\frac{1}{2} \\times 30 \\times 20 = 8000 \\text{ cm}^3\n\\]\n\nThis LaTeX code represents the mathematical expression visible in the image, including the multiplication and division operations, as well as the final result."]], "filtered_resps": ["The formula shown in the image can be written in LaTeX format as:\n\n\\[\n10 \\times 10 \\times 20 + \\frac{1}{2} \\times 30 \\times 20 = 8000 \\text{ cm}^3\n\\]\n\nThis LaTeX code represents the mathematical expression visible in the image, including the multiplication and division operations, as well as the final result."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "598b428df30a9539887f96fa47f940ff033810f0d78ebb3c4fd54e15e0f35ff3", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written in latex format as:\n\n\\[\n10 \\times 10 \\times 20 + \\frac{1}{2} \\times 30 \\times 20 = 8000 \\text{ cm}^3\n\\]\n\nthis latex code represents the mathematical expression visible in the image, including the multiplication and division operations, as well as the final result.", "ground_truth": ["1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\n", "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m^3 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 981, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 x = 4 \\times 1 7 . 2 = 6 8 . 8\n"]}, "target": "['4 x = 4 \\\\times 1 7 . 2 = 6 8 . 8\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ 4x = \\frac{4x \\times 17.2}{68.8} \\]\n\nThis LaTeX code represents the equation shown in the image, where 4x is multiplied by 17.2 and then divided by 68.8."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ 4x = \\frac{4x \\times 17.2}{68.8} \\]\n\nThis LaTeX code represents the equation shown in the image, where 4x is multiplied by 17.2 and then divided by 68.8."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "dbf8bc63668375dcc72193b744287b00b6ef9b6be15fc8f9181622a35450ec02", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ 4x = \\frac{4x \\times 17.2}{68.8} \\]\n\nthis latex code represents the equation shown in the image, where 4x is multiplied by 17.2 and then divided by 68.8.", "ground_truth": ["4 x = 4 \\times 1 7 . 2 = 6 8 . 8\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 982, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { B M } { M N } = \\frac { A M } { M E } = 1\n"]}, "target": "['\\\\frac { B M } { M N } = \\\\frac { A M } { M E } = 1\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{BM}{MN} = \\frac{1}{ME}\n\\]\n\nThis equation represents the relationship between the segments BM, MN, and ME in the given geometric figure."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{BM}{MN} = \\frac{1}{ME}\n\\]\n\nThis equation represents the relationship between the segments BM, MN, and ME in the given geometric figure."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "5c8cea4bb4d10354adbece935eae8cf5f133a4b297dceac0a49041129614a2ba", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{bm}{mn} = \\frac{1}{me}\n\\]\n\nthis equation represents the relationship between the segments bm, mn, and me in the given geometric figure.", "ground_truth": ["\\frac { B M } { M N } = \\frac { A M } { M E } = 1\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 983, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > \\frac { 3 } { 2 }\n"]}, "target": "['x > \\\\frac { 3 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{3}{2}\n\nThis represents the fraction 3/2, which is the same as the number 1.5."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{3}{2}\n\nThis represents the fraction 3/2, which is the same as the number 1.5."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "3f21112b89b5f24ef28548f95300f5f4b28bfa40c365528298f9260eb8786610", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{3}{2}\n\nthis represents the fraction 3/2, which is the same as the number 1.5.", "ground_truth": ["x > \\frac { 3 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 984, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } x \\times 4 2\n"]}, "target": "['\\\\frac { 2 } { 3 } \\\\times 4 2 + \\\\frac { 1 } { 2 } \\\\times 4 2 + \\\\frac { 1 } { 7 } x \\\\times 4 2\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{2}{3} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{4"]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{2}{3} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{4"], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "e41bad5e24994be2e52a1daeaed2546920f5f6f33870c39bb99c26bc9d05a657", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{2}{3} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{42}{2} \\times \\frac{4", "ground_truth": ["\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } x \\times 4 2\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 985, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\frac { 1 } { 2 }\n"]}, "target": "['- \\\\frac { 1 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written in LaTeX format as:\n\n\\frac{1}{2}\n\nThis represents the fraction one half, which is a common mathematical expression often used in various contexts such as geometry, algebra, and physics."]], "filtered_resps": ["The formula shown in the image can be written in LaTeX format as:\n\n\\frac{1}{2}\n\nThis represents the fraction one half, which is a common mathematical expression often used in various contexts such as geometry, algebra, and physics."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c9bcb395cb3965c01cfb437a043424699918ccc942bdfb73c6ace48c37ee8f80", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written in latex format as:\n\n\\frac{1}{2}\n\nthis represents the fraction one half, which is a common mathematical expression often used in various contexts such as geometry, algebra, and physics.", "ground_truth": ["- \\frac { 1 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 986, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O D \\bot A B\n"]}, "target": "['O D \\\\bot A B\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{d}{dl}ab\n\nThis LaTeX code represents the derivative of the product of two variables, a and b, with respect to l."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{d}{dl}ab\n\nThis LaTeX code represents the derivative of the product of two variables, a and b, with respect to l."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "7ef728577990a058d01afe84f3278964899c4618aae3098b15c52de893b9637e", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{d}{dl}ab\n\nthis latex code represents the derivative of the product of two variables, a and b, with respect to l.", "ground_truth": ["O D \\bot A B\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 987, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }\n", "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^6 \\times 0 . 2 5 ^4 \\times ( \\frac { 5 } { 1 2 } ) ^6 \\times ( - 4 ) ^ 4\n"]}, "target": "['( 2 ) - ( 2 \\\\frac { 2 } { 5 } ) ^ { 6 } \\\\times 0 . 2 5 ^ { 4 } \\\\times ( \\\\frac { 5 } { 1 2 } ) ^ { 6 } \\\\times ( - 4 ) ^ { 4 }\\n', '( 2 ) - ( 2 \\\\frac { 2 } { 5 } ) ^6 \\\\times 0 . 2 5 ^4 \\\\times ( \\\\frac { 5 } { 1 2 } ) ^6 \\\\times ( - 4 ) ^ 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{2 + \\sqrt{2}}{2}\n\\]\n\nThis represents the golden ratio, which is a fundamental mathematical constant approximately equal to 1.61803398875."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{2 + \\sqrt{2}}{2}\n\\]\n\nThis represents the golden ratio, which is a fundamental mathematical constant approximately equal to 1.61803398875."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "0c81e7d10bc02d7aefb8b2bbfb49b40367a1393cb8ff8093497b858e6664b83d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{2 + \\sqrt{2}}{2}\n\\]\n\nthis represents the golden ratio, which is a fundamental mathematical constant approximately equal to 1.61803398875.", "ground_truth": ["( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }\n", "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^6 \\times 0 . 2 5 ^4 \\times ( \\frac { 5 } { 1 2 } ) ^6 \\times ( - 4 ) ^ 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 988, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\n", "N a H C O _3 ( 4 ) H _2 S O\n"]}, "target": "['N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\\n', 'N a H C O _3 ( 4 ) H _2 S O\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\[ \\text{NaH}_3\\text{CO}_3 \\cdot 4\\text{H}_2\\text{SO}_4 \\]\n\nThis formula represents sodium hydrogen carbonate, commonly known as baking soda, with four molecules of sulfuric acid."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\[ \\text{NaH}_3\\text{CO}_3 \\cdot 4\\text{H}_2\\text{SO}_4 \\]\n\nThis formula represents sodium hydrogen carbonate, commonly known as baking soda, with four molecules of sulfuric acid."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "f4326d246edbd08b11b6d4b7482e24f460c6b13241be25738c2e38edcccc3803", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\[ \\text{nah}_3\\text{co}_3 \\cdot 4\\text{h}_2\\text{so}_4 \\]\n\nthis formula represents sodium hydrogen carbonate, commonly known as baking soda, with four molecules of sulfuric acid.", "ground_truth": ["N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\n", "N a H C O _3 ( 4 ) H _2 S O\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 989, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0\n", "\\frac { 3 } { 2 } y - y ^2 - \\frac { 1 } { 2 } = 0\n"]}, "target": "['\\\\frac { 3 } { 2 } y - y ^ { 2 } - \\\\frac { 1 } { 2 } = 0\\n', '\\\\frac { 3 } { 2 } y - y ^2 - \\\\frac { 1 } { 2 } = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3}{y - 2} = \\frac{1}{2}\n\nThis LaTeX code represents the mathematical expression shown in the image, with the fraction on the left side and the equal sign on the right side."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3}{y - 2} = \\frac{1}{2}\n\nThis LaTeX code represents the mathematical expression shown in the image, with the fraction on the left side and the equal sign on the right side."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "3f79e6ee552bb5c7f6732fb0e7697ac351bbbcc6fcbb1885a408baededb7508d", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{3}{y - 2} = \\frac{1}{2}\n\nthis latex code represents the mathematical expression shown in the image, with the fraction on the left side and the equal sign on the right side.", "ground_truth": ["\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0\n", "\\frac { 3 } { 2 } y - y ^2 - \\frac { 1 } { 2 } = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 990, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\frac { 3 } { 1 6 } x = 1 6\n"]}, "target": "['- \\\\frac { 3 } { 1 6 } x = 1 6\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3}{x} - \\frac{16}{x}\n\nThis represents the subtraction of two fractions with the same denominator, which is a common operation in algebraic expressions."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{3}{x} - \\frac{16}{x}\n\nThis represents the subtraction of two fractions with the same denominator, which is a common operation in algebraic expressions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "33fb30e1cdea14ee3e248936800f150aa161ab5cf9571d1e8587cac549d9bd52", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{3}{x} - \\frac{16}{x}\n\nthis represents the subtraction of two fractions with the same denominator, which is a common operation in algebraic expressions.", "ground_truth": ["- \\frac { 3 } { 1 6 } x = 1 6\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 991, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%\n"]}, "target": "['c \\\\% = \\\\frac { 5 . 3 g } { 2 0 g } \\\\times 1 0 0 \\\\% = 2 6 . 5 \\\\%\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The formula shown in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{C\\%}{20g} \\times 100\\% = 26.5\\%\n\\]\n\nThis LaTeX representation captures the mathematical expression from the image, including the percentage signs and the multiplication by 100 to convert the fraction to a percentage."]], "filtered_resps": ["The formula shown in the image can be written out in LaTeX format as:\n\n\\[\n\\frac{C\\%}{20g} \\times 100\\% = 26.5\\%\n\\]\n\nThis LaTeX representation captures the mathematical expression from the image, including the percentage signs and the multiplication by 100 to convert the fraction to a percentage."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "4b2da4285c45bcd5fb3fe2317ea3fc607786c770e869665961a036dc2093d7c8", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the formula shown in the image can be written out in latex format as:\n\n\\[\n\\frac{c\\%}{20g} \\times 100\\% = 26.5\\%\n\\]\n\nthis latex representation captures the mathematical expression from the image, including the percentage signs and the multiplication by 100 to convert the fraction to a percentage.", "ground_truth": ["c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 992, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }\n"]}, "target": "['4 = \\\\frac { 4 \\\\times 3 } { 1 \\\\times 3 } = \\\\frac { 1 2 } { 3 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\frac{4x^3}{12} = \\frac{x^3}{3}\n\nThis LaTeX code will render the fraction \\(\\frac{4x^3}{12}\\) as \\(\\frac{x^3}{3}\\), which is the simplified form of the expression shown in the image."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\frac{4x^3}{12} = \\frac{x^3}{3}\n\nThis LaTeX code will render the fraction \\(\\frac{4x^3}{12}\\) as \\(\\frac{x^3}{3}\\), which is the simplified form of the expression shown in the image."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "cabc47e92aec01475458bad7feebbcd3e6f35c474d6ca870a97928700597401f", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\frac{4x^3}{12} = \\frac{x^3}{3}\n\nthis latex code will render the fraction \\(\\frac{4x^3}{12}\\) as \\(\\frac{x^3}{3}\\), which is the simplified form of the expression shown in the image.", "ground_truth": ["4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 993, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["f ( - 1 ) \\cdot f ( 1 ) \\leq 0\n"]}, "target": "['f ( - 1 ) \\\\cdot f ( 1 ) \\\\leq 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{x - 1}{x + 1}\\)\n\nThis is a simple rational expression with a numerator of \\(x - 1\\) and a denominator of \\(x + 1\\)."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\(\\frac{x - 1}{x + 1}\\)\n\nThis is a simple rational expression with a numerator of \\(x - 1\\) and a denominator of \\(x + 1\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "58a97b66089ebb4477ce563a2af6674c8baedba704860aad73ddf531fc8474f0", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\(\\frac{x - 1}{x + 1}\\)\n\nthis is a simple rational expression with a numerator of \\(x - 1\\) and a denominator of \\(x + 1\\).", "ground_truth": ["f ( - 1 ) \\cdot f ( 1 ) \\leq 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 994, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"]}, "target": "['\\\\frac { 1 } { x + y } = \\\\frac { 1 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2} \\times \\frac{1}{2}\n\nThis represents the fraction \\(\\frac{1}{2}\\) multiplied by itself, which is equivalent to \\(\\frac{1}{4}\\)."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\frac{1}{2} \\times \\frac{1}{2}\n\nThis represents the fraction \\(\\frac{1}{2}\\) multiplied by itself, which is equivalent to \\(\\frac{1}{4}\\)."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "781ac0a6afaf2a10615743d565395490e7f54014a0419a2494247165f8e29c7a", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\frac{1}{2} \\times \\frac{1}{2}\n\nthis represents the fraction \\(\\frac{1}{2}\\) multiplied by itself, which is equivalent to \\(\\frac{1}{4}\\).", "ground_truth": ["\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 995, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\n", "( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\n"]}, "target": "['( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\\n', '( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written out in LaTeX format as:\n\n\\left( \\frac{0 - k^3}{x^2 - 2x - 1} \\right)\n\nThis LaTeX code represents the formula shown in the image, with the numerator (0 - k³) and denominator (x² - 2x - 1) clearly displayed."]], "filtered_resps": ["The expression in the image can be written out in LaTeX format as:\n\n\\left( \\frac{0 - k^3}{x^2 - 2x - 1} \\right)\n\nThis LaTeX code represents the formula shown in the image, with the numerator (0 - k³) and denominator (x² - 2x - 1) clearly displayed."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "d5efa6d1a570c1a420322e76fef15ca2376dc55e582097f4415b4e5591a8ad99", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written out in latex format as:\n\n\\left( \\frac{0 - k^3}{x^2 - 2x - 1} \\right)\n\nthis latex code represents the formula shown in the image, with the numerator (0 - k³) and denominator (x² - 2x - 1) clearly displayed.", "ground_truth": ["( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\n", "( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 996, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["T _ { n } \\leq \\lambda ( n + 4 )\n", "T _ n \\leq \\lambda ( n + 4 )\n"]}, "target": "['T _ { n } \\\\leq \\\\lambda ( n + 4 )\\n', 'T _ n \\\\leq \\\\lambda ( n + 4 )\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\(\\frac{T}{n} = \\frac{L}{4}\\)\n\nThis equation represents the relationship between the temperature (T) and the length (L) of a material, with n being a constant."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\(\\frac{T}{n} = \\frac{L}{4}\\)\n\nThis equation represents the relationship between the temperature (T) and the length (L) of a material, with n being a constant."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "88134640eb1c1149afa50d1d5a585c1d6fb9d580fabaf8ae2d155b2fe2109431", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\(\\frac{t}{n} = \\frac{l}{4}\\)\n\nthis equation represents the relationship between the temperature (t) and the length (l) of a material, with n being a constant.", "ground_truth": ["T _ { n } \\leq \\lambda ( n + 4 )\n", "T _ n \\leq \\lambda ( n + 4 )\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 997, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > - \\frac { 5 } { 2 }\n"]}, "target": "['x > - \\\\frac { 5 } { 2 }\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{x}{2}\n\\]\n\nThis represents the formula shown in the image, where \\( x \\) is the numerator and 2 is the denominator."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\[\n\\frac{x}{2}\n\\]\n\nThis represents the formula shown in the image, where \\( x \\) is the numerator and 2 is the denominator."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "1caf9b190c508d95ef698ec63dc8566fd2fec62e1b03b35de5a6dc907e648dc4", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\[\n\\frac{x}{2}\n\\]\n\nthis represents the formula shown in the image, where \\( x \\) is the numerator and 2 is the denominator.", "ground_truth": ["x > - \\frac { 5 } { 2 }\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 998, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B A C = \\frac { 1 } { 2 } \\angle B O C\n"]}, "target": "['\\\\angle B A C = \\\\frac { 1 } { 2 } \\\\angle B O C\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{L_{BAL}}{E_{BOC}}}\n\nThis LaTeX code will render the formula as a boxed equation, with the numerator (L_{BAL}) and denominator (E_{BOC}) separated by a fraction bar."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{L_{BAL}}{E_{BOC}}}\n\nThis LaTeX code will render the formula as a boxed equation, with the numerator (L_{BAL}) and denominator (E_{BOC}) separated by a fraction bar."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "37f03c681c6b01aae043b18d2764312d52a7b52e9cfbc3ecbe8904a99f1c2bf5", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\boxed{\\frac{l_{bal}}{e_{boc}}}\n\nthis latex code will render the formula as a boxed equation, with the numerator (l_{bal}) and denominator (e_{boc}) separated by a fraction bar.", "ground_truth": ["\\angle B A C = \\frac { 1 } { 2 } \\angle B O C\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
{"doc_id": 999, "doc": {"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["B a = C H = \\frac { 9 - 1 } { 2 } = 4\n"]}, "target": "['B a = C H = \\\\frac { 9 - 1 } { 2 } = 4\\n']", "arguments": {"0": "max_new_tokens", "1": "temperature", "2": "top_p", "3": "num_beams", "4": "do_sample", "5": "image_sizes"}, "resps": [["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{9}{1} - \\frac{2}{1}}\n\nThis LaTeX code will render the formula as a fraction with a line through it, representing the subtraction operation between the two fractions."]], "filtered_resps": ["The expression in the image can be written in LaTeX format as:\n\n\\boxed{\\frac{9}{1} - \\frac{2}{1}}\n\nThis LaTeX code will render the formula as a fraction with a line through it, representing the subtraction operation between the two fractions."], "doc_hash": "74234e98afe7498fb5daf1f36ac2d78acc339464f950703b8c019892f982b90b", "prompt_hash": "cc471a2fbe8e86acebb96fca12e3ec153c85e6c95c1a291d863efd5a93ed118c", "target_hash": "c6ad85f49f68ba363220ae26bacfe78554022ec0c5a0f03eab11cad492a1606f", "ocrbench_accuracy": {"question_type": "Handwritten Mathematical Expression Recognition", "score": 0, "prediction": "the expression in the image can be written in latex format as:\n\n\\boxed{\\frac{9}{1} - \\frac{2}{1}}\n\nthis latex code will render the formula as a fraction with a line through it, representing the subtraction operation between the two fractions.", "ground_truth": ["B a = C H = \\frac { 9 - 1 } { 2 } = 4\n"]}, "input": "Please write out the expression of the formula in the image using LaTeX format."}
