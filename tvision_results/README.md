# TowerVision Benchmark Results

This repository contains evaluation results of multiple **TowerVision models** across different benchmarks.  
Each benchmark folder contains per-model results.

---

## ğŸ”¤ Nomenclature & Conventions

We follow a structured naming convention for models and results:

### ğŸ§© Model Naming
- **`base`** â†’ TowerPlus base model  
- **`instruct`** â†’ TowerPlus instruct model  
- **`full`** â†’ trained with **all available data**, including cultural-ground extensions  
- **`siglip1`** â†’ model uses **SigLIP1** as the vision encoder  
- **`siglip2-512`** â†’ model uses **SigLIP2** with higher-resolution input (512)  
- **(no encoder specified)** â†’ by default, the model uses **SigLIP2**  

Examples:
- `towerp_2b_base` â†’ Base model  
- `towerp_2b_instruct` â†’ Instruct model  
- `towerp_2b_base_full_siglip1` â†’ Base model, trained on full data, with SigLIP1 encoder  
- `towerp_2b_base_full_siglip512` â†’ Base model, full data, with SigLIP2 (512 resolution)  

exception:
- `towerp_2b_cpt_full_97` â†’ Base model, full data, siglip2 but trained until 97% (cluster id down temporarily)

---

### ğŸ“ˆ Results Interpretation
- Benchmarks include the **number of samples per benchmark** and **per language**.  
- **`sub`** in a language result â†’ language is supported by TowerVision  
- **`not support`** â†’ language is not supported by TowerVision  
- **`+` (plus sign)** next to a language â†’ additional data for that language was included in **full mode**  

---

## ğŸ“Š Benchmarks

- **alm_bench-all**  
- **cc-ocr-multi-lan**  
- **m3exam**  
- **multi30k-all**  
- **ocrbench**  
- **textvqa**

---

## ğŸ§  Models Evaluated
Examples of models tested:
- `towerp_2b_base`
- `towerp_2b_instruct`
- `towerp_9b_base`
- `towerp_9b_instruct`
- `towerp_2b_base_full_siglip1`
- `towerp_2b_base_full_siglip512 + siglip2_512`
- and more...

---

## ğŸŒ Results by Language
![Language Results](lan_results.png)

---

## ğŸ† Results by Benchmark
![Benchmark Results](towervision_results.png)
